{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1RinuVfkPG8"
      },
      "source": [
        "# **Planteamiento del Proyecto Final – Asistente Coreográfico Inteligente**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDjII2yIkY63"
      },
      "source": [
        "1. Problema / Motivación\n",
        "\n",
        "En la creación coreográfica, los coreógrafos trabajan con múltiples variables al mismo tiempo:\n",
        "- calidad de movimiento,\n",
        "- ocupación del espacio,\n",
        "- dinámica rítmica,\n",
        "- coordinación grupal,\n",
        "- estética escénica.\n",
        "Actualmente, no existe una herramienta inteligente que les dé retroalimentación objetiva en tiempo real o a partir de un vídeo grabado.\n",
        "\n",
        "El problema:\n",
        "- El proceso creativo depende únicamente de la percepción subjetiva del coreógrafo y carece de métricas automáticas o sugerencias asistidas por IA.\n",
        "\n",
        "Objetivo:\n",
        "- crear un asistente coreográfico basado en visión por computador y machine learning que analice vídeos (subidos o en directo por cámara) y proporcione sugerencias de mejora en términos de estética, técnica y composición espacial.\n",
        "\n",
        "2. Obtención de Datos\n",
        "- Fuentes de datos abiertas y específicas:\n",
        "- Datasets de movimiento humano:\n",
        "- AIST++ (Google Research): dataset de danza contemporánea y urbana con música sincronizada.\n",
        "- Pose Estimation Pretrained Datasets:\n",
        "- COCO, MPII (pose humana general).\n",
        "- Videos propios / capturados:\n",
        "- Posibilidad de grabar muestras de ensayos o fragmentos de repertorio clásico y contemporáneo.\n",
        "- Herramientas para anotación:\n",
        "- OpenPose, MediaPipe para generar puntos clave (skeletons).\n",
        "- Enriquecer con metadatos: tiempo, nivel de energía, calidad espacial.\n",
        "\n",
        "3. Procesamiento y Limpieza de Datos\n",
        "\n",
        "- Conversión de vídeos a frames.\n",
        "- Extracción de poses humanas (skeletons 2D o 3D con OpenPose/MediaPipe).\n",
        "- Normalización de las secuencias (longitud, fps).\n",
        "- Anotación de variables relevantes:\n",
        "  - amplitud del movimiento,\n",
        "  - simetría,\n",
        "  - alineación corporal,\n",
        "  - sincronización grupal,\n",
        "  - desplazamiento espacial.\n",
        "\n",
        "4. Análisis Exploratorio (EDA)\n",
        "- Visualización de trayectorias de movimiento (plots de keypoints).\n",
        "- Distribución de amplitud y velocidad en diferentes estilos.\n",
        "- Comparación entre intérpretes y patrones.\n",
        "- Extracción de métricas:\n",
        "  - rango de movimiento,\n",
        "  - fluidez (velocidad y aceleración),\n",
        "  - nivel espacial (alto/medio/bajo),\n",
        "  - balance entre simetría/asimetría.\n",
        "\n",
        "5. Feature Engineering\n",
        "- Diseñar features coreográficas a partir de las poses:\n",
        "- Técnicos: alineación postural, equilibrio, extensiones.\n",
        "- Espaciales: uso del espacio, direcciones, niveles.\n",
        "- Rítmicos: tempo, sincronización con música (opcional).\n",
        "- Estéticos: fluidez, originalidad de trayectorias.\n",
        "- Los features serán el input del modelo de ML.\n",
        "\n",
        "6. Modelado (Machine Learning / Deep Learning)\n",
        "- Modelos candidatos:\n",
        "  - Clasificación / recomendación:\n",
        "    Red neuronal recurrente (LSTM/GRU) para secuencias de poses → sugiere patrones alternativos.\n",
        "    SVM o Random Forest para evaluar calidad técnica.\n",
        "  - Análisis en tiempo real:\n",
        "  - Pose estimacion + métricas evaluadas frame a frame.\n",
        "- Sistema de sugerencias:\n",
        "  - Basado en similitud con patrones de repertorio (ej. Giselle, Lago de los Cisnes) o movimientos de datasets.\n",
        "  - Basado en reglas estéticas (ejemplo: sugerir más uso de diagonales si todo está frontal).\n",
        "\n",
        "7. Evaluación de Resultados\n",
        "- Métricas de clasificación: accuracy, F1-score (para calidad técnica).\n",
        "- Métricas cuantitativas: rango de movimiento, simetría, velocidad promedio.\n",
        "- Validación cualitativa: feedback de coreógrafos/profesores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7MEqotImlvz"
      },
      "source": [
        "**Introducción**\n",
        "\n",
        "La creación coreográfica es un proceso complejo en el que confluyen múltiples dimensiones: técnica, expresiva, estética, espacial y musical. Tradicionalmente, los coreógrafos se apoyan en la observación subjetiva y en la interacción directa con los intérpretes para tomar decisiones durante la composición. Sin embargo, en la actualidad, el desarrollo de tecnologías de visión por computador y aprendizaje automático abre la posibilidad de incorporar herramientas inteligentes que analicen objetivamente el movimiento y ofrezcan sugerencias en tiempo real o a partir de grabaciones.\n",
        "El presente proyecto propone el desarrollo de un asistente coreográfico inteligente que, a partir de la captura en vídeo (subido o en streaming), procese las secuencias de movimiento de los bailarines y proporcione retroalimentación automatizada para mejorar la calidad técnica, la expresividad y la organización espacial de la coreografía. Este enfoque busca situarse en la intersección entre arte y tecnología, ofreciendo al coreógrafo un apoyo complementario que no sustituye la creatividad humana, sino que la amplifica mediante análisis basados en datos.\n",
        "\n",
        "Objetivos\n",
        "\n",
        "Objetivo General\n",
        "\n",
        "Diseñar e implementar un sistema de asistencia coreográfica basado en visión por computador y machine learning, capaz de analizar vídeos de danza y generar sugerencias constructivas que apoyen el proceso creativo de coreógrafos y bailarines.\n",
        "\n",
        "Objetivos Específicos\n",
        "\n",
        "Revisión crítica del problema: identificar los principales retos en la creación coreográfica que pueden beneficiarse de un análisis automatizado (técnicos, espaciales, rítmicos, estéticos).\n",
        "\n",
        "Obtención y procesamiento de datos: recopilar datasets de danza y procesar secuencias de movimiento mediante técnicas de pose estimation (MediaPipe, OpenPose).\n",
        "\n",
        "Definición de features coreográficas: diseñar métricas objetivas a partir de los keypoints del cuerpo humano (amplitud, simetría, ocupación espacial, fluidez).\n",
        "\n",
        "Entrenamiento de modelos de machine learning: explorar algoritmos de clasificación y análisis secuencial (SVM, Random Forest, LSTM/GRU) para evaluar la calidad del movimiento y generar sugerencias.\n",
        "\n",
        "Validación del sistema: comparar métricas técnicas con evaluaciones de expertos en danza, asegurando la pertinencia de las recomendaciones.\n",
        "Desarrollo de una interfaz de uso: implementar un prototipo funcional en un notebook (y eventualmente en una app sencilla) donde se puedan subir vídeos o conectar una cámara en directo.\n",
        "\n",
        " Revisión del Problema\n",
        "\n",
        "El proceso de creación coreográfica enfrenta varios desafíos:\n",
        "Subjetividad en la evaluación: la calidad de un movimiento o de una secuencia depende en gran medida de la mirada del coreógrafo, lo que puede limitar la retroalimentación objetiva.\n",
        "\n",
        "Dificultad de análisis en tiempo real: en ensayos, es complicado analizar simultáneamente la técnica individual, la coordinación grupal y el uso del espacio.\n",
        "\n",
        "Falta de herramientas digitales específicas para danza: existen aplicaciones de análisis de movimiento en deportes o rehabilitación, pero pocas enfocadas en las necesidades de la composición coreográfica.\n",
        "\n",
        "Limitada retroalimentación cuantitativa: la mayoría de las observaciones son cualitativas; faltan métricas visualizables que permitan al coreógrafo detectar patrones o desequilibrios.\n",
        "\n",
        "El asistente coreográfico propuesto busca dar respuesta a estas limitaciones mediante:\n",
        "- la captura y análisis automático de movimiento;\n",
        "- la transformación de datos en sugerencias útiles;\n",
        "- la complementariedad entre la intuición artística y el análisis objetivo.\n",
        "\n",
        "Este proyecto se alinea con la tendencia emergente de integrar la inteligencia artificial en las artes escénicas, fomentando nuevas formas de creación y colaboración entre el arte y la tecnología.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4veVfImiosKK"
      },
      "source": [
        "# **DEPENDENCIAS Y UTILIDADES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tUtVAy_qtlSV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d89c22f-c2b5-4669-a88e-174be47f90ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "q15I1i6guCjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f6727e-3573-4488-c56a-d8bd765c6f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "parallel is already the newest version (20210822+ds-2).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Sistema (ffmpeg para vídeo; parallel útil en batch)\n",
        "!apt-get -yq update\n",
        "!apt-get -yq install ffmpeg parallel\n",
        "\n",
        "# Paquetes Python esenciales para esta notebook\n",
        "%pip -q install --upgrade pandas scipy scikit-learn tqdm matplotlib \\\n",
        "                         opencv-python-headless ultralytics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "qmNj9iTRhhGM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a0df8fa-bd45-4910-bf9b-96d799623adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.5)\n",
            "Collecting numpy<2 (from mediapipe)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python (from mediapipe)\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, numpy, sounddevice, opencv-contrib-python, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.6\n",
            "    Uninstalling numpy-2.2.6:\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "    Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.21 numpy-1.26.4 opencv-contrib-python-4.11.0.86 protobuf-4.25.8 sounddevice-0.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "3d53202ca89c4de4addc5cab71aca29b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS3cKJdQnSD4"
      },
      "source": [
        "# **Búsqueda y Recopilación de Datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "46d3a68c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d990c41-e87b-43ac-ae52-20f87be5f39b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Estructura creada:\n",
            "BASE: /content/drive/MyDrive/asistente_coreografico\n",
            "RAW: /content/drive/MyDrive/asistente_coreografico/data/raw/aistpp\n",
            "PROCESSED: /content/drive/MyDrive/asistente_coreografico/data/processed/aistpp\n",
            "ANN: /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp\n",
            "LOGS: /content/drive/MyDrive/asistente_coreografico/logs\n"
          ]
        }
      ],
      "source": [
        "#  MONTAMOS GOOGLE DRIVE Y CREAMOS ESTRUCTURA DE CARPETA\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ROOT = pathlib.Path(\"/content/drive/MyDrive/asistente_coreografico\")\n",
        "else:\n",
        "    ROOT = pathlib.Path.home() / \"asistente_coreografico\"\n",
        "\n",
        "# Definición de rutas usando pathlib para mejor compatibilidad\n",
        "BASE = ROOT\n",
        "RAW = BASE / \"data/raw/aistpp\"\n",
        "PROCESSED = BASE / \"data/processed/aistpp\"\n",
        "ANN = BASE / \"data/annotations/aistpp\"\n",
        "LOGS = BASE / \"logs\"\n",
        "\n",
        "# Crear directorios\n",
        "for directory in [BASE, RAW, PROCESSED, ANN, LOGS]:\n",
        "    directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Estructura creada:\")\n",
        "print(f\"BASE: {BASE}\")\n",
        "print(f\"RAW: {RAW}\")\n",
        "print(f\"PROCESSED: {PROCESSED}\")\n",
        "print(f\"ANN: {ANN}\")\n",
        "print(f\"LOGS: {LOGS}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VfHZO4fFuJh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777aa5ab-7619-42e9-da11-c3db9f66bf35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constantes inicializadas. COCO_EDGES: 12 aristas\n"
          ]
        }
      ],
      "source": [
        "# === Imports globales ===\n",
        "import os, sys, glob, json, math, shutil, zipfile, itertools, pathlib\n",
        "from typing import Optional, Tuple, List, Dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "# === Constantes ===\n",
        "# COCO-17: índices estándar usados por YOLOv8/MMPose (pose de 17 puntos)\n",
        "COCO_EDGES = [\n",
        "    (5,7),(7,9),   # brazo izquierdo: hombro->codo->muñeca\n",
        "    (6,8),(8,10),  # brazo derecho\n",
        "    (11,13),(13,15), # pierna izq: cadera->rodilla->tobillo\n",
        "    (12,14),(14,16), # pierna dcha\n",
        "    (5,6), (11,12),  # hombros, caderas\n",
        "    (5,11), (6,12)   # hombro- cadera (lado)\n",
        "]\n",
        "\n",
        "# Rutas del proyecto\n",
        "BASE = ROOT\n",
        "RAW = f\"{BASE}/data/raw/aistpp\"\n",
        "PROCESSED = f\"{BASE}/data/processed/aistpp\"\n",
        "ANN = f\"{BASE}/data/annotations/aistpp\"\n",
        "LOGS = f\"{BASE}/logs\"\n",
        "\n",
        "print(\"Constantes inicializadas. COCO_EDGES:\", len(COCO_EDGES), \"aristas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descarga y extracción de AIST++"
      ],
      "metadata": {
        "id": "5r6DNu6YM_X3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9JESejjting3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69b3033-ddf7-4e45-e18d-d4ccb5cdaf2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AIST++ ya presente en: /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp\n"
          ]
        }
      ],
      "source": [
        "# === Descarga y extracción de AIST++  ===\n",
        "ZIP_URL = \"https://storage.googleapis.com/aist_plusplus_public/20210308/fullset.zip\"\n",
        "ZIP_PATH = f\"{ANN}/fullset.zip\"\n",
        "\n",
        "# ¿Ya está descomprimido?\n",
        "already = list(pathlib.Path(ANN).rglob(\"aist_plusplus_final\"))\n",
        "if not already:\n",
        "    if not os.path.exists(ZIP_PATH) or os.path.getsize(ZIP_PATH) < 10_000_000:\n",
        "        print(\"Descargando AIST++ annotations...\")\n",
        "        !wget -q -O \"$ZIP_PATH\" \"$ZIP_URL\"\n",
        "        assert os.path.exists(ZIP_PATH), \"No se descargó el ZIP.\"\n",
        "\n",
        "    print(\"Descomprimiendo...\")\n",
        "    !unzip -q -o \"$ZIP_PATH\" -d \"$ANN\"\n",
        "    print(\"OK. Carpeta:\", ANN)\n",
        "else:\n",
        "    print(\"AIST++ ya presente en:\", ANN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y0VTMwl4scgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f1a618-7628-4df6-9cd7-076e5604c82c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carpeta de anotaciones: /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp\n",
            "Ejemplo de archivos:\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/fullset.zip\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/ignore_list.txt\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting5.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting10_1.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting7_1.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting9_1.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting2.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting10_2.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting6.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting1.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting4.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting7_2.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/mapping.txt\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting9_2.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting3.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting8_2.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/cameras/setting8_1.json\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gJB_sBM_cAll_d09_mJB4_ch04.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gLO_sBM_cAll_d15_mLO2_ch09.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gKR_sBM_cAll_d30_mKR4_ch07.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gLH_sBM_cAll_d16_mLH0_ch04.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gBR_sBM_cAll_d06_mBR5_ch10.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gWA_sBM_cAll_d25_mWA2_ch08.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gBR_sBM_cAll_d06_mBR3_ch03.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gKR_sBM_cAll_d30_mKR3_ch08.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gKR_sBM_cAll_d30_mKR3_ch03.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gBR_sBM_cAll_d05_mBR4_ch03.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gJB_sBM_cAll_d08_mJB0_ch10.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gJB_sBM_cAll_d08_mJB1_ch03.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gLH_sBM_cAll_d16_mLH2_ch05.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gKR_sBM_cAll_d30_mKR2_ch07.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gHO_sFM_cAll_d20_mHO3_ch11.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gLO_sBM_cAll_d13_mLO0_ch07.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gLO_sBM_cAll_d14_mLO5_ch06.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gLH_sBM_cAll_d18_mLH4_ch03.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gLH_sBM_cAll_d17_mLH5_ch09.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gHO_sBM_cAll_d20_mHO5_ch07.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gHO_sBM_cAll_d19_mHO1_ch04.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gPO_sBM_cAll_d10_mPO3_ch03.pkl\n",
            " - /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gPO_sBM_cAll_d11_mPO5_ch01.pkl\n",
            "\n",
            "Conteo por extensión:\n",
            "{'.pkl': 4326, '.json': 14, '.txt': 9, '.zip': 1}\n"
          ]
        }
      ],
      "source": [
        "#Chequeamos la creación y descarga.\n",
        "\n",
        "import os, glob, json, zipfile, pathlib\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "ANN  = f\"{BASE}/data/annotations/aistpp\"\n",
        "\n",
        "def tree(root, max_items=200):\n",
        "    files = []\n",
        "    for p in pathlib.Path(root).rglob(\"*\"):\n",
        "        if p.is_file(): files.append(str(p))\n",
        "        if len(files) >= max_items: break\n",
        "    return files\n",
        "\n",
        "print(\"Carpeta de anotaciones:\", ANN)\n",
        "print(\"Ejemplo de archivos:\")\n",
        "for p in tree(ANN, max_items=40):\n",
        "    print(\" -\", p)\n",
        "\n",
        "# Conteos por tipo\n",
        "def count_by_ext(root):\n",
        "    exts = {}\n",
        "    for p in pathlib.Path(root).rglob(\"*\"):\n",
        "        if p.is_file():\n",
        "            exts[p.suffix.lower()] = exts.get(p.suffix.lower(), 0) + 1\n",
        "    return dict(sorted(exts.items(), key=lambda x: -x[1]))\n",
        "\n",
        "print(\"\\nConteo por extensión:\")\n",
        "print(count_by_ext(ANN))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1RBXt_4c2QN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98462718-f87b-44df-80eb-08fafb919bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Buscando archivos en: /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints2d\n",
            " Encontrado: /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints2d/gBR_sBM_cAll_d04_mBR0_ch01.pkl  (patrón: **/*.pkl)\n",
            " load_any: 0.881s | tipo: <class 'dict'>\n",
            " to_TJD: 0.000s | shape: None\n",
            " No se pudo convertir a (T,J,D). Revisa el contenido del archivo.\n",
            " Claves disponibles: ['keypoints2d', 'det_scores', 'timestamps'] \n"
          ]
        }
      ],
      "source": [
        "# === Utilidades para keypoints 2D/3D ===\n",
        "\n",
        "def find_first_glob(root: str, pattern: str) -> Optional[str]:\n",
        "    rootp = pathlib.Path(root)\n",
        "    m = sorted(rootp.rglob(pattern))\n",
        "    return str(m[0]) if m else None\n",
        "\n",
        "def load_any(path: str):\n",
        "    suf = pathlib.Path(path).suffix.lower()\n",
        "    if suf in (\".pkl\", \".pickle\"):\n",
        "        import pickle\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    if suf == \".npz\":\n",
        "        data = np.load(path, allow_pickle=True)\n",
        "        return {k: data[k] for k in data.files}\n",
        "    if suf == \".npy\":\n",
        "        return np.load(path, allow_pickle=True)\n",
        "    raise ValueError(f\"Formato no soportado: {suf} ({path})\")\n",
        "\n",
        "def to_TJD(obj) -> Optional[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Convierte estructuras comunes a un ndarray (T, J, D) float32, D in {2,3}.\n",
        "    Devuelve None si no puede inferirse.\n",
        "    \"\"\"\n",
        "    if obj is None:\n",
        "        return None\n",
        "    # Casos tipo dict con clave 'keypoints' o similar\n",
        "    if isinstance(obj, dict):\n",
        "        for k in (\"keypoints\", \"poses\", \"kpts\", \"arr\", \"data\"):\n",
        "            if k in obj:\n",
        "                arr = np.asarray(obj[k])\n",
        "                break\n",
        "        else:\n",
        "            # si no hay claves típicas, prueba cualquier valor ndarray\n",
        "            for v in obj.values():\n",
        "                if isinstance(v, np.ndarray) and v.ndim >= 2:\n",
        "                    arr = np.asarray(v); break\n",
        "            else:\n",
        "                return None\n",
        "    else:\n",
        "        arr = np.asarray(obj)\n",
        "\n",
        "    arr = np.asarray(arr)\n",
        "    # Si viene (N, J*D) → reshape\n",
        "    if arr.ndim == 2 and arr.shape[1] in (17*2, 17*3):\n",
        "        D = 2 if arr.shape[1] == 34 else 3\n",
        "        J = 17\n",
        "        T = arr.shape[0]\n",
        "        arr = arr.reshape(T, J, D)\n",
        "    # Si viene (T, J, D)\n",
        "    if arr.ndim == 3 and arr.shape[-1] in (2,3):\n",
        "        return arr.astype(np.float32)\n",
        "    return None\n",
        "\n",
        "def interpolate_nan(K: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Interplola NaNs por columna en cada joint-dim independiente.\"\"\"\n",
        "    K = K.copy()\n",
        "    T, J, D = K.shape\n",
        "    for j in range(J):\n",
        "        for d in range(D):\n",
        "            v = K[:, j, d]\n",
        "            nans = ~np.isfinite(v)\n",
        "            if nans.any():\n",
        "                t = np.arange(T)\n",
        "                if (~nans).sum() >= 2:\n",
        "                    K[:, j, d] = np.interp(t, t[~nans], v[~nans])\n",
        "                else:\n",
        "                    # si casi todo es NaN, rellena con 0\n",
        "                    K[:, j, d] = 0.0\n",
        "    return K\n",
        "\n",
        "def normalize_2d(K: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Normaliza 2D por traslación (centro de masa aproximado) y escala (distancia hombros).\n",
        "    Devuelve (T,J,2).\n",
        "    \"\"\"\n",
        "    K = K.copy().astype(np.float32)\n",
        "    # centro de masa aprox = media de caderas (11,12) si existen; si no, media global\n",
        "    if K.shape[1] >= 13:\n",
        "        com = np.nanmean(K[:, [11,12], :], axis=1)\n",
        "    else:\n",
        "        com = np.nanmean(K, axis=1)\n",
        "    K = K - com[:, None, :]\n",
        "\n",
        "    # escala: distancia entre hombros (5 y 6) si existen; si no, mediana de distancias\n",
        "    if K.shape[1] >= 7:\n",
        "        shoulder_dist = np.linalg.norm(K[:,5,:] - K[:,6,:], axis=1)\n",
        "        s = np.nanmedian(shoulder_dist)\n",
        "    else:\n",
        "        # mediana de distancias a COM\n",
        "        s = np.nanmedian(np.linalg.norm(K, axis=2))\n",
        "    if not np.isfinite(s) or s <= 1e-6:\n",
        "        s = 1.0\n",
        "    K /= s\n",
        "    return K\n",
        "\n",
        "def plot_skeleton_2d(frame: np.ndarray, title=\"\"):\n",
        "    xs, ys = frame[:,0], frame[:,1]\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.scatter(xs, ys, s=12)\n",
        "    for a,b in COCO_EDGES:\n",
        "        if a < len(frame) and b < len(frame):\n",
        "            plt.plot([xs[a], xs[b]], [ys[a], ys[b]])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.axis(\"equal\");\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "import os, time, pathlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Si no existiera COCO_EDGES en tu entorno, lo definimos para la visualización\n",
        "try:\n",
        "    COCO_EDGES\n",
        "except NameError:\n",
        "    COCO_EDGES = [\n",
        "        (5,7),(7,9),(6,8),(8,10),(11,13),(13,15),\n",
        "        (12,14),(14,16),(5,6),(11,12),(5,11),(6,12)\n",
        "    ]\n",
        "\n",
        "# ---------- Helpers de demo (usan TUS funciones) ----------\n",
        "def pick_best_frame(K2: np.ndarray) -> int:\n",
        "    \"\"\"Elige el frame con más puntos finitos para visualizar.\"\"\"\n",
        "    if K2.ndim != 2 or K2.shape[1] < 2:\n",
        "        return 0\n",
        "    # K2 es (J,2) en un frame; cuando venga la secuencia usamos fuera\n",
        "    return 0\n",
        "\n",
        "def pick_best_frame_seq(K2_seq: np.ndarray) -> int:\n",
        "    \"\"\"Para una secuencia (T,J,2), elige el frame con más joints válidos.\"\"\"\n",
        "    T = K2_seq.shape[0]\n",
        "    fin = np.isfinite(K2_seq[:,:,0]) & np.isfinite(K2_seq[:,:,1])\n",
        "    cnt = fin.sum(axis=1)\n",
        "    return int(np.argmax(cnt)) if T else 0\n",
        "\n",
        "def demo_keypoints_debug(ROOT: str):\n",
        "    print(f\" Buscando archivos en: {ROOT}\")\n",
        "    patterns = [\"**/*.npz\", \"**/*.npy\", \"**/*.pkl\", \"**/*.pickle\"]\n",
        "    path = None\n",
        "    for pat in patterns:\n",
        "        path = find_first_glob(ROOT, pat)\n",
        "        if path:\n",
        "            print(f\" Encontrado: {path}  (patrón: {pat})\")\n",
        "            break\n",
        "    if not path:\n",
        "        print(\" No se encontró ningún archivo de keypoints en la ruta dada.\")\n",
        "        return\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    obj = load_any(path)\n",
        "    print(f\" load_any: {time.perf_counter()-t0:.3f}s | tipo: {type(obj)}\")\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    K = to_TJD(obj)\n",
        "    print(f\" to_TJD: {time.perf_counter()-t0:.3f}s | shape: {None if K is None else K.shape}\")\n",
        "\n",
        "    if K is None:\n",
        "        print(\" No se pudo convertir a (T,J,D). Revisa el contenido del archivo.\")\n",
        "        if isinstance(obj, dict):\n",
        "            keys = list(obj.keys())\n",
        "            print(\" Claves disponibles:\", keys[:10], \"...\" if len(keys)>10 else \"\")\n",
        "        return\n",
        "\n",
        "    # Estadísticas básicas\n",
        "    T,J,D = K.shape\n",
        "    n_nan = int(np.isnan(K).sum())\n",
        "    print(f\"Frames: {T} | Joints: {J} | Dims: {D} | NaNs totales: {n_nan}\")\n",
        "\n",
        "    # Interpolación de NaNs (tu función)\n",
        "    Kc = interpolate_nan(K)\n",
        "    print(\" Interpolación de NaNs aplicada.\")\n",
        "\n",
        "    # Normalización y visualización 2D si hay al menos 2D\n",
        "    if D >= 2:\n",
        "        K2_seq = normalize_2d(Kc[...,:2])  # (T,J,2) normalizado\n",
        "        f = pick_best_frame_seq(K2_seq)\n",
        "        print(f\" Visualizando frame #{f} (mayor nº de puntos válidos).\")\n",
        "        plot_skeleton_2d(K2_seq[f], title=f\"Frame {f}\")\n",
        "    else:\n",
        "        print(\"ℹ Datos con D<2; se omite visualización de esqueleto.\")\n",
        "\n",
        "# ==== EJECUTAR DEMO ====\n",
        "ROOT = \"/content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints2d\"\n",
        "demo_keypoints_debug(ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1BZxBBzxJ3_A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "b6e43668-7c0c-4a99-928c-920285ffa521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detectado 2D: /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints2d/gBR_sBM_cAll_d04_mBR0_ch01.pkl\n",
            "Detectado 3D: /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gBR_sBM_cAll_d04_mBR0_ch01.pkl\n",
            "K2 shape: None\n",
            "K3 shape: (720, 17, 3)\n",
            "Joints 3D usados: 17 de 17\n",
            "Métricas 3D (limpias): {'amp_media_por_eje': [57.23964309692383, 28.56692886352539, 56.94464111328125], 'vel_media': 1.387666940689087, 'simetria_indice': 0.27437493205070496}\n",
            "K2 generado por proyección de K3 (x,y). Shape: (720, 17, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAF2CAYAAACRaQhmAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAU+FJREFUeJzt3Xd4VFX+x/H3zGTSSGZCOoEkNKUIAlJCELFRBKSJq7gUC8qqgAsqIrv2hrqusiJl3VVYf8C6yyqIoGiIFBVQQUPvBAiENEJmSJ9yfn+EDAwJJWGSKfm+nmceyW3zvZnk4825556jUUophBBC+AStuwsQQgjhOhLqQgjhQyTUhRDCh0ioCyGED5FQF0IIHyKhLoQQPkRCXQghfIiEuhBC+BAJdSGE8CES6qLB0Wg0vPTSS46vFy5ciEaj4ciRIy59n7fffpu2bdtit9tdetyaWLduHRqNhnXr1jmWPfDAAzRv3txj6rlSzz77LElJSa4vysdIqHuQynCp7vXss8+6uzxRA2azmbfeeovp06ej1Vb8mp06dYq//OUv9OnTh6ioKMLCwujZsyf/+c9/qux/4c9CYGAgcXFxDBgwgPfff58zZ87U9ym53ZQpU9i2bRsrVqxwdykezc/dBYiqXnnlFVq0aOG0rEOHDm6qxveNHTuWUaNGERAQ4LJjfvzxx1itVu677z7Hsk2bNvHnP/+ZQYMG8dxzz+Hn58dnn33GqFGj2L17Ny+//HKV41T+LFgsFrKysli3bh1Tpkzh3XffZcWKFVx//fU1ru0f//iH2/566NOnDyUlJfj7+9d439jYWIYNG8Y777zD0KFD66A6H6GEx1iwYIEC1C+//HLF+5SUlCibzVaHVfkeQL344ot1+h7XX3+9GjNmjNOyw4cPqyNHjjgts9vt6rbbblMBAQGqsLDQsfxSPwupqakqKChIJSYmquLi4kvWsXbtWgWotWvX1v5kPMj//vc/pdFo1KFDh9xdiseS5hcvUtke+emnn/Lcc8/RtGlTgoODMZvN5Ofn8/TTT9OxY0dCQkIwGAwMHDiQbdu2VXuM//73v7z88ss0bdqU0NBQ7r77bkwmE2VlZUyZMoXo6GhCQkJ48MEHKSsrq1LLokWL6Nq1K0FBQYSHhzNq1CgyMjIuew4vvfQSGo2G/fv3M2bMGIxGI1FRUTz//PMopcjIyGDYsGEYDAZiY2P561//WuUYZWVlvPjii7Ru3ZqAgADi4+N55plnqtRZVlbG1KlTiYqKIjQ0lKFDh3L8+PEqx7tYm/rcuXO57rrrCAgIIC4ujokTJ1JQUHDZc0xPT2f79u307dvXaXmLFi1ITEx0WqbRaBg+fDhlZWUcPnz4sscGuO2223j++ec5evQoixYtuqJ9zndhm/qRI0fQaDS88847zJkzh5YtWxIcHEz//v3JyMhAKcWrr75Ks2bNCAoKYtiwYeTn5zsds3nz5tx55518++23dO7cmcDAQNq3b8/nn3/utN3F2tSXLl3q+HmKjIxkzJgxnDhxokrtld/TL774osbn3VBIqHsgk8lEXl6e0+t8r776KqtWreLpp5/mjTfewN/fn8OHD7N8+XLuvPNO3n33XaZNm8aOHTu4+eabyczMrPIeM2fO5JtvvuHZZ5/loYce4vPPP+fRRx/loYceYv/+/bz00kvcddddLFy4kLfeestp39dff51x48ZxzTXX8O677zJlyhRSU1Pp06fPFYUewL333ovdbufNN98kKSmJ1157jVmzZtGvXz+aNm3KW2+9RevWrXn66afZsGGDYz+73c7QoUN55513GDJkCLNnz2b48OG899573HvvvU7v8fDDDzNr1iz69+/Pm2++iV6vZ/DgwVdU30svvcTEiROJi4vjr3/9KyNHjuTvf/87/fv3x2KxXHLfjRs3AnDDDTdc0XtlZWUBEBkZeUXbQ0WTEcC33357xftczuLFi5k7dy6TJ0/mqaeeYv369dxzzz0899xzrF69munTpzNhwgS+/PJLnn766Sr7HzhwgHvvvZeBAwcyc+ZM/Pz8+N3vfkdKSsol33fhwoXcc8896HQ6Zs6cySOPPMLnn39O7969q/w8GY1GWrVqxY8//uiy8/Y57v5TQZxT+Sd3dS+lzv0p3bJlyyp/dpeWllZphklPT1cBAQHqlVdecSyrPEaHDh1UeXm5Y/l9992nNBqNGjhwoNMxkpOTVWJiouPrI0eOKJ1Op15//XWn7Xbs2KH8/PyqLL/Qiy++qAA1YcIExzKr1aqaNWumNBqNevPNNx3LT58+rYKCgtT999/vWPZ///d/SqvVqu+//97puPPnz1eA+vHHH5VSSqWlpSlAPf74407b/f73v6/S/FL5fU9PT1dKKZWTk6P8/f1V//79nb6nH3zwgQLUxx9/fMlzfO655xSgzpw5c8ntlFLq1KlTKjo6Wt10001Oy6+kKc5oNKouXbpc8vjVNb/cf//9Tp9penq6AlRUVJQqKChwLJ8xY4YCVKdOnZTFYnEsv++++5S/v78qLS11LEtMTFSA+uyzzxzLTCaTatKkiVONF9ZTXl6uoqOjVYcOHVRJSYlju5UrVypAvfDCC1XOqX///qpdu3aXPO+GTK7UPdCcOXNISUlxep3v/vvvJygoyGlZQECAo5eFzWbj1KlThISE0KZNG3799dcq7zFu3Dj0er3j66SkJJRSPPTQQ07bJSUlkZGRgdVqBeDzzz/Hbrdzzz33OP0lERsbyzXXXMPatWuv6Bwffvhhx791Oh3dunVDKcX48eMdy8PCwmjTpo1Ts8TSpUtp164dbdu2dXr/2267DcDx/l999RUATzzxhNP7Tpky5bK1rVmzhvLycqZMmeL4ngI88sgjGAwGVq1adcn9T506hZ+fHyEhIZfczm63M3r0aAoKCpg9e/Zl67pQSEiIS3vB/O53v8NoNDq+ruw+OGbMGPz8/JyWl5eXV2keiYuLY8SIEY6vDQYD48aN47fffnP8NXKhLVu2kJOTw+OPP05gYKBj+eDBg2nbtm213+vGjRtX+etVnCO9XzxQjx496Nat20XXX9gzBioC4m9/+xtz584lPT0dm83mWBcREVFl+4SEBKevK3+Z4+Pjqyy32+2YTCYiIiI4cOAASimuueaaams7/38Ul1Ld+wcGBlZpgjAajZw6dcrx9YEDB9izZw9RUVHVHjcnJweAo0ePotVqadWqldP6Nm3aXLa2o0ePVrutv78/LVu2dKy/WpMnT2b16tV88skndOrUqcb7FxYWEh0d7ZJaoGY/EwCnT592Wt66dWs0Go3TsmuvvRaoaLePjY2t8p4X+14DtG3blh9++KHKcqVUlfcR50ioe6ELr9IB3njjDZ5//nkeeughXn31VcLDw9FqtUyZMqXa7ms6na7aY19suTo766Hdbkej0fD1119Xu+3lrk4v9T6Xe+/K9+/YsSPvvvtutdteGEDuEBERgdVq5cyZM4SGhla7zcsvv8zcuXN58803He3jNXH8+HFMJhOtW7e+2nIdavszUd9Onz5do/sPDY2Euo/43//+x6233spHH33ktLygoMClvwCtWrVCKUWLFi0cV2H1qVWrVmzbto3bb7/9kldriYmJ2O12Dh065HQVuG/fvsu+R2UPlX379tGyZUvH8vLyctLT06v0arlQ27ZtgYpeMNX1I58zZw4vvfQSU6ZMYfr06Zetpzr/93//B8CAAQNqtX9dOHjwYJWr6P379wNc9AnW87/XlU1olfbt21eltxBUfF9r85dNQyFt6j5Cp9NVuXJaunRptd3CrsZdd92FTqfj5ZdfrvJ+SimnppK6cM8993DixAn+8Y9/VFlXUlJCUVERAAMHDgTg/fffd9pm1qxZl32Pvn374u/vz/vvv+90jh999BEmk+myPWiSk5OBivbiC/3nP//hiSeeYPTo0Rf9a+NyvvvuO1599VVatGjB6NGja3WMupCZmcmyZcscX5vNZj755BM6d+5cbdMLQLdu3YiOjmb+/PlOXVK//vpr9uzZU+V7bTKZOHToEL169aqbk/ABcqXuI+68805eeeUVHnzwQXr16sWOHTtYvHix05WmK7Rq1YrXXnuNGTNmcOTIEYYPH05oaCjp6eksW7aMCRMmVNvdzVXGjh3Lf//7Xx599FHWrl3LjTfeiM1mY+/evfz3v//lm2++oVu3bnTu3Jn77ruPuXPnYjKZ6NWrF6mpqRw8ePCy7xEVFcWMGTN4+eWXueOOOxg6dCj79u1j7ty5dO/enTFjxlxy/5YtW9KhQwfWrFnjdOP5559/Zty4cURERHD77bezePFip/169epV5fP6+uuv2bt3L1arlezsbL777jtSUlJITExkxYoVTjcX3e3aa69l/Pjx/PLLL8TExPDxxx+TnZ3NggULLrqPXq/nrbfe4sEHH+Tmm2/mvvvuIzs7m7/97W80b96cqVOnOm2/Zs0alFIMGzasrk/Ha0mo+4g//elPFBUVsWTJEv7zn/9www03sGrVqjoZM+bZZ5/l2muv5b333nM82h4fH0///v3r/PFtrVbL8uXLee+99/jkk09YtmwZwcHBtGzZkj/+8Y9OTUIff/wxUVFRLF68mOXLl3PbbbexatWqK2p3f+mll4iKiuKDDz5g6tSphIeHM2HCBN54440ruhn80EMP8cILL1BSUuK4B7J7927Ky8vJzc2t0ssIYMGCBVVC/YUXXgAqbtKGh4fTsWNHZs2axYMPPnjR9np3ueaaa5g9ezbTpk1j3759tGjRgv/85z+XbSJ64IEHCA4O5s0332T69Ok0atSIESNG8NZbbxEWFua07dKlS+ndu3eVG+DiHI1y190OIXyYyWSiZcuWvP32207dNH1V8+bN6dChAytXrqyz98jKyqJFixZ8+umncqV+CdKmLkQdMBqNPPPMM/zlL39x69C7vmTWrFl07NhRAv0y5EpdCHHV6uNKXVwZuVIXQggfIlfqQgjhQ+RKXQghfIiEuhBC+BCv7Kdut9vJzMwkNDRUBvYRQvgEpRRnzpwhLi7OaXTQmvLKUM/MzPSIgZuEEMLVMjIyaNasWa3398pQr3ySLiMjA4PB4OZqhBDi6pnNZuLj46/6SWGvDPXKJheDwSChLoTwKVfbpCw3SoUQwodIqAshhA+RUBdCCB8ioS6EED5EQl0IIXyIhLoQQvgQCXUhhPAhEupCCOFDvPLhI+G7Si02Fm0+SkZ+MfHhwYzpmUigXufusoTwGhLqwmOUWmyMnLeR3SfN+KGwomHZbyf47LFeEuxCXCFpfhEeY9HmoxzLOEG/7BSGnliOsit2nzSzaPNRd5cmhNeQUBceIyO/GDR+tCpOJ64si7iyk+g0morlQogrIqEuPEZ8eDCFuiD2hrQBoIspDZtSxIcHu7kyIbyHhLrwGGN6JtK+iYE0YycU0LL4KN2M5Yzpmeju0oTwGhLqwmME6nV89lgvJg5PxtrsOgAeCD0iN0mFqAEJdeFRAvU6Hr6pJeP+8CAA+39cz5n8PDdXJYT3kFAXHinu2rY0bXsddpuV377+8qqPV2qx8c/vD/PiFzv55/eHKbXYXFClEJ5H+qkLj9V96F2c2LuLbSlfkzTiHgKCG9XqOOf3f9dpNNiUkv7vwmfJlbrwWC27dCe8aTzlJcVsX7O61sdZtPkoh07mc6vfAeLIRynp/y58l4S68FgarZbuQ+4C4NevvsBmtdTqOBn5xbTSnSJBV0Anv0xAI/3fhc+SUBcerW3vWwhpHE7h6Xz2/LC+VseIDw+mpbbiZusBWySA9H8XPsvloW6z2Xj++edp0aIFQUFBtGrVildffRWllGMbpRQvvPACTZo0ISgoiL59+3LgwAFXlyJ8gJ9eT5eBQwHY8uXnKLu9xsfo29yfCG0xNqXhmIpAo4H2TQzS/134JJeH+ltvvcW8efP44IMP2LNnD2+99RZvv/02s2fPdmzz9ttv8/777zN//nx++uknGjVqxIABAygtLXV1OcIHdOo3EP+gIE4dP8bh37bUeP/dO7YDEBKTwO+SWvHnQe3kJqnwWS7v/bJx40aGDRvG4MGDAWjevDn//ve/+fnnn4GKq/RZs2bx3HPPMWzYMAA++eQTYmJiWL58OaNGjXJ1ScLLBQQ34vq+A9ny5ef8suIzWnXtccX7Wq1WduzYAcBd/XpzzTXX1FWZQngEl1+p9+rVi9TUVPbv3w/Atm3b+OGHHxg4cCAA6enpZGVl0bdvX8c+RqORpKQkNm3aVO0xy8rKMJvNTi/RsNwwaChanR8n9u4ic//eK95v3759lJSUEBoaSqtWreqwQiE8g8tD/dlnn2XUqFG0bdsWvV5Ply5dmDJlCqNHjwYgKysLgJiYGKf9YmJiHOsuNHPmTIxGo+MVHx/v6rKFhwsNj6Rd71uAirb1K/Xbb78B0LlzZ7Ra6RcgfJ/Lf8r/+9//snjxYpYsWcKvv/7Kv/71L9555x3+9a9/1fqYM2bMwGQyOV4ZGRkurFh4i25DRgBw4JdN5GeeuOz2JpOJQ4cOARWhLkRD4PJQnzZtmuNqvWPHjowdO5apU6cyc+ZMAGJjYwHIzs522i87O9ux7kIBAQEYDAanl2h4IuMTaXlDd1CKrSuXXXb7bdu2oZQiISGBiIiIeqhQCPdzeagXFxdX+TNXp9NhP9sVrUWLFsTGxpKamupYbzab+emnn0hOTnZ1OcLHdB8yEoBdG1IpKjh90e2UUqSlpQHQpUuX+ihNCI/g8lAfMmQIr7/+OqtWreLIkSMsW7aMd999lxEjKv501mg0TJkyhddee40VK1awY8cOxo0bR1xcHMOHD3d1OcLHNG13HU1at8FmsfDb6pUX3e7YsWPk5+ej1+tp3759PVYohHu5PNRnz57N3XffzeOPP067du14+umn+cMf/sCrr77q2OaZZ55h8uTJTJgwge7du1NYWMjq1asJDAx0dTnCx2g0GroPrbhaT/t2JeWlJdVuV3mDtEOHDgQEBNRbfUK4m0ad/6inlzCbzRiNRkwmk7SvN0B2u42FTz7G6ZOZ3DLuEboOHua0vqysjHfeeQeLxcKDDz5IYqI8OSo8n6tyTfp4Ca+j1eroOriiOW/rV8uxWa1O63fv3o3FYiE8PJyEhAR3lCiE20ioC6/U/ubbCDaGcSYvl/2bvndaV9n00qVLFzQajTvKE8JtJNSFV9L7B9DljiEA/LLiM8eAcXl5eRw7dgyNRkOnTp3cWaIQbiGhLrxWp/6D0AcEknvsCEe3V1ydV3ZjbNWqldxvEQ2ShLrwWkEhoXS8rT9QcbVut9vZtm0bIH3TRcMloS68WtfBw9FotRzbuY0tP37PmTNnCAoKok2bNu4uTQi3kFAXXs0QFU3bXn0A2LShYmak66+/Hj8/mVNdNEwS6sLrdRtyF3adH6fLK7o2StOLaMgk1IXXi27eEkPb60GjpZHe76IDwwnREEioC59QFtoYAHvmMYrNJjdXI4T7SKgLr3fy5EnyTSZQCl1+NmnfrHJ3SUK4jYS68HqVT5DGx8agsdtI+2YlljKZxFw0TBLqwqudP7H0TbfdjiEqhpIzZnatS73MnkL4Jgl14dXOn1i69TXX0O3O4QBsWbUMu83m3uKEcAMJdeHVLpxYusMt/QgMNWDKzuLAzxvdXJ0Q9U9CXXit6iaW1gcG0mXAYMB5oC8hGgoJdeG1KieWTkxMdJpYuvOAO/HT+5N9+CAZu3a4sUIh6p+EuvBK508sXXmVXinYYOS6W/sB8MuXn9VzZUK4l4S68EqVE0v7+/tXO7F0t8HD0Wi0HEnbSu7RdDdUKIR7SKgLr1R5g/S6666rdmLpsNgmXNPzRgC2fPl5vdYmhDtJqAuvU1ZWxq5du4BLD97VfchdAOzduAFzXk691CaEu0moC69TObF0REQE8fHxF90uttU1xF93PXabjV+/+qIeKxTCfSTUhdc5v2/65SaW7j50JADb13xDaWFhndcmhLtJqAuvUtOJpZt3uoHIhOZYykrZlvJVPVQohHtJqAuvUtmNsXXr1lc0sbRGo3G0rf/69Qqs5eV1WZ4QbiehLrzG+RNLX9g3/VLa9OpDaEQUxaYCdn//XR1VJ4RnkFAXXuPQoUO1mlha5+dH18HDANjy5TKU3V5XJQrhdhLqwmtU3iCtzcTSHW/rT0BwI06fPMHBrT/VRXlCeIQah/qGDRsYMmQIcXFxaDQali9f7rReKcULL7xAkyZNCAoKom/fvhw4cMBpm/z8fEaPHo3BYCAsLIzx48dTKD0TxCUUFRWxd+9eoHYTS/sHBdOp/yAAfvnifzLQl/BZNQ71oqIiOnXqxJw5c6pd//bbb/P+++8zf/58fvrpJxo1asSAAQMoLT03E83o0aPZtWsXKSkprFy5kg0bNjBhwoTan4XweTt27MBut9OkSZNaTyx9w8Ch6Pz8OHlgHyf27XZxhUJ4hpr9DQsMHDiQgQMHVrtOKcWsWbN47rnnGDasog3zk08+ISYmhuXLlzNq1Cj27NnD6tWr+eWXX+jWrRsAs2fPZtCgQbzzzjvExcVdxekIX3WxwbtqolFYY9rffDs7Ur/hlxWf0aztda4pTggP4tI29fT0dLKysujbt69jmdFoJCkpiU2bNgGwadMmwsLCHIEO0LdvX7RaLT/9VH1bZ1lZGWaz2eklGo6TJ0+SlZWFTqejY8eOV3WsbneOAI2Gw1t/5tTxDBdVKITncGmoZ2VlARATE+O0PCYmxrEuKyuL6Ohop/V+fn6Eh4c7trnQzJkzMRqNjtelHg0XvqXUYmPxqvUA+Ec0Q6uvOnhXTYTHNaN1tyQAtqyUgb6E7/GK3i8zZszAZDI5XhkZcoXVEJRabDw8+wfOZBwEYMXxAEbO20ip5ermHq0cOmD3hrUU5p+66jqF8CQuDfXKG1jZ2dlOy7Ozsx3rYmNjyclxHjHParWSn59/0RtgAQEBGAwGp5fwfYs2H6VR3gk0Giv+KoBMu4HdJ80s2nz0qo4bd207mrZtj91m5devV7ioWiE8g0tDvUWLFsTGxpKamupYZjab+emnn0hOTgYgOTmZgoICtm7d6tjmu+++w263k5SU5MpyhJfLyC8mjEASbVE0ssai0KDTaMjIL77qY3cbUnG1vi3la8qKr/54QniKGod6YWEhaWlpjt4I6enppKWlOQZZmjJlCq+99horVqxgx44djBs3jri4OIYPHw5Au3btuOOOO3jkkUf4+eef+fHHH5k0aRKjRo2Sni/CSXx4MJF2A/0s12OxVdxHsSlFfHjwVR+71Q3dCY9rRnlJMdtTV1/18YTwFDUO9S1bttClSxfHAyBPPvkkXbp04YUXXgDgmWeeYfLkyUyYMIHu3btTWFjI6tWrCQwMdBxj8eLFtG3blttvv51BgwbRu3dvPvzwQxedkvAVY3omck1gxY3RXI1Co4H2TQyM6Zl41cfWaLV0G3p2oK+vvsBmtVz1MYXwBBrlhY/Wmc1mjEYjJpNJ2td9XNYHv2E9XkjKtY2wXRPGmJ6JBOp1Ljm21WLhH5MeorjgNCU9f0dst5tcenwhasJVueYVvV9Ew2U3lQEwesC1PHxTS5cGrhUt243XV3yxJZW/frnbJb1rhHAnCXXhsZTFjv1MRbOILuzq+qdXZ9Hmo3xHKxT+BFnzaV+U4ZLeNUK4U42HCRCivtjOXqVr9Fq0wa7/Uc3IL8bmF0hRcCeM1mK02sYu610jhLtIqAuPZS2oGARO1zjgsnOR1kZ8eDA2pcgITSay3I9ArQWbsrqkd40Q7iLNL8Jj2QoqrtR1YYGX2bJ2xvRMpH0TAwW6ikkzGtu1LutdI4S7SKgLj2U9XRHqfnXQng4QqNfx2WO96NejGQBtQwL57LFe0vtFeDUJdeGxzl2p102oQ0Ww33tby4r3KbIR4Ce/EsK7yU+w8Fg2R5t63TS/VAqNDESj1WAtt1NUUF6n7yVEXZNQFx7LevZK3c9Yd1fqADqdltCIiv9xmHKk54vwbhLqwiMpuzrX/NK4bkMdICw6CIACCXXh5STUhUeyF1rApkADOoN/nb+fMbqiG6Mpp6TO30uIuiShLjySo4+6IQCNru5/TOVKXfgKCXXhkeqj58v5HFfquXKlLrybhLrwSPXZng7nrtRNuSUou9cNXCqEg4S68EjW0xXNL3X14NGFQsMD0Wo12Cx2Cs/+D0UIbyShLjxSXQ8RcCGtTosh6my7era0qwvvJaEuPFJ9N7/AeU0wcrNUeDEJdeGRHA8e1VPzC5y7WVog3RqFF5NQFx7HXmZFlViB+uv9AnKlLnyDhLrwOLazozNqgvzQBtTfkP9ypS58gYS68DjuaHoBMJ69UjfnlWC32ev1vYVwFQl14XEcozPWc6iHNg5E56fFblOcyZdujcI7SagLj1PZ88WvjofcvZBGq3F0a5R2deGtJNSFx6mc8ai+r9Th/DFgpF1deCcJdeFx6nvcl/OFOW6WypW68E4S6sLjuKtNHc7dLJXmF+GtJNSFR1E2OzZzxZRy9d2mDudfqUvzi/BOEurCo9hM5aAAnQZtI329v39lX/Uzp0qxSbdG4YVqHOobNmxgyJAhxMXFodFoWL58uWOdxWJh+vTpdOzYkUaNGhEXF8e4cePIzMx0OkZ+fj6jR4/GYDAQFhbG+PHjKSwsvOqTEd7Pdl4fdY1WU+/v3yjMHz9/LcquOJNXWu/vL8TVqnGoFxUV0alTJ+bMmVNlXXFxMb/++ivPP/88v/76K59//jn79u1j6NChTtuNHj2aXbt2kZKSwsqVK9mwYQMTJkyo/VkIn2F1Y3s6gEajwRglN0uF96rxM9gDBw5k4MCB1a4zGo2kpKQ4Lfvggw/o0aMHx44dIyEhgT179rB69Wp++eUXunXrBsDs2bMZNGgQ77zzDnFxcbU4DeErbKfrd8jd6oRFB3HqRKHMVyq8Up23qZtMJjQaDWFhYQBs2rSJsLAwR6AD9O3bF61Wy08//VTX5QgPZzO5rztjJaN0axRerE5HSyotLWX69Oncd999GAwGALKysoiOjnYuws+P8PBwsrKyqj1OWVkZZWXnHts2m811V7RwK8eMR/U4jvqFwmJksgzhversSt1isXDPPfeglGLevHlXdayZM2diNBodr/j4eBdVKTxNfc94VB3HJNTS/CK8UJ2EemWgHz16lJSUFMdVOkBsbCw5OTlO21utVvLz84mNja32eDNmzMBkMjleGRkZdVG2cDOllFPvF3ep7Kt+5nQpVovNbXUIURsuD/XKQD9w4ABr1qwhIiLCaX1ycjIFBQVs3brVsey7777DbreTlJRU7TEDAgIwGAxOL+F77MVWlKWib7g729SDQvXoA3WgwJwr3RqFd6lxm3phYSEHDx50fJ2enk5aWhrh4eE0adKEu+++m19//ZWVK1dis9kc7eTh4eH4+/vTrl077rjjDh555BHmz5+PxWJh0qRJjBo1Snq+NHC2s+3p2lA9Gj/3PRen0WgIiw4m99gZCnKKCY9r5LZahKipGv/mbNmyhS5dutClSxcAnnzySbp06cILL7zAiRMnWLFiBcePH6dz5840adLE8dq4caPjGIsXL6Zt27bcfvvtDBo0iN69e/Phhx+67qyEVzrX9OK+9vRK58aAkXZ14V1qfKV+yy23oJS66PpLrasUHh7OkiVLavrWwsdZ3Tg644UcY8DkSg8Y4V1k7BfhMRw9X9zYnbGSjNYovJWEuvAYlW3qfkb3h7rjSj1bml+Ed5FQFx7DWvk0qRuG3L1QZagXFZRhKZdujcJ7SKgLj2Fz4zR2FwoM0RMQXHHLSW6WCm8ioS48gr3chr3IArj3waPznXuyVNrVhfeQUBceoXIgL42/Dk1QnQ5JdMXOTUItoS68h4S68AiOppfGAWg09T85RnVkDBjhjSTUhUfwhDFfLiRX6sIbSagLj+DuGY+qI1fqwhtJqAuPcO7BI/d3Z6xUeaVebC6nvNTq5mqEuDIS6sIjWE97XvNLQLCeoFA9IFfrwntIqAuP4AnT2FVHJqEW3kZCXbidsiuPmPGoOmEyBozwMhLqwu3sZ8rBrkALOoO/u8txcm4Saml+Ed5BQl24nWPIXUMAGq1n9FGvJKM1Cm8joS7crnJ0Rk8YcvdCYXKlLryMhLpwO6sHzXh0ocor9dJCC2XFFjdXI8TlSagLt7N50IxHF/IP9CP4bDu/XK0LbyChLtzOk0MdICymcsIMaVcXnk9CXbidtXLGIw96mvR8crNUeBMJdeF2Hn+lLjdLhReRUBduZS+xosoqpovz1FAPDq+oa/f+U/zz+8OUWmR6O+G5JNSFW1X2fNEG+6H117m5mqpKLTZeWrsfAIu5nNdX7WHkvI0S7MJjecYUM6LBOtdH3b3t6UopztjsZJdZyCm3kFNuJafcQsqhPDbH6timLJQWWdDbNew+aWbR5qM8fFNLt9YsRHUk1IVbOQbyMtZN04vVrsizVAR0dpmF3HIr2eUWssut5J5dlnP23yV2Vf1BmgVTfPgMmnI7/gF+2O02MvLlpqnwTBLqwm1KLTZ+3ZFNS2BvcSndLDYC9VfWBFNktZFzNqArr6pzyirCuuJK20J2mZVTFisXiepqheq0xAToifL3I8ZfT05uMT/vzyVYq6UEG0VlVtBAfHhwrc5ZiLomoS7cotRiY+S8jdyTaaEler48coo35m3kw/E9MCn72YA+d2WdU251LMspt1Jks1/xe2mBKH8/ov31RPvriQmo/Lff2a8r/h3lrydY53ybqdRiY+T2U+w6+zSpAtpEhzCmZ6ILvxtCuI6EunCLRZuPsvukmRZRH1JqyeDHNq9w0BBJt5/3XPExgrRaYgIqrqgrr6zPv8qO9vcjJkBPuN4PXS0nsw7U6/jssV4s2nyUuesOkV9UzqgeCVf8F4UQ9U1CXbhFRn4xOo2GhPJDNC/LIEblcFDbDIAIvR8xlVfWAZUBXfHvaH+9I7BD/OonWAP1OsdN0ddW7WHl9pM8eGOLenlvIWqqxl0aN2zYwJAhQ4iLi0Oj0bB8+fKLbvvoo4+i0WiYNWuW0/L8/HxGjx6NwWAgLCyM8ePHU1hYWNNShBeLDw/GphS51nAArtl1gMD1J3lZF8qu3h34rkdbPu3civfbJfLnVnE8Eh/FsOjGJIeF0DI4oN4C/XxDOsWh1cDWo6c5dkpulArPVONQLyoqolOnTsyZM+eS2y1btozNmzcTFxdXZd3o0aPZtWsXKSkprFy5kg0bNjBhwoSaliK82JieibRvYuBQeUWox5Rlc114CPcnN3dvYZcQYwjkxtaRACxPO+HmaoSoXo1DfeDAgbz22muMGDHiotucOHGCyZMns3jxYvR6vdO6PXv2sHr1av75z3+SlJRE7969mT17Np9++imZmZk1PwPhlSrbqltf0w6AQfFWPnusl8e3VQ/v3BSA5b+dQKma9KsRon64/IlSu93O2LFjmTZtGtddd12V9Zs2bSIsLIxu3bo5lvXt2xetVstPP/1U7THLysowm81OL+H9AvU6ul1/PQBtAk97fKADDOgQS6Bey+G8IrYfN7m7HCGqcHmov/XWW/j5+fHEE09Uuz4rK4vo6GinZX5+foSHh5OVlVXtPjNnzsRoNDpe8fHxri5buEvY2c/SdNy9dVyhkAA/+rePBWDZb9IEIzyPS0N969at/O1vf2PhwoVoatmFrDozZszAZDI5XhkZGS47tnAz43mhbr/yvufuNKJLRRPMl9sysdSgv7wQ9cGlof7999+Tk5NDQkICfn5++Pn5cfToUZ566imaN28OQGxsLDk5OU77Wa1W8vPziY2Nrfa4AQEBGAwGp5fwEYY40GjBVg5Fue6u5or0viaSiEb+nCoq54eDee4uRwgnLg31sWPHsn37dtLS0hyvuLg4pk2bxjfffANAcnIyBQUFbN261bHfd999h91uJykpyZXlCG+g00Nok4p/m7zjLzC9TsuQThW9upZLE4zwMDV++KiwsJCDBw86vk5PTyctLY3w8HASEhKIiIhw2l6v1xMbG0ubNm0AaNeuHXfccQePPPII8+fPx2KxMGnSJEaNGlVt90fRABibgfkEFByDZt0uv70HGN6lKQs3HuGbXVkUllkJCZDn+IRnqPGV+pYtW+jSpQtdunQB4Mknn6RLly688MILV3yMxYsX07ZtW26//XYGDRpE7969+fDDD2taivAVRu+6WQrQqZmRFpGNKLXY+WZn9Tf4hXCHGl9e3HLLLTXqn3vkyJEqy8LDw1myZElN31r4KkcPGO9ofgHQaDSM6NKUd1P2szztBCO7NnN3SUIAMvOR8ATGs4FY4D2hDuceRPrxYB7Z5lI3VyNEBQl14X7GhIr/elHzC0BCRDBdExtjVxXdG4XwBBLqwv0czS/H3FtHLQw/22ddHkQSnkJCXbhfZfNLqQlKvWsIiDs7NsFPq2FXppn92WfcXY4QEurCAwSEQmBYxb+9rAmmcSN/bmlTMeyF9FkXnkBCXXgGL+wBU6ly2IAv0jKxX2zyaiHqiYS68AxG7w3129tFExrgx4mCEn45ku/uckQDJ6EuPENlqHtZt0aoGEJ4YMeKcYtk8gzhbhLqwjNU3iz1wit1ONcLZuX2k5RabG6uRjRkEurCM3jZuOoX6tkigibGQM6UWlm3L+fyOwhRRyTUhWeofADJC5tfALRaDUM7VwxIJ33WhTtJqAvPUNn8cuYkWMvdW0stVfaCWbs3l4Ji7zwH4f0k1IVnaBQFugBAwRnvfOS+bayBtrGhlNvsfLVDRm4U7iGhLjyDVuu1A3udr/JqXR5EEu4ioS48h5f3gAEY2jkOjQZ+PpJPRn6xu8sRDZCEuvAcXt4DBqCJMYjklhWzf30hfdaFG0ioC8/h6AHjfaM1nm/EeSM31mRCGSFcQUJdeA5H84v3XqkD3NEhlgA/LYdyi9h5wrtGnRTeT0JdeA4vHtTrfKGBevq1jwGkz7qofxLqwnOcf6Xu5c0WlU0wK7ZlYrXZ3VyNaEgk1IXnMDQDNGAthaI8d1dzVfpcG0XjYD15hWX8eOiUu8sRDYiEuvAcfv4QWjHaoTdObXc+vU7LkE4VwwZIn3VRnyTUhWfxgQeQKlWO3Lh6ZxZFZVY3VyMaCgl14VmM3t9XvVKX+DASI4IpsdhI2Z3t7nJEAyGhLjyLj/SAAdBoNAzvfK7PuhD1QUJdeBYfulKHc00w3x/IJfdMmZurEQ2BhLrwLI5p7bz7RmmlFpGN6Bwfhl3Bl9u8c/RJ4V0k1IVn8aHml0qOkRtlLBhRD2oc6hs2bGDIkCHExcWh0WhYvnx5lW327NnD0KFDMRqNNGrUiO7du3Ps2Lkrr9LSUiZOnEhERAQhISGMHDmS7Gy5kSQ41/ul5DSUFbq3Fhe58/om6LQath83cTDHN85JeK4ah3pRURGdOnVizpw51a4/dOgQvXv3pm3btqxbt47t27fz/PPPExgY6Nhm6tSpfPnllyxdupT169eTmZnJXXfdVfuzEL4j0AgBxop/+0i7ekRIADdfGwXIyI2i7mnUVQwjp9FoWLZsGcOHD3csGzVqFHq9nv/7v/+rdh+TyURUVBRLlizh7rvvBmDv3r20a9eOTZs20bNnz8u+r9lsxmg0YjKZMBgMtS1feKq5vSBnF4z+H1zTz93VuMSKbZk88e/faNY4iO+fuRWNRuPukoSHcVWuubRN3W63s2rVKq699loGDBhAdHQ0SUlJTk00W7duxWKx0LdvX8eytm3bkpCQwKZNm1xZjvBWPtiu3q9dDI38dRw/XcLWo6fdXY7wYS4N9ZycHAoLC3nzzTe54447+PbbbxkxYgR33XUX69evByArKwt/f3/CwsKc9o2JiSErq/p5HcvKyjCbzU4v4cMcPWB8J9SD/HXc0aEJIH3WRd1y+ZU6wLBhw5g6dSqdO3fm2Wef5c4772T+/Pm1Pu7MmTMxGo2OV3x8vKtKFp7IB6a1q85dN1T0glm5/STlVhm5UdQNl4Z6ZGQkfn5+tG/f3ml5u3btHL1fYmNjKS8vp6CgwGmb7OxsYmNjqz3ujBkzMJlMjldGhm/9sosL+MC0dtXp2TKCGEMAphILa/fluLsc4aNcGur+/v50796dffv2OS3fv38/iYmJAHTt2hW9Xk9qaqpj/b59+zh27BjJycnVHjcgIACDweD0Ej7MMa2db/3PW6fVMOzssAEycqOoK3413aGwsJCDBw86vk5PTyctLY3w8HASEhKYNm0a9957L3369OHWW29l9erVfPnll6xbtw4Ao9HI+PHjefLJJwkPD8dgMDB58mSSk5OvqOeLaAAqm1/OZILNCroa/5h6rOGdm/LhhsOk7snBVGLBGKR3d0nC16gaWrt2rQKqvO6//37HNh999JFq3bq1CgwMVJ06dVLLly93OkZJSYl6/PHHVePGjVVwcLAaMWKEOnny5BXXYDKZFKBMJlNNyxfewGZT6uUIpV40KHX6qLurcSm73a76v7teJU5fqf79k2+dm7g6rsq1q+qn7i7ST70B+FsnOH0EHvgKmt/o7mpcat66Q7y1ei9JLcL5zx+qb3IUDY9H9lMXwmV8bLTG8w3rXDEj0k/p+ZwoKHFzNcLXSKgLzxR29mapl09rV524sCB6tgwHZNgA4XoS6sIz+dC0dtWpHLlx2a8n8MIWUOHBJNSFZ/Lh5heAOzo0wd9Py4GcQnaflCekhetIqAvP5IPjv5zPGKSnb7toQPqsC9eSUBee6fzxX3y0eaJy/tIv0jKx2X3zHEX9k1AXnslQEXhYS6A437211JFb2kQTFqwn50wZmw6dcnc5wkdIqAvPpA+EkJiKf/tgDxgAfz8tgzvKyI3CtSTUhedyjNbomzdL4VwvmNU7T1JSbnNzNcIXSKgLz+WD46pfqGtiY5o1DqKo3EbKHpmnV1w9CXXhuXx0XPXzaTQax9W69IIRriChLjyX46lS3w11gOFnQ339/lxOFZa5uRrh7STUhedqAM0vAK2iQujUzIjNrli5/aS7yxFeTkJdeK4G0PxSqfJq/XNpghFXSUJdeK7Kp0qLT0F5sXtrqWN3Xh+HTqthW0YBh3ML3V2O8GIS6sJzBYaBf2jFv324WyNAVGgAN10TCcDytEw3VyO8mYS68FwazXlNML75ANL5zu8FIyM3itqSUBeeLcy3R2s8X7/2MQT76ziWX8yvxwrcXY7wUhLqwrM1kB4wAMH+ftxxXSwgfdZF7UmoC8/WAIYKOF9lL5iV2zMpt9rdXI3wRhLqwrM1kAeQKvVqFUFUaACniy1s2J/r7nKEF5JQF57Nx6e1u5CfTsvQThUTUy+T+UtFLUioC89W2aZuPgH2hjGKYWUvmDW7szGXWtxcjfA2EurCs4XGgtYPlA3ONIxH6K+LM9A6OoQyq53VO7PcXY7wMhLqwrNpdWCoaI5oKE0wMnKjuBoS6sLzGStvljaMHjCAo1190+FTnDSVuLka4U0k1IXnczyA5PtPlVaKDw+mR/NwlIIVMmyAqAEJdeH5Glhf9UqVfdZl/lJRExLqwvM1oKdKzze4YxP8dVr2Zp1hz0mzu8sBoNRi45/fH+bFL3byz+8PU2ppGD2SvEmNQ33Dhg0MGTKEuLg4NBoNy5cvd1pfWFjIpEmTaNasGUFBQbRv35758+c7bVNaWsrEiROJiIggJCSEkSNHkp0t8zOKi3A0vzSsUDcG67mtbTQAyz2gz3qpxcbIeRt586udLNp8lNdW7WHkvI0S7B6mxqFeVFREp06dmDNnTrXrn3zySVavXs2iRYvYs2cPU6ZMYdKkSaxYscKxzdSpU/nyyy9ZunQp69evJzMzk7vuuqv2ZyF8m/G8Qb0a2OiFlU0wX/yWid3unnO32RUHc84w/bPt7Mo0M9dvFqv003nJbyEJ2Wv434bf3FKXqJ5GXcUYnxqNhmXLljF8+HDHsg4dOnDvvffy/PPPO5Z17dqVgQMH8tprr2EymYiKimLJkiXcfffdAOzdu5d27dqxadMmevbsedn3NZvNGI1GTCYTBoOhtuULb2EpgdcrBrrimXQIDndvPfWozGqj+2trMJdaWfJwEr1aR9bp+1ltdg7mFrLjuIldmWZ2njCx+6SZ4vKKq3ENdn4L+ANhmiLnHaPbQ/PeFa/EG6FR3dbpi1yVa34urAmAXr16sWLFCh566CHi4uJYt24d+/fv57333gNg69atWCwW+vbt69inbdu2JCQkXDTUy8rKKCs7NyGv2ewZ7YuinuiDIDgSivMqmmAaUKgH+OkYfH0c//75GMt+O+HSUC+z2jiQXcjOEyZ2ZprYccLM3pNmyqoZSCxIryMixJ/jp0u4teyv9NDupad2D8na3bTVZkDO7orXzx9W7OAI+ZvOhnyEy+oWl+byUJ89ezYTJkygWbNm+Pn5odVq+cc//kGfPn0AyMrKwt/fn7CwMKf9YmJiyMqq/um5mTNn8vLLL7u6VOFNwuLPhvpxaNLJ3dXUqxFdmvLvn4/x9c4sXh3egUC9rsbHKLXY2Jt1hh0nTOw6G+L7ss5gsVX9Qz0kwI/r4gx0aGqkY1MjHZoaaBEZgsVmZ+S8jew+Cakk8a2tB+2jDXx2fxsCT2yGI9/DkR/OBbxTyF93wZW8hHxdqZNQ37x5MytWrCAxMZENGzYwceJE4uLinK7Oa2LGjBk8+eSTjq/NZjPx8fGuKll4A2M8ZP7W4HrAAHRLbEzTsCBOFJSwZk82d14fd8nti8qs7DlZ0XSy44SZXZkmDuQUYqumTd4YpKdjUyPXNTXQIc5Ih6ZGEsOD0Wo1VbbVaXV89lgvFm0+SkZ+MfHhwYzpmVjxPxnjUGg/9GwBeXD0x4qAd4T8rorXz3+v2EZCvs64NNRLSkr405/+xLJlyxg8eDAA119/PWlpabzzzjv07duX2NhYysvLKSgocLpaz87OJjY2ttrjBgQEEBAQ4MpShbcxNsweMABarYY7r2/C3zcc5s2v95JlKnWEqbnUwq6zwV0R4iYO5xVVez85opG/09X3dXFGmjUOQqOpGuAXE6jX8fBNLS+9UaNIaD+s4gVXFvIxHZxDvgE1sbmaS0PdYrFgsVjQap071eh0Ouz2ina6rl27otfrSU1NZeTIkQDs27ePY8eOkZyc7MpyhC9poN0aoaLpZM2eii6/x08XM+vbn3k/9QCNg/05ml9c7T6xhkBHcFcGeYwhoEYB7jIXhnxhrnPI5+6B7J0Vr5/Odn+WkK+1God6YWEhBw8edHydnp5OWloa4eHhJCQkcPPNNzNt2jSCgoJITExk/fr1fPLJJ7z77rsAGI1Gxo8fz5NPPkl4eDgGg4HJkyeTnJx8RT1fRAPVQB9AslhOs3zjf7ij6TesKL2DjDPNiGuUzf6CUMylVgCahgXRoanhbDOKkQ5xRqJCPfgv25AouG54xQuuIOQ1F4R8Lwn5S6hxl8Z169Zx6623Vll+//33s3DhQrKyspgxYwbffvst+fn5JCYmMmHCBKZOneq4SigtLeWpp57i3//+N2VlZQwYMIC5c+detPnlQtKlsQHKTIMPb4ZG0TDtgLurqVMlJcfJzUshNzcFk2kLSlV0J/z26C2sTr+d6OBcDplac3vbaN6+uxONG/m7t2BXK8yFoz+cF/J7L9jAN0PeVbl2Vf3U3UVCvQEqzoe3W1T8+8/ZoA90bz0upJSisHAPubkp5OalUFi4x2l9GS355tC1bM2+nuOFTQENGg38eVC7y7dv+4LCnAuu5C8R8i1ugoRkrwx5j+2nLkSdCGoM+kZgKaro1hjZ2t0VXRW73YrJtMUR5KWl5w8DoCUsrDtRUf2IiuyHxq8Jc3Zu5ESRGT+tBptStG9iYEzPRLfVX69CouG6ERUvOBfy6We7UObtg+wdFa+f5gEaiO1Q0Uf+7JV8qZ+h+l47Pkiu1IX3+KBHxS/w2OXQqmoToKez2UrIz//+bJB/h9Va4Fin1QYSEX4TUVH9iIi4FX9/5yvNUoutwYRSjRXmnLuKrwz58yg0HNa1YF1ZG35R7Vln70irJpF89lgvj/oeypW6aHjC4it+Yb2oB0x5+Sny8taSm5dCfv732O3nnozW6xsTGXk7UZF9CQ/vjU4XdNHjXFFXwoYqJBo63FXxgiohr8nbRyvbYVr5HeZBtZouZX9n90kzizYf9cnvqYS68B6V46p7eA+YkpJj5OauITcvhYKCLcC5x+4DA+MdzSpG4w1otfIr6HIXhPxf/reejLQ19GAXERozJkLw02jIuEh3UG8nP1HCe5w/WqMHUUpxpnAXubkp5OWmUFjk/Od/aMh1REb1IyqqHyGN2rinr3gD1jgmnrm2nqxQ57pM25QiPjzYjVXVHQl14T3CKucqdf+Vut1uoaDgF0fXw7Kyk451Go2OsLAeREX2IzKyL0FBTd1YqRjTM5Flv51g90kzOo3v32iWUBfew9H84p65Sq3WIseNzrxTa7FaTY51Wm0QERF9zgb5rej1YW6pUVQVqL/EmDU+SEJdeI/K5hdzJthtoK37X8ry8jzy8r4jNzeF/NM/YLeXO9bp9eFERfYlMqov4Y1vRKfznb7zvqYh3WiWUBfeI7QJaHRgt0BhNhguPVphbRUXHyE3b83ZJzq3Aud6/QYFJlTc6Izqj9HYBY3GN6/2hPeSUBfeQ+dXEeSmjIqbpS4KdaUUZ87scDwIVFTkPAxBaGgHoiIrbnQ2anSt3OgUHk1CXXgXY3xFqBccg/getT6M3V7O6YKfyTvb9bCs7NwELRqNH43Dkip6rETeTmBg3fxFIERdkFAX3qXyZmktesBYrYWcyt9Abm4Kp06txWo941in0wUTEX7z2Sc6b0GvN7qqYiHqlYS68C5hNeurXlaWS15e6tknOjei1Pk3OiOIiupLVGQ/GjfuhU7nwcPVCnGFJNSFd7mCcdWLi9PJzf224kanOQ2nG51BiURF9Scqqh9GQ2e50Sl8joS68CrlIXH4A9kZB/jy+8OM6ZlIgJ8Gc+WNztwUiosPOu1jMHQ62/WwH42CW8uNTuHTJNSF1yi12Jjy9SnmA8ElJ1n+0+eYs/bSvckuystzHNtpNH40bpxc8SBQ1O0EBlzZ5CtC+AIJdeE1Fm0+ylZTAQChmhKmXT8Hq5+W8nLQ6UIcT3RW3OiUIZlFwyShLryCqczEVyf+QWni15Sd0BBgVVjO+LOhKInw8L48MehetFq50SmEhLrwaCXWEhbvWczHOz/mTPkZ0MLsyOacNHdm9S+DKdYE8+dB7STQhThLQl14JKvdyvKDy5mXNo+ckor28tZh12A60Zc5GQnoNFpsGt8ebU+I2pBQFx5FKcWaY2t4/9f3OWI+AkBcozgmdZnEoBaDsNhoMKPtCVEbEurCY/x88mdm/TqLHXk7AGgc0JgJ10/gnjb34K/zB0CnpcGMtidEbUioC7fbm7+XWVtn8WPmjwAE+QUxrv04HrjuAUL8Q9xcnRDeRUJduE3GmQw++O0Dvkr/CgA/jR93X3s3f+j0ByKDIt1cnRDeSUJd1LtTJaf4+/a/s3T/Uqx2KwADmw9kUpdJJBgS3FydEN5NQl3UmyJLEf/a9S8W7lpIibUEgF5xvfjjDX+kfUR7N1cnhG+QUBd1rtxWztL9S/lw+4fkl+YD0CGiA1O6TiGpSZKbq6t/pRZbjXvw1GYf0TBJqIs6Y1d2Vh1exZy0OZwoPAFAc0NzJneZTL/Efg1yYK1Si42R8zY6zWy/7LcTfPZYr4uGdG32EQ2XtiYbz5w5k+7duxMaGkp0dDTDhw9n3759TtuUlpYyceJEIiIiCAkJYeTIkWRnZzttc+zYMQYPHkxwcDDR0dFMmzYNq9V69WcjPIJSiu+Pf8/vvvwdf/rhT5woPEFUUBTP93yez4d9Tv/m/RtkoENFH/vdJ80oBVa7QinYfdLMos1HXbqPaLhqdKW+fv16Jk6cSPfu3bFarfzpT3+if//+7N69m0aNGgEwdepUVq1axdKlSzEajUyaNIm77rqLH3+s6K5ms9kYPHgwsbGxbNy4kZMnTzJu3Dj0ej1vvPGG689Q1KttuduYtXUWW7K3ABCqD+Whjg8xut1ogvyC3Fyd+2XkF6PTaLCqc2O86zQaMvKLXbqPaLhqFOqrV692+nrhwoVER0ezdetW+vTpg8lk4qOPPmLJkiXcdtttACxYsIB27dqxefNmevbsybfffsvu3btZs2YNMTExdO7cmVdffZXp06fz0ksv4e/v77qzE/XmsOkw7//6PqnHUgHw1/rz+3a/Z3yH8YQFhrm3OA8SHx6M7bxwBrApRXx4sEv3EQ1XjZpfLmQymQAIDw8HYOvWrVgsFvr27evYpm3btiQkJLBp0yYANm3aRMeOHYmJiXFsM2DAAMxmM7t27ar2fcrKyjCbzU4v4RmyirJ4ceOLjPhiBKnHUtFqtIxoPYJVd63iqW5PSaBfYEzPRNo3MaDRgJ9Wg0aD0/g1pRYb//z+MC9+sZN/fn+YUovtsvsIcb5a3yi12+1MmTKFG2+8kQ4dOgCQlZWFv78/YWFhTtvGxMSQlZXl2Ob8QK9cX7muOjNnzuTll1+ubamiDpjKTHy04yOW7F1Cma0MgFvjb+WPN/yRVmGt3Fyd5wrU6/jssV7V9mS51A3Ri+0jxIVqHeoTJ05k586d/PDDD66sp1ozZszgySefdHxtNpuJj4+v8/cVVZVaS1m8ZzEf7fyoYihc4IboG5jadSqdozu7tzgvEajXVTt+jdMN0bPNLZU3RB++qaWMeSOuSK1CfdKkSaxcuZINGzbQrFkzx/LY2FjKy8spKChwulrPzs4mNjbWsc3PP//sdLzK3jGV21woICCAgAAZL9udqh8KtzVTu07lpqY3+XRvlvrqIy43RIUr1CjUlVJMnjyZZcuWsW7dOlq0aOG0vmvXruj1elJTUxk5ciQA+/bt49ixYyQnJwOQnJzM66+/Tk5ODtHR0QCkpKRgMBho316eKvQ0SilSj6Xyt1//5jQU7sQuExncYjA6rW83AdS2X3lt/icgN0SFK9Qo1CdOnMiSJUv44osvCA0NdbSBG41GgoKCMBqNjB8/nieffJLw8HAMBgOTJ08mOTmZnj17AtC/f3/at2/P2LFjefvtt8nKyuK5555j4sSJcjXuYX7J+oVZW2exPW87AGEBYUy4fgL3trnXMRSur7tck8iFruZBoTE9E1n22wmnfeWGqKipGoX6vHnzALjlllucli9YsIAHHngAgPfeew+tVsvIkSMpKytjwIABzJ0717GtTqdj5cqVPPbYYyQnJ9OoUSPuv/9+Xnnllas7E+Eye/P3MuvXWfx4QobCrWmTSE3/J3C+S91EFeJK1bj55XICAwOZM2cOc+bMueg2iYmJfPXVVzV5a1FHzm8qMISeIUu3nNVHzg2FO/LakTza6dEGOxRuTZtErrZd/GI3UYW4UjL2SwNW2VRwPOMgjcLWY26ahkZjA2Qo3Eo1bRKRdnHhbhLqDdh/vt1G8oZZDN63m5/aaJjTTIe18BrGtnmMP93cz93leYSaNolIu7hwNwn1BsiSk0P+xwu4YfESeljKAWiaq6cs/X7s5W0pa9bEzRV6lpo0iUi7uHA3CfUGxJKVxal/fkTB0qWosjL8gP1hzfj39Z3Z3LgPlGrRaKSp4GpJu7hwJwn1BsCSmUneP/6B6X+foSwWAII6dcLw6GM8vV3D7qwz+ElTgRA+QULdh5UfP86pv39IwfLlUBnm3boS9fjjBCcno9Fo+Ky3zKgjhC+RUPdB5UePkvf3DzF98QXYKnqzBPfsSeTjj9GoRw+nbaWpQAjfIqHuQ8oOHyZv/nzMK1eB3Q5AoxtvJPLxxwju2tXN1Qkh6oOEug8oO3CAvHnzMX/9NZztIx1y881EPv4YQZ06ubk6IUR9klD3YqV795I3dx5nvv3WsSzk9tuJfOwxgjpc58bKhBDuIqHuhUp27iJv3jwKU1Mdy0L79yfysUcJbNfOjZUJIdxNQt2LlGzbRt7ceRSuX1+xQKPBMHAgEY/+gcBrr3VvcUIIjyCh7gWKf/2VvDlzKfqxYtREtFoMdw4m8tFHCWgpPVeEEOdIqHuwop9/Jm/uPIo3b65YoNNhHDaMyAmP4N+8uVtrE0J4Jgl1D6OUonjTJnLnzqVky9aKhXo9YcOHEzHhEfxlblYhxCVIqHsIpRRFP/xA3py5lKSlAaDR6wn73d1EPPww+rg49xYohPAKEupuppSicO068ubOpXTnTgA0AQGE3XMPEQ+PRx8T4+YKhRDeRELdTZTdzpnUVPLmzaNs9x4ANIGBNB41ivCHHkR/dlJuIYSoCQn1eqZsNs58+y158+ZTtn8/AJrgYMJH/57wBx7ALyLCzRUKIbyZhHo9UTYb5q++Jm/+fMoPHQJA26gRjceOIfz++/Fr3NjNFQohfIGEuoudP5FzfHgwo7s1peybrzk1/++UHzkCgNZgIHzcOMLHjkFnNLq3YCGET5FQd6HKiZx3nzTjr+zcfHQLbQ59R6Q5DwCd0Uj4gw/QePRodKGhbq5WCOGLJNRdaNHmo+w+aeb2I78wZu+3xJScBsASYiDuDw/T+L7fowtp5OYqhRC+TELdhTLyi9FpNLQynSCm5DT5AaEsu/YWDPfcwwu/6+bu8oQQDYCEugvFhwdjU4r/XXMrWY0i+Lp5Tyx+ev4cG+7u0oQQDYTW3QX4kjE9E2nfxEB+sJFV1/TB4qeXiZyFEPVKrtRdKFCv47PHeslEzkIIt5FQdzGZyFkI4U41an6ZOXMm3bt3JzQ0lOjoaIYPH86+ffsc6/Pz85k8eTJt2rQhKCiIhIQEnnjiCUwmk9Nxjh07xuDBgwkODiY6Oppp06ZhtVpdc0ZCCNGA1SjU169fz8SJE9m8eTMpKSlYLBb69+9PUVERAJmZmWRmZvLOO++wc+dOFi5cyOrVqxk/frzjGDabjcGDB1NeXs7GjRv517/+xcKFC3nhhRdce2ZCCNEAaZQ6O/18LeTm5hIdHc369evp06dPtdssXbqUMWPGUFRUhJ+fH19//TV33nknmZmZxJwdgXD+/PlMnz6d3Nxc/P39L/u+ZrMZo9GIyWTCYDDUtnwhhPAYrsq1q+r9UtmsEh5+8S57lQX6+VU032/atImOHTs6Ah1gwIABmM1mdu3adTXlCCFEg1frG6V2u50pU6Zw44030qFDh2q3ycvL49VXX2XChAmOZVlZWU6BDji+zsrKqvY4ZWVllJWVOb42m821LVsIIXxara/UJ06cyM6dO/n000+rXW82mxk8eDDt27fnpZdequ3bABU3aI1Go+MVL1O6CSFEtWoV6pMmTWLlypWsXbuWZs2aVVl/5swZ7rjjDkJDQ1m2bBl6vd6xLjY2luzsbKftK7+OjY2t9v1mzJiByWRyvDIyMmpTthBC+LwahbpSikmTJrFs2TK+++47WrRoUWUbs9lM//798ff3Z8WKFQQGBjqtT05OZseOHeTk5DiWpaSkYDAYaN++fbXvGxAQgMFgcHoJIYSoqkZt6hMnTmTJkiV88cUXhIaGOtrAjUYjQUFBjkAvLi5m0aJFmM1mR/t3VFQUOp2O/v370759e8aOHcvbb79NVlYWzz33HBMnTiQgIOCK6qjssCNt60IIX1GZZ1fRIZHKA1wxoNrXggULlFJKrV279qLbpKenO45z5MgRNXDgQBUUFKQiIyPVU089pSwWyxXXkZGRcdH3kZe85CUvb35lZGTUJJaruKp+6u5it9vJzMwkNDQUjUZzRfuYzWbi4+PJyMjw6uYbXzgPOQfP4AvnAL5xHpXnsHv3btq0aYNWW/ve5l459otWq632Bu2V8JU2eV84DzkHz+AL5wC+cR5Nmza9qkAHGXpXCCF8ioS6EEL4kAYT6gEBAbz44otX3MPGU/nCecg5eAZfOAfwjfNw5Tl45Y1SIYQQ1WswV+pCCNEQSKgLIYQPkVAXQggfIqEuhBA+xOdDfd26dWg0mmpfv/zyCwBHjhypdv3mzZvdXL2z5s2bV6nxzTffdNpm+/bt3HTTTQQGBhIfH8/bb7/tpmqrOnLkCOPHj6dFixYEBQXRqlUrXnzxRcrLy5228YbPYs6cOTRv3pzAwECSkpL4+eef3V3SRV1ubmGAW265pcr3/NFHH3VTxVW99NJLVepr27atY31paSkTJ04kIiKCkJAQRo4cWWU0WHer7vdXo9EwceJEwIWfwVUNMuAFysrK1MmTJ51eDz/8sGrRooWy2+1KKaXS09MVoNasWeO0XXl5uZurd5aYmKheeeUVpxoLCwsd600mk4qJiVGjR49WO3fuVP/+979VUFCQ+vvf/+7Gqs/5+uuv1QMPPKC++eYbdejQIfXFF1+o6Oho9dRTTzm28YbP4tNPP1X+/v7q448/Vrt27VKPPPKICgsLU9nZ2e4urVoDBgxQCxYsUDt37lRpaWlq0KBBKiEhweln5+abb1aPPPKI0/fcZDK5sWpnL774orruuuuc6svNzXWsf/TRR1V8fLxKTU1VW7ZsUT179lS9evVyY8VV5eTkONWfkpKiALV27VqllOs+A58P9QuVl5erqKgo9corrziWVQbJb7/95r7CrkBiYqJ67733Lrp+7ty5qnHjxqqsrMyxbPr06apNmzb1UF3tvP3226pFixaOr73hs+jRo4eaOHGi42ubzabi4uLUzJkz3VjVlcvJyVGAWr9+vWPZzTffrP74xz+6r6jLePHFF1WnTp2qXVdQUKD0er1aunSpY9mePXsUoDZt2lRPFdbcH//4R9WqVSvHxaWrPgOfb3650IoVKzh16hQPPvhglXVDhw4lOjqa3r17s2LFCjdUd3lvvvkmERERdOnShb/85S9YrVbHuk2bNtGnTx+nybsHDBjAvn37OH36tDvKvSyTyVTtHLee+lmUl5ezdetW+vbt61im1Wrp27cvmzZtcmNlV+5icwsvXryYyMhIOnTowIwZMyguLnZHeRd14MAB4uLiaNmyJaNHj+bYsWMAbN26FYvF4vSZtG3bloSEBI/9TMrLy1m0aBEPPfSQ06CErvgMvHJAr6vx0UcfMWDAAKcBwUJCQvjrX//KjTfeiFar5bPPPmP48OEsX76coUOHurFaZ0888QQ33HAD4eHhbNy4kRkzZnDy5EneffddoGKO1wsnLjl//tfGjRvXe82XcvDgQWbPns0777zjWObpn0VeXh42m63aeXb37t3rpqqu3MXmFv79739PYmIicXFxbN++nenTp7Nv3z4+//xzN1Z7TlJSEgsXLqRNmzacPHmSl19+mZtuuomdO3eSlZWFv78/YWFhTvvExMRcdN5jd1u+fDkFBQU88MADjmUu+wyu+lrfTaZPn37ZcYn37NnjtE9GRobSarXqf//732WPP3bsWNW7d++6Kt+hNudR6aOPPlJ+fn6qtLRUKaVUv3791IQJE5y22bVrlwLU7t27Peocjh8/rlq1aqXGjx9/2ePX12dxJU6cOKEAtXHjRqfl06ZNUz169HBTVVfu0UcfVYmJiZcdszs1NVUB6uDBg/VUWc2cPn1aGQwG9c9//lMtXrxY+fv7V9mme/fu6plnnnFDdZfXv39/deedd15ym9p+Bl57pf7UU085/V+uOi1btnT6esGCBURERFzRFV9SUhIpKSlXU+IVqc15VEpKSsJqtXLkyBHatGlTq/lfXaGm55CZmcmtt95Kr169+PDDDy97/Pr6LK5EZGQkOp2u2u9zXX6PXaFybuENGzZcdujqpKQkoOKvqVatWtVHeTUSFhbGtddey8GDB+nXrx/l5eUUFBQ4Xa176mdy9OhR1qxZc9kr8Np+Bl4b6lFRUURFRV3x9kopFixYwLhx45wmwr6YtLQ0mjRpcjUlXpGansf50tLS0Gq1REdHAxXzv/75z3/GYrE4zjElJYU2bdrUadNLTc7hxIkT3HrrrXTt2pUFCxZc0djR9fVZXAl/f3+6du1Kamoqw4cPByqaNFJTU5k0aZJ7i7sIpRSTJ09m2bJlrFu3rtq5hS+UlpYG4DHf9wsVFhZy6NAhxo4dS9euXdHr9aSmpjJy5EgA9u3bx7Fjx0hOTnZzpVUtWLCA6OhoBg8efMntav0Z1P4PCO+yZs2aizZlLFy4UC1ZskTt2bNH7dmzR73++utKq9Wqjz/+2A2VVm/jxo3qvffeU2lpaerQoUNq0aJFKioqSo0bN86xTUFBgYqJiVFjx45VO3fuVJ9++qkKDg72mC6Nx48fV61bt1a33367On78uFPXrUre8Fl8+umnKiAgQC1cuFDt3r1bTZgwQYWFhamsrCx3l1atxx57TBmNRrVu3Tqn73lxcbFSSqmDBw+qV155RW3ZskWlp6erL774QrVs2VL16dPHzZWf89RTT6l169ap9PR09eOPP6q+ffuqyMhIlZOTo5SqaFZKSEhQ3333ndqyZYtKTk5WycnJbq66KpvNphISEtT06dOdlrvyM2gwoX7fffddtN/qwoULVbt27VRwcLAyGAyqR48eTt2jPMHWrVtVUlKSMhqNKjAwULVr10698cYbjvb0Stu2bVO9e/dWAQEBqmnTpurNN990U8VVLViw4KJt7pW84bNQSqnZs2erhIQE5e/vr3r06KE2b97s7pIu6mLf88q5hY8dO6b69OmjwsPDVUBAgGrdurWaNm2aR/VTv/fee1WTJk2Uv7+/atq0qbr33nud2ppLSkrU448/rho3bqyCg4PViBEjnC4WPMU333yjALVv3z6n5a78DGToXSGE8CENrp+6EEL4Mgl1IYTwIRLqQgjhQyTUhRDCh0ioCyGED5FQF0IIHyKhLoQQPkRCXQghfIiEuhBC+BAJdSGE8CES6kII4UMk1IUQwof8P35Z6jlmqaLRAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os, pathlib, pickle, json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "ANN  = f\"{BASE}/data/annotations/aistpp\"\n",
        "\n",
        "def load_any(path):\n",
        "    suf = pathlib.Path(path).suffix.lower()\n",
        "    if suf == \".pkl\" or suf == \".pickle\":\n",
        "        with open(path, \"rb\") as f: obj = pickle.load(f)\n",
        "        return obj\n",
        "    if suf == \".npz\":\n",
        "        data = np.load(path, allow_pickle=True)\n",
        "        return {k: data[k] for k in data.files}\n",
        "    if suf == \".npy\":\n",
        "        return np.load(path, allow_pickle=True)\n",
        "    if suf == \".json\":\n",
        "        with open(path, \"r\") as f: return json.load(f)\n",
        "    raise ValueError(f\"Extensión no soportada: {suf}\")\n",
        "\n",
        "def to_TJD_from_any(obj):\n",
        "    # Si el PKL/NPZ es dict, busca arrays grandes dentro\n",
        "    def iter_arrays(o, depth=3):\n",
        "        if depth < 0: return\n",
        "        if isinstance(o, np.ndarray):\n",
        "            yield o\n",
        "        elif isinstance(o, dict):\n",
        "            for v in o.values(): yield from iter_arrays(v, depth-1)\n",
        "        elif isinstance(o, (list,tuple)):\n",
        "            for v in o: yield from iter_arrays(v, depth-1)\n",
        "\n",
        "    for arr in iter_arrays(obj, 4):\n",
        "        if not isinstance(arr, np.ndarray): continue\n",
        "        if arr.ndim != 3: continue\n",
        "        # probar permutaciones a (T,J,D) con D=2/3\n",
        "        for axes in [(0,1,2),(0,2,1),(1,0,2),(1,2,0),(2,0,1),(2,1,0)]:\n",
        "            T,J,D = arr.shape[axes[0]], arr.shape[axes[1]], arr.shape[axes[2]]\n",
        "            if D in (2,3) and 5 <= J <= 80 and T >= 8:\n",
        "                return np.transpose(arr, axes).astype(np.float32)\n",
        "    # si ya era np.ndarray (T,J,D)\n",
        "    if isinstance(obj, np.ndarray) and obj.ndim == 3 and obj.shape[-1] in (2,3):\n",
        "        return obj.astype(np.float32)\n",
        "    return None\n",
        "\n",
        "def find_first_existing(patterns, root):\n",
        "    rootp = pathlib.Path(root)\n",
        "    for pat in patterns:\n",
        "        f = sorted(rootp.rglob(pat))\n",
        "        if f: return str(f[0])\n",
        "    return None\n",
        "\n",
        "# Buscar 2D y 3D típicos dentro de aist_plusplus_final\n",
        "p2 = find_first_existing([\n",
        "    \"aist_plusplus_final/keypoints2d/*.pkl\",\n",
        "    \"aist_plusplus_final/**/keypoints2d*.pkl\",\n",
        "    \"*keypoints2d*.pkl\",\"*pose2d*.pkl\",\"*2d*keypoints*.pkl\",\n",
        "    \"*keypoints2d*.npz\",\"*pose2d*.npz\",\"*2d*keypoints*.npz\",\n",
        "], ANN)\n",
        "\n",
        "p3 = find_first_existing([\n",
        "    \"aist_plusplus_final/keypoints3d/*.pkl\",\n",
        "    \"aist_plusplus_final/**/keypoints3d*.pkl\",\n",
        "    \"*keypoints3d*.pkl\",\"*pose3d*.pkl\",\"*3d*keypoints*.pkl\",\n",
        "    \"*keypoints3d*.npz\",\"*pose3d*.npz\",\"*3d*keypoints*.npz\",\n",
        "], ANN)\n",
        "\n",
        "print(\"Detectado 2D:\", p2)\n",
        "print(\"Detectado 3D:\", p3)\n",
        "\n",
        "K2 = None\n",
        "if p2:\n",
        "    obj2 = load_any(p2)\n",
        "    K2 = to_TJD_from_any(obj2)\n",
        "    print(\"K2 shape:\", None if K2 is None else K2.shape)\n",
        "\n",
        "K3 = None\n",
        "if p3:\n",
        "    obj3 = load_any(p3)\n",
        "    K3 = to_TJD_from_any(obj3)\n",
        "    print(\"K3 shape:\", None if K3 is None else K3.shape)\n",
        "\n",
        "# ---------- Limpieza NaN (interpolación temporal por joint,dim) ----------\n",
        "def interpolate_nan_1d(y):\n",
        "    \"\"\"\n",
        "    y: (T,) con NaN -> forward/backward fill + interp lineal\n",
        "    \"\"\"\n",
        "    y = y.astype(np.float32)\n",
        "    T = len(y)\n",
        "    idx = np.arange(T)\n",
        "    mask = ~np.isnan(y)\n",
        "    if mask.sum() == 0:\n",
        "        return y  # todo NaN -> se tratará arriba\n",
        "    # forward/backward\n",
        "    # forward fill\n",
        "    last = np.nan\n",
        "    for t in range(T):\n",
        "        if np.isnan(y[t]) and not np.isnan(last):\n",
        "            y[t] = last\n",
        "        else:\n",
        "            last = y[t]\n",
        "    # backward fill\n",
        "    last = np.nan\n",
        "    for t in range(T-1, -1, -1):\n",
        "        if np.isnan(y[t]) and not np.isnan(last):\n",
        "            y[t] = last\n",
        "        else:\n",
        "            last = y[t]\n",
        "    # si aún quedan NaN extremos, interp lineal con puntos válidos\n",
        "    mask = ~np.isnan(y)\n",
        "    if mask.sum() >= 2 and mask.sum() < T:\n",
        "        y[~mask] = np.interp(idx[~mask], idx[mask], y[mask])\n",
        "    return y\n",
        "\n",
        "def clean_nan_interpolate(K, min_valid_ratio=0.15):\n",
        "    \"\"\"\n",
        "    Interpola NaN en cada joint y dimensión. Descarta joints con pocos datos válidos.\n",
        "    Devuelve K_clean (T,J',D) y un mapa de joints usados.\n",
        "    \"\"\"\n",
        "    if K is None: return None, []\n",
        "    Kc = K.copy()\n",
        "    T,J,D = Kc.shape\n",
        "    use_joints = []\n",
        "    for j in range(J):\n",
        "        valid = ~np.isnan(Kc[:,j,:]).any(axis=1)\n",
        "        ratio = valid.mean()\n",
        "        if ratio < min_valid_ratio:\n",
        "            # demasiados NaN -> descartar este joint completo\n",
        "            Kc[:,j,:] = np.nan\n",
        "            continue\n",
        "        for d in range(D):\n",
        "            Kc[:,j,d] = interpolate_nan_1d(Kc[:,j,d])\n",
        "        use_joints.append(j)\n",
        "    # si todos descartados, devolvemos tal cual para evitar crash\n",
        "    if not use_joints:\n",
        "        return Kc, []\n",
        "    # filtrar a solo joints útiles (al menos uno no NaN)\n",
        "    Kc = Kc[:, use_joints, :]\n",
        "    return Kc, use_joints\n",
        "\n",
        "def nanrobust_metrics(K):\n",
        "    \"\"\"\n",
        "    Métricas que ignoran NaNs: amplitud (nanmax-nanmin), velocidad media, simetría.\n",
        "    \"\"\"\n",
        "    T,J,D = K.shape\n",
        "    amp = (np.nanmax(K, axis=0) - np.nanmin(K, axis=0))  # (J,D)\n",
        "    amp_mean = np.nanmean(amp, axis=0)                   # (D,)\n",
        "    vel = np.diff(K, axis=0)                             # (T-1,J,D)\n",
        "    vel_norm = np.linalg.norm(vel, axis=2)               # (T-1,J)\n",
        "    vel_mean = np.nanmean(vel_norm)\n",
        "    # simetría: pares COCO, usando solo joints presentes\n",
        "    COCO_PAIRS = [(5,6),(7,8),(9,10),(11,12),(13,14),(15,16)]\n",
        "    pair_vals = []\n",
        "    for a,b in COCO_PAIRS:\n",
        "        if a < J and b < J:\n",
        "            diff = K[:,a,:] - K[:,b,:]\n",
        "            # Check number of dimensions before calculating norm\n",
        "            if diff.ndim > 1:\n",
        "                d = np.linalg.norm(diff, axis=1).mean()  # Correct axis for (T, D) -> (T,)\n",
        "                pair_vals.append(np.nanmean(d))\n",
        "    sym_raw = np.nanmean(pair_vals) if pair_vals else np.nan\n",
        "    # avoid division by zero if all amplitude means are zero or NaN\n",
        "    sum_amp_mean = np.nansum(amp_mean)\n",
        "    sym_index = sym_raw / (sum_amp_mean + 1e-6) if sum_amp_mean > 0 else np.nan\n",
        "    return {\n",
        "        \"amp_media_por_eje\": [float(x) if np.isfinite(x) else float('nan') for x in amp_mean],\n",
        "        \"vel_media\": float(vel_mean) if np.isfinite(vel_mean) else float('nan'),\n",
        "        \"simetria_indice\": float(sym_index) if np.isfinite(sym_index) else float('nan')\n",
        "    }\n",
        "\n",
        "# Limpiar K3 y medir\n",
        "if K3 is not None:\n",
        "    K3c, used3 = clean_nan_interpolate(K3, min_valid_ratio=0.10)\n",
        "    print(f\"Joints 3D usados: {len(used3)} de {K3.shape[1]}\")\n",
        "    m3 = nanrobust_metrics(K3c)\n",
        "    print(\"Métricas 3D (limpias):\", m3)\n",
        "\n",
        "# Intentar localizar/cargar 2D si no se encontró\n",
        "if K2 is None:\n",
        "    # En muchos paquetes 2D falta, trabajamos con proyección (x,y) del 3D limpio\n",
        "    if K3 is not None:\n",
        "        K2 = K3c[:,:,:2]  # x,y\n",
        "        print(\"K2 generado por proyección de K3 (x,y). Shape:\", K2.shape)\n",
        "\n",
        "# Visualización 2D\n",
        "def plot_skeleton_2d(frame, title=\"Skeleton 2D\"):\n",
        "    # Conectividad COCO aproximada\n",
        "    edges = [(5,7),(7,9),(6,8),(8,10),(11,13),(13,15),\n",
        "             (12,14),(14,16),(5,6),(11,12),(5,11),(6,12)]\n",
        "    xs, ys = frame[:,0], frame[:,1]\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.scatter(xs, ys, s=12)\n",
        "    for a,b in edges:\n",
        "        if a < len(frame) and b < len(frame):\n",
        "            plt.plot([xs[a], xs[b]], [ys[a], ys[b]])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.axis(\"equal\"); plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "if K2 is not None and np.isfinite(K2).any():\n",
        "    mid = len(K2)//2\n",
        "    plot_skeleton_2d(K2[mid], \"Frame medio (2D limpio)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nLieogpXuDmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e0b6d3-22db-4d0c-e0a6-e278eae2c94f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extraídos:\n",
            " {'amplitud_x': np.float32(57.239643), 'amplitud_y': np.float32(28.566929), 'velocidad_media': np.float32(1.0696408), 'simetria': np.float32(30.30207), 'nivel_alto': np.float32(147.81683), 'nivel_bajo': np.float32(138.101), 'nivel_rango': np.float32(-9.715836), 'variedad_direcciones': np.float32(1.1550457)}\n",
            "\n",
            "Sugerencias coreográficas:\n",
            " - Aumentar amplitud: proyectar más en horizontal/vertical y ampliar desplazamientos.\n",
            " - Explorar niveles: añadir suelo (low) y saltos (high) para mayor contraste vertical.\n",
            " - Incrementar fluidez: encadenar transiciones y elevar tonicidad en pasajes clave.\n",
            " - Capa estética: alinear intención (peso/flujo) con cualidad del movimiento para coherencia expresiva.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Centro de masas aproximado = promedio de caderas (11 y 12 en COCO)\n",
        "def center_of_mass(K):\n",
        "    if K.shape[1] >= 13:\n",
        "        return (K[:,11,:] + K[:,12,:]) / 2\n",
        "    return K.mean(axis=1)\n",
        "\n",
        "def features_coreograficos(K):\n",
        "    T,J,D = K.shape\n",
        "    feats = {}\n",
        "\n",
        "    # --- amplitud global ---\n",
        "    amp = (K.max(axis=0) - K.min(axis=0)).mean(axis=0)\n",
        "    feats[\"amplitud_x\"], feats[\"amplitud_y\"] = amp[0], amp[1]\n",
        "    if D == 3: feats[\"amplitud_z\"] = amp[2]\n",
        "\n",
        "    # --- velocidad / fluidez ---\n",
        "    vel = np.linalg.norm(np.diff(K,axis=0),axis=2).mean()\n",
        "    feats[\"velocidad_media\"] = vel\n",
        "\n",
        "    # --- simetría ---\n",
        "    pairs = [(5,6),(7,8),(9,10),(11,12),(13,14),(15,16)]\n",
        "    pair_dists = []\n",
        "    for a,b in pairs:\n",
        "        if a<J and b<J:\n",
        "            d = np.linalg.norm(K[:,a,:]-K[:,b,:],axis=1).mean()\n",
        "            pair_dists.append(d)\n",
        "    feats[\"simetria\"] = np.mean(pair_dists)\n",
        "\n",
        "    # --- niveles espaciales ---\n",
        "    com = center_of_mass(K)\n",
        "    feats[\"nivel_alto\"] = np.percentile(com[:,1],90)\n",
        "    feats[\"nivel_bajo\"] = np.percentile(com[:,1],10)\n",
        "    feats[\"nivel_rango\"] = feats[\"nivel_bajo\"] - feats[\"nivel_alto\"]\n",
        "\n",
        "    # --- variedad direccional ---\n",
        "    disp = np.diff(com,axis=0)\n",
        "    dirs = np.arctan2(disp[:,1], disp[:,0])\n",
        "    cambios = np.abs(np.diff(dirs))\n",
        "    feats[\"variedad_direcciones\"] = np.mean(cambios)\n",
        "\n",
        "    return feats\n",
        "\n",
        "def sugerencias(feats: dict):\n",
        "    \"\"\"\n",
        "    Genera sugerencias coreográficas a partir de métricas de análisis.\n",
        "    Soporta nombres alternativos y métricas opcionales.\n",
        "    Devuelve lista de strings (sugerencias deduplicadas y priorizadas).\n",
        "    \"\"\"\n",
        "\n",
        "    S = []  # sugerencias\n",
        "\n",
        "    # ===== Helpers =====\n",
        "    def val(*keys, default=None):\n",
        "        for k in keys:\n",
        "            if k in feats and feats[k] is not None:\n",
        "                return feats[k]\n",
        "        return default\n",
        "\n",
        "    def add(msg, prio=5):\n",
        "        # prio: 1 (crítico) .. 9 (cosmético)\n",
        "        if msg:\n",
        "            S.append((prio, msg.strip()))\n",
        "\n",
        "    # ===== Normalización de nombres frecuentes =====\n",
        "    ax   = val(\"amplitud_x\", \"amp_x\")\n",
        "    ay   = val(\"amplitud_y\", \"amp_y\")\n",
        "    az   = val(\"amplitud_z\", \"amp_z\")\n",
        "    vel  = val(\"velocidad_media\", \"vel_media\")\n",
        "    vel_sd = val(\"velocidad_std\", \"vel_std\")\n",
        "    sim  = val(\"simetria\", \"simetria_raw\")\n",
        "    varD = val(\"variedad_direcciones\", \"variedad_dir\")          # (rad prom |Δθ|)\n",
        "    nivel = val(\"nivel_rango\", \"nivel_rango\")                   # (p10 - p90 Y COM)\n",
        "    disp = val(\"disp_total\", \"trayectoria_longitud\")            # longitud de COM\n",
        "    frames = val(\"frames\")\n",
        "    dims   = val(\"dims\")\n",
        "\n",
        "    # ===== Técnicos (alineación, equilibrio, extensiones, torso, cabeza) =====\n",
        "    # Alineación postural (0..100, mayor=mejor)\n",
        "    alineacion = val(\"alineacion_postural\", \"posture_alignment_score\")\n",
        "    if alineacion is not None:\n",
        "        if alineacion < 40:\n",
        "            add(\"Alinear ejes corporales: revisar colocación de pelvis, caja torácica y eje cabeza‑cadera.\", prio=2)\n",
        "        elif alineacion < 70:\n",
        "            add(\"Ajustar alineación: activar centro (core) para estabilizar hombros y caderas.\", prio=4)\n",
        "\n",
        "    # Equilibrio/estabilidad (0..100, mayor=mejor) y sway/temblor (grande=peor)\n",
        "    equilibrio = val(\"equilibrio\", \"balance_score\")\n",
        "    sway = val(\"oscilacion_centro\", \"sway_mag\")  # pix o normalizado\n",
        "    if equilibrio is not None:\n",
        "        if equilibrio < 40:\n",
        "            add(\"Equilibrio bajo: ensayar apoyos con foco visual y transferencias de peso más claras.\", prio=2)\n",
        "        elif equilibrio < 70:\n",
        "            add(\"Mejorar estabilidad: trabajar relevés controlados y anclajes de pie de apoyo.\", prio=4)\n",
        "    if sway is not None and sway > 15:\n",
        "        add(\"Reducir oscilación de tronco: reforzar control de core y respiración dirigida.\", prio=4)\n",
        "\n",
        "    # Extensiones (0..180º aproximado)\n",
        "    ext_pierna = val(\"extension_pierna\", \"leg_extension_deg\")\n",
        "    if ext_pierna is not None:\n",
        "        if ext_pierna < 60:\n",
        "            add(\"Extensión de pierna limitada: trabajar flexibilidad de isquios y acción de psoas.\", prio=5)\n",
        "        elif ext_pierna > 150:\n",
        "            add(\"Gran extensión de pierna: cuidar cierre limpio y sostén para evitar colapso.\", prio=6)\n",
        "\n",
        "    ext_brazo = val(\"extension_brazo\", \"arm_extension_deg\")\n",
        "    if ext_brazo is not None and ext_brazo < 70:\n",
        "        add(\"Mayor proyección de brazos: extender a través de dedos manteniendo hombros bajos.\", prio=5)\n",
        "\n",
        "    # Torso / cabeza (0..100, mayor=mejor)\n",
        "    torso_estab = val(\"estabilidad_torso\", \"torso_stability\")\n",
        "    if torso_estab is not None and torso_estab < 50:\n",
        "        add(\"Torso inestable: aislar disociaciones pelvis/torso para limpiar líneas.\", prio=4)\n",
        "    control_cabeza = val(\"control_cabeza\", \"head_control\")\n",
        "    if control_cabeza is not None and control_cabeza < 50:\n",
        "        add(\"Control de cabeza bajo: definir focos (spotting) y direcciones claras de mirada.\", prio=5)\n",
        "\n",
        "    # ===== Espaciales =====\n",
        "    if ax is not None and ay is not None:\n",
        "        if ax < 50 or ay < 50:\n",
        "            add(\"Aumentar amplitud: proyectar más en horizontal/vertical y ampliar desplazamientos.\", prio=3)\n",
        "        if abs(ax - ay) > 120:\n",
        "            add(\"Equilibrar uso de ejes: compensar horizontal/vertical para un espacio más homogéneo.\", prio=5)\n",
        "        if max(ax, ay) > 260:\n",
        "            add(\"Amplitud muy alta: alternar micro‑gestos/pausas para no saturar el encuadre.\", prio=6)\n",
        "    if az is not None and az > 40:\n",
        "        add(\"Exceso de variación en profundidad: estabilizar planos antes de cambiar de nivel Z.\", prio=7)\n",
        "\n",
        "    if nivel is not None:\n",
        "        if nivel < 30:\n",
        "            add(\"Explorar niveles: añadir suelo (low) y saltos (high) para mayor contraste vertical.\", prio=3)\n",
        "        elif nivel > 120:\n",
        "            add(\"Rango vertical muy amplio: introducir tránsitos por nivel medio para cohesión.\", prio=6)\n",
        "\n",
        "    if disp is not None:\n",
        "        if disp < 150:\n",
        "            add(\"Ocupación espacial reducida: abrir diagonales y profundidades para ventilar la escena.\", prio=4)\n",
        "        elif disp > 2000:\n",
        "            add(\"Demasiada traslación: crear anclajes y puntos de fijación para foco visual.\", prio=6)\n",
        "\n",
        "    # Curvatura/entropía de trayectoria (0 lineal .. alto curvo/errático)\n",
        "    curv = val(\"curvatura_trayectoria\", \"path_curvature\", \"trayectoria_curv\")\n",
        "    if curv is not None:\n",
        "        if curv < 0.15:\n",
        "            add(\"Trayectorias lineales: incorporar arcos/espirales para enriquecer el dibujo espacial.\", prio=6)\n",
        "        elif curv > 0.8:\n",
        "            add(\"Trayectorias muy sinuosas: incluir líneas rectas para contraste y legibilidad.\", prio=7)\n",
        "\n",
        "    # ===== Direcciones / Orientación =====\n",
        "    if varD is not None:\n",
        "        if varD < 0.4:\n",
        "            add(\"Poca variedad direccional: sumar giros, cambios de orientación y diagonales.\", prio=4)\n",
        "        elif varD > 1.8:\n",
        "            add(\"Exceso de quiebres: sostener orientación más tiempo para dar foco.\", prio=6)\n",
        "\n",
        "    # ===== Dinámica (velocidad/fluidez/pausas/energía) =====\n",
        "    if vel is not None:\n",
        "        if vel < 2.0:\n",
        "            add(\"Incrementar fluidez: encadenar transiciones y elevar tonicidad en pasajes clave.\", prio=3)\n",
        "        elif vel > 8.0:\n",
        "            add(\"Regular velocidad: introducir suspensiones/pausas para respiración escénica.\", prio=5)\n",
        "\n",
        "    if vel_sd is not None:\n",
        "        if vel_sd < 0.5:\n",
        "            add(\"Dinámica monótona: añadir acentos, contrastes de peso y calidad de esfuerzo.\", prio=4)\n",
        "        elif vel_sd > 3.0:\n",
        "            add(\"Dinámica errática: clarificar acentos primarios y limpiar transiciones.\", prio=5)\n",
        "\n",
        "    pausas = val(\"pausas_ratio\", \"pause_ratio\")  # 0..1\n",
        "    if pausas is not None:\n",
        "        if pausas < 0.05:\n",
        "            add(\"Casi sin pausas: insertar micro‑silencios para realzar acentos.\", prio=6)\n",
        "        elif pausas > 0.35:\n",
        "            add(\"Muchas pausas: conectar frases para sostener la continuidad.\", prio=6)\n",
        "\n",
        "    energia = val(\"energia\", \"energy_level\")  # 0..100\n",
        "    if energia is not None:\n",
        "        if energia < 30:\n",
        "            add(\"Energía baja: elevar ataque y proyección para sostener presencia escénica.\", prio=4)\n",
        "        elif energia > 85:\n",
        "            add(\"Energía muy alta: modular con momentos de contención para diversidad.\", prio=6)\n",
        "\n",
        "    # ===== Simetría / Disociaciones =====\n",
        "    if sim is not None:\n",
        "        if sim < 10:\n",
        "            add(\"Demasiada asimetría: introducir momentos de equilibrio bilateral.\", prio=5)\n",
        "        elif sim > 80:\n",
        "            add(\"Mucha simetría: explorar disociaciones entre lados para enriquecer textura.\", prio=5)\n",
        "\n",
        "    disoc = val(\"disociacion\", \"disociation_score\")  # 0..100\n",
        "    if disoc is not None:\n",
        "        if disoc < 30:\n",
        "            add(\"Poca disociación: trabajar independencia de brazos/torso/piernas.\", prio=6)\n",
        "        elif disoc > 85:\n",
        "            add(\"Mucha disociación: re‑sincronizar focos corporales para cohesión del fraseo.\", prio=7)\n",
        "\n",
        "    # ===== Suelo / Peso / Tiempo / Espacio / Fluidez (Laban) =====\n",
        "    tiempo_suelo = val(\"tiempo_en_suelo\", \"ground_time_ratio\")  # 0..1\n",
        "    if tiempo_suelo is not None:\n",
        "        if tiempo_suelo < 0.1:\n",
        "            add(\"Casi sin trabajo de suelo: integrar apoyos low para contraste de niveles.\", prio=5)\n",
        "        elif tiempo_suelo > 0.6:\n",
        "            add(\"Mucho suelo: alternar elevaciones a nivel medio/alto para balance.\", prio=6)\n",
        "\n",
        "    laban_peso   = val(\"laban_peso\", \"effort_weight\")     # 0 ligero .. 1 fuerte\n",
        "    laban_tiempo = val(\"laban_tiempo\", \"effort_time\")     # 0 sostenido .. 1 súbito\n",
        "    laban_espacio= val(\"laban_espacio\", \"effort_space\")   # 0 indirecto .. 1 directo\n",
        "    laban_fluidez= val(\"laban_fluidez\", \"effort_flow\")    # 0 ligado .. 1 libre\n",
        "    if laban_peso is not None:\n",
        "        if laban_peso < 0.3: add(\"Calidad ligera: incorporar acentos de peso para densidad expresiva.\", prio=6)\n",
        "        elif laban_peso > 0.8: add(\"Peso muy marcado: intercalar pasajes ligeros para contraste.\", prio=6)\n",
        "    if laban_tiempo is not None:\n",
        "        if laban_tiempo < 0.3: add(\"Tiempo sostenido: sumar ataques súbitos para sorpresa rítmica.\", prio=6)\n",
        "        elif laban_tiempo > 0.8: add(\"Tiempo muy súbito: incluir prolongaciones/sostenidos para respirar.\", prio=6)\n",
        "    if laban_espacio is not None:\n",
        "        if laban_espacio < 0.3: add(\"Espacio indirecto: introducir trayectorias directas y precisas.\", prio=7)\n",
        "        elif laban_espacio > 0.8: add(\"Espacio muy directo: sumar exploración periférica/curvilínea.\", prio=7)\n",
        "    if laban_fluidez is not None:\n",
        "        if laban_fluidez < 0.3: add(\"Flujo muy ligado: liberar articulaciones para mayor libertad.\", prio=7)\n",
        "        elif laban_fluidez > 0.8: add(\"Flujo muy libre: consolidar cierres para claridad formal.\", prio=7)\n",
        "\n",
        "    # ===== Música / Ritmo =====\n",
        "    onbeat = val(\"onbeat_ratio\")\n",
        "    bpm    = val(\"bpm\")\n",
        "    if onbeat is not None:\n",
        "        if onbeat < 0.4:\n",
        "            add(\"Poca alineación con el pulso: anclar acentos coreográficos a pulsos o contratiempos claros.\", prio=4)\n",
        "        elif onbeat > 0.9:\n",
        "            add(\"Adherencia total al pulso: jugar con síncopas/silencios para sorpresa.\", prio=6)\n",
        "    if bpm is not None and vel is not None:\n",
        "        if bpm >= 120 and vel < 3.0:\n",
        "            add(\"Música rápida con baja energía motriz: aumentar ataque y desplazamiento.\", prio=4)\n",
        "        if bpm <= 80 and vel > 6.0:\n",
        "            add(\"Música lenta con alta energía: introducir suspensión y foco en calidad del gesto.\", prio=5)\n",
        "\n",
        "    # ===== Grupales (sync/unison/spread/área/proximidad/centros) =====\n",
        "    gm_sync  = val(\"gm_sync\", \"sync\")\n",
        "    gm_unis  = val(\"gm_unison\", \"unison\")\n",
        "    gm_sp    = val(\"gm_spread\", \"spread\")\n",
        "    gm_area  = val(\"formation_area\", \"gm_formation_area\")\n",
        "    prox_col = val(\"colisiones_proximidad\", \"near_collisions\")  # conteo/frame\n",
        "    centros  = val(\"centros_atencion\", \"attention_centers\")     # 1..N\n",
        "\n",
        "    if gm_sync is not None:\n",
        "        if gm_sync < 0.2:\n",
        "            add(\"Sincronía grupal baja: ensayar cues comunes y respiración compartida.\", prio=2)\n",
        "        elif gm_sync > 0.6:\n",
        "            add(\"Buena sincronía: insertar contrapuntos/cánones controlados para riqueza.\", prio=6)\n",
        "    if gm_unis is not None:\n",
        "        if gm_unis < 0.5:\n",
        "            add(\"Unísono débil: trabajar frases espejo y acentos comunes antes del contrapunto.\", prio=3)\n",
        "        elif gm_unis > 0.85:\n",
        "            add(\"Unísono muy alto: introducir micro‑variaciones de timing/nivel.\", prio=6)\n",
        "    if gm_sp is not None:\n",
        "        if gm_sp < 60:\n",
        "            add(\"Formación compacta: abrir aperturas y diagonales para respiración escénica.\", prio=3)\n",
        "        elif gm_sp > 300:\n",
        "            add(\"Formación muy dispersa: crear subgrupos/centros de atención para dirigir la mirada.\", prio=4)\n",
        "    if gm_area is not None:\n",
        "        if gm_area < 3000:\n",
        "            add(\"Área de formación pequeña: explorar geometrías amplias (V, X, diagonales).\", prio=5)\n",
        "        elif gm_area > 20000:\n",
        "            add(\"Área de formación grande: condensar en momentos clave para impacto visual.\", prio=6)\n",
        "    if prox_col is not None and prox_col > 0.2:\n",
        "        add(\"Riesgo de colisiones: definir rutas y prioridades de paso entre intérpretes.\", prio=3)\n",
        "    if centros is not None:\n",
        "        if centros < 1.5:\n",
        "            add(\"Un solo foco escénico: sumar focos secundarios o desplazarlos en el tiempo.\", prio=7)\n",
        "        elif centros > 3.5:\n",
        "            add(\"Demasiados focos: simplificar para no dispersar la atención del público.\", prio=6)\n",
        "\n",
        "    # ===== Composición / Continuidad / Originalidad =====\n",
        "    continuidad = val(\"continuidad\", \"continuity_score\")  # 0..100\n",
        "    if continuidad is not None and continuidad < 50:\n",
        "        add(\"Continuidad débil: mejorar enlaces entre frases y cadencias de cierre/apertura.\", prio=4)\n",
        "\n",
        "    trans_suaves = val(\"transiciones_suaves\", \"smooth_transitions\")  # 0..100\n",
        "    if trans_suaves is not None and trans_suaves < 50:\n",
        "        add(\"Transiciones bruscas: trabajar anticipación y preparación para cambios limpios.\", prio=5)\n",
        "\n",
        "    originalidad = val(\"originalidad_trayectoria\", \"trajectory_novelty\")  # 0..100\n",
        "    if originalidad is not None and originalidad < 40:\n",
        "        add(\"Baja originalidad espacial: introducir permutaciones (retrogradación, inversión, desplazamientos).\", prio=7)\n",
        "\n",
        "    densidad = val(\"densidad_mov\", \"movement_density\")  # acciones/s\n",
        "    if densidad is not None:\n",
        "        if densidad < 0.5:\n",
        "            add(\"Densidad escasa: añadir ornamentación o capas secundarias en transiciones.\", prio=7)\n",
        "        elif densidad > 3.0:\n",
        "            add(\"Densidad alta: limpiar material y dejar respirar los acentos.\", prio=7)\n",
        "\n",
        "    # ===== Comunicación/Escena =====\n",
        "    contacto_visual = val(\"contacto_visual\", \"gaze_contact_ratio\")  # 0..1\n",
        "    if contacto_visual is not None:\n",
        "        if contacto_visual < 0.15:\n",
        "            add(\"Poco contacto visual: definir miradas/focos para aumentar conexión con público y partener.\", prio=6)\n",
        "        elif contacto_visual > 0.8:\n",
        "            add(\"Contacto visual constante: alternar miradas periféricas para matizar la relación escénica.\", prio=7)\n",
        "\n",
        "    respiracion = val(\"respiracion_sync\", \"breath_sync\")  # 0..100\n",
        "    if respiracion is not None and respiracion < 50:\n",
        "        add(\"Baja sincronía respiratoria: utilizar respiración como cue de inicio/cierre de frases.\", prio=6)\n",
        "\n",
        "    # ===== Heurísticas de duración =====\n",
        "    if frames is not None and frames < 150:  # ~5s a 30 FPS\n",
        "        add(\"Frase corta: repetir con variaciones (canon, retrogradación, inversión) para consolidar material.\", prio=8)\n",
        "\n",
        "    # ===== Capa estética global =====\n",
        "    add(\"Capa estética: alinear intención (peso/flujo) con cualidad del movimiento para coherencia expresiva.\", prio=9)\n",
        "\n",
        "    # ===== Post-proceso: de-dup + orden por prioridad =====\n",
        "    # Eliminamos duplicados manteniendo la mayor prioridad (menor número)\n",
        "    agg = {}\n",
        "    for p, m in S:\n",
        "        if m not in agg or p < agg[m]:\n",
        "            agg[m] = p\n",
        "    ordered = sorted([(p, m) for m, p in agg.items()], key=lambda x: x[0])\n",
        "\n",
        "    # Devuelve solo los textos\n",
        "    out = [m for _, m in ordered]\n",
        "    return out or [\"El movimiento presenta buena diversidad y balance.\"]\n",
        "\n",
        "# ---- aplicar sobre K2 (si está cargado) ----\n",
        "if K2 is not None:\n",
        "    feats = features_coreograficos(K2)\n",
        "    print(\"Features extraídos:\\n\", feats)\n",
        "    print(\"\\nSugerencias coreográficas:\")\n",
        "    for sug in sugerencias(feats):\n",
        "        print(\" -\", sug)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9acc7bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "001977c2-c558-47d6-eed7-0d830b269a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1408 annotation files.\n",
            "\n",
            "Successfully extracted features and saved to /content/drive/MyDrive/asistente_coreografico/data/processed/aistpp/features_coreograficos.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   amplitud_x  amplitud_y  amplitud_z  velocidad_media   simetria  nivel_alto  \\\n",
              "0  227.528519   41.328522   91.622879         2.359743  40.723736  147.239853   \n",
              "1   25.468586   32.092312   32.247986         1.138402  40.259823  150.203766   \n",
              "2  101.514961   45.013420   47.687611         2.576868  45.424957  147.822052   \n",
              "3   93.244843   21.825813   84.305893         1.555619  32.353634  148.301926   \n",
              "4   57.560638   40.952541   62.890690         3.272810  40.687447  122.561562   \n",
              "\n",
              "   nivel_bajo  nivel_rango  variedad_direcciones  \\\n",
              "0  139.325241    -7.914612              1.627065   \n",
              "1  141.449493    -8.754272              1.852321   \n",
              "2  128.938416   -18.883636              0.739498   \n",
              "3  141.038330    -7.263596              1.419307   \n",
              "4  109.722549   -12.839012              1.862236   \n",
              "\n",
              "                         filename  \\\n",
              "0  gJB_sBM_cAll_d09_mJB4_ch04.pkl   \n",
              "1  gLO_sBM_cAll_d15_mLO2_ch09.pkl   \n",
              "2  gKR_sBM_cAll_d30_mKR4_ch07.pkl   \n",
              "3  gLH_sBM_cAll_d16_mLH0_ch04.pkl   \n",
              "4  gBR_sBM_cAll_d06_mBR5_ch10.pkl   \n",
              "\n",
              "                                            filepath  frames  joints  dims  \n",
              "0  /content/drive/MyDrive/asistente_coreografico/...     480      17     3  \n",
              "1  /content/drive/MyDrive/asistente_coreografico/...     576      17     3  \n",
              "2  /content/drive/MyDrive/asistente_coreografico/...     480      17     3  \n",
              "3  /content/drive/MyDrive/asistente_coreografico/...     720      17     3  \n",
              "4  /content/drive/MyDrive/asistente_coreografico/...     443      17     3  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b707aaa-ef26-4eb4-b31a-f96a48852c95\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>amplitud_x</th>\n",
              "      <th>amplitud_y</th>\n",
              "      <th>amplitud_z</th>\n",
              "      <th>velocidad_media</th>\n",
              "      <th>simetria</th>\n",
              "      <th>nivel_alto</th>\n",
              "      <th>nivel_bajo</th>\n",
              "      <th>nivel_rango</th>\n",
              "      <th>variedad_direcciones</th>\n",
              "      <th>filename</th>\n",
              "      <th>filepath</th>\n",
              "      <th>frames</th>\n",
              "      <th>joints</th>\n",
              "      <th>dims</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>227.528519</td>\n",
              "      <td>41.328522</td>\n",
              "      <td>91.622879</td>\n",
              "      <td>2.359743</td>\n",
              "      <td>40.723736</td>\n",
              "      <td>147.239853</td>\n",
              "      <td>139.325241</td>\n",
              "      <td>-7.914612</td>\n",
              "      <td>1.627065</td>\n",
              "      <td>gJB_sBM_cAll_d09_mJB4_ch04.pkl</td>\n",
              "      <td>/content/drive/MyDrive/asistente_coreografico/...</td>\n",
              "      <td>480</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25.468586</td>\n",
              "      <td>32.092312</td>\n",
              "      <td>32.247986</td>\n",
              "      <td>1.138402</td>\n",
              "      <td>40.259823</td>\n",
              "      <td>150.203766</td>\n",
              "      <td>141.449493</td>\n",
              "      <td>-8.754272</td>\n",
              "      <td>1.852321</td>\n",
              "      <td>gLO_sBM_cAll_d15_mLO2_ch09.pkl</td>\n",
              "      <td>/content/drive/MyDrive/asistente_coreografico/...</td>\n",
              "      <td>576</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>101.514961</td>\n",
              "      <td>45.013420</td>\n",
              "      <td>47.687611</td>\n",
              "      <td>2.576868</td>\n",
              "      <td>45.424957</td>\n",
              "      <td>147.822052</td>\n",
              "      <td>128.938416</td>\n",
              "      <td>-18.883636</td>\n",
              "      <td>0.739498</td>\n",
              "      <td>gKR_sBM_cAll_d30_mKR4_ch07.pkl</td>\n",
              "      <td>/content/drive/MyDrive/asistente_coreografico/...</td>\n",
              "      <td>480</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>93.244843</td>\n",
              "      <td>21.825813</td>\n",
              "      <td>84.305893</td>\n",
              "      <td>1.555619</td>\n",
              "      <td>32.353634</td>\n",
              "      <td>148.301926</td>\n",
              "      <td>141.038330</td>\n",
              "      <td>-7.263596</td>\n",
              "      <td>1.419307</td>\n",
              "      <td>gLH_sBM_cAll_d16_mLH0_ch04.pkl</td>\n",
              "      <td>/content/drive/MyDrive/asistente_coreografico/...</td>\n",
              "      <td>720</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57.560638</td>\n",
              "      <td>40.952541</td>\n",
              "      <td>62.890690</td>\n",
              "      <td>3.272810</td>\n",
              "      <td>40.687447</td>\n",
              "      <td>122.561562</td>\n",
              "      <td>109.722549</td>\n",
              "      <td>-12.839012</td>\n",
              "      <td>1.862236</td>\n",
              "      <td>gBR_sBM_cAll_d06_mBR5_ch10.pkl</td>\n",
              "      <td>/content/drive/MyDrive/asistente_coreografico/...</td>\n",
              "      <td>443</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b707aaa-ef26-4eb4-b31a-f96a48852c95')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8b707aaa-ef26-4eb4-b31a-f96a48852c95 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8b707aaa-ef26-4eb4-b31a-f96a48852c95');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e319bbd9-af90-41de-9e90-19b24194b37c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e319bbd9-af90-41de-9e90-19b24194b37c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e319bbd9-af90-41de-9e90-19b24194b37c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"\\\\nNo features were extracted\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"amplitud_x\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          25.468585968017578,\n          57.560638427734375,\n          101.51496124267578\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"amplitud_y\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          32.09231185913086,\n          40.95254135131836,\n          45.01342010498047\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"amplitud_z\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          32.24798583984375,\n          62.890689849853516,\n          47.6876106262207\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"velocidad_media\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.138401985168457,\n          3.2728097438812256,\n          2.5768680572509766\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"simetria\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          40.259822845458984,\n          40.68744659423828,\n          45.424957275390625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nivel_alto\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          150.20376586914062,\n          122.56156158447266,\n          147.82205200195312\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nivel_bajo\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          141.44949340820312,\n          109.72254943847656,\n          128.93841552734375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nivel_rango\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -8.7542724609375,\n          -12.839012145996094,\n          -18.883636474609375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"variedad_direcciones\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.8523212671279907,\n          1.8622362613677979,\n          0.7394978404045105\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"gLO_sBM_cAll_d15_mLO2_ch09.pkl\",\n          \"gBR_sBM_cAll_d06_mBR5_ch10.pkl\",\n          \"gKR_sBM_cAll_d30_mKR4_ch07.pkl\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filepath\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gLO_sBM_cAll_d15_mLO2_ch09.pkl\",\n          \"/content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gBR_sBM_cAll_d06_mBR5_ch10.pkl\",\n          \"/content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final/keypoints3d/gKR_sBM_cAll_d30_mKR4_ch07.pkl\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"frames\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 112,\n        \"min\": 443,\n        \"max\": 720,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          576,\n          443,\n          480\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"joints\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 17,\n        \"max\": 17,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dims\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pathlib\n",
        "import pickle\n",
        "\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "ANN  = f\"{BASE}/data/annotations/aistpp\"\n",
        "PROCESSED = f\"{BASE}/data/processed/aistpp\"\n",
        "\n",
        "\n",
        "CSV  = f\"{PROCESSED}/features_coreograficos.csv\"\n",
        "ANNOTATION_DIR = f\"{ANN}/aist_plusplus_final/keypoints3d\" # Se Asumen los 3D keypoints que se estan utilizando\n",
        "\n",
        "all_features = []\n",
        "\n",
        "# Encontramos todos los pkl files en el directorio de anotaciones\n",
        "pkl_files = list(pathlib.Path(ANNOTATION_DIR).rglob(\"*.pkl\"))\n",
        "\n",
        "print(f\"Found {len(pkl_files)} annotation files.\")\n",
        "\n",
        "for pkl_file in pkl_files:\n",
        "    try:\n",
        "        obj = load_any(str(pkl_file))\n",
        "        K = to_TJD_from_any(obj)\n",
        "\n",
        "        if K is not None and np.isfinite(K).any():\n",
        "            # Liempieza de valores NaN e interpolación\n",
        "            Kc, used_joints = clean_nan_interpolate(K, min_valid_ratio=0.10)\n",
        "\n",
        "            if Kc is not None and np.isfinite(Kc).any():\n",
        "                 # Extraemos los features\n",
        "                feats = features_coreograficos(Kc)\n",
        "\n",
        "                # Añadimos metadata\n",
        "                feats['filename'] = pkl_file.name\n",
        "                feats['filepath'] = str(pkl_file)\n",
        "                feats['frames'] = Kc.shape[0]\n",
        "                feats['joints'] = Kc.shape[1]\n",
        "                feats['dims'] = Kc.shape[2]\n",
        "\n",
        "                all_features.append(feats)\n",
        "            else:\n",
        "                print(f\"Skipping {pkl_file.name}: No valid data after cleaning.\")\n",
        "        else:\n",
        "             print(f\"Skipping {pkl_file.name}: Could not load or find valid keypoint data.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pkl_file.name}: {e}\")\n",
        "\n",
        "\n",
        "if all_features:\n",
        "    df = pd.DataFrame(all_features)\n",
        "    df.to_csv(CSV, index=False)\n",
        "    print(f\"\\nSuccessfully extracted features and saved to {CSV}\")\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"\\nNo features were extracted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HnQwy0VxF2Z"
      },
      "source": [
        "# **# Entrenar un modelo base (multietiqueta)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gZsWpTzxpb60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e40078-ea40-4059-cd53-0d3f0152b7f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 macro (test, con umbrales óptimos): 0.9681851150956631\n",
            "F1 micro (test, con umbrales óptimos): 0.9681632653061224\n",
            "\n",
            "✔️ Artefactos guardados en: /content/drive/MyDrive/asistente_coreografico/artifacts\n",
            " - complete_model_thresholded_bundle.joblib\n",
            " - complete_label_names.csv\n",
            " - complete_feature_cols.csv\n",
            " - complete_thresholds.json\n"
          ]
        }
      ],
      "source": [
        "# ENTRENAMIENTO (umbral por etiqueta)\n",
        "import numpy as np, pandas as pd, os, json, joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# --- Rutas ---\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "PROC = f\"{BASE}/data/processed/aistpp\"\n",
        "ART  = f\"{BASE}/artifacts\"\n",
        "CSV  = f\"{PROC}/features_coreograficos.csv\"\n",
        "os.makedirs(ART, exist_ok=True)\n",
        "\n",
        "# --- Cargar datos base ---\n",
        "df = pd.read_csv(CSV)\n",
        "\n",
        "# Si ya tienes y_complete de celdas previas, lo usamos. Si no, lo reconstruimos pequeño.\n",
        "if 'y_complete' not in globals():\n",
        "    # Etiquetas mínimas por si no existe y_complete (usa tus reglas si ya las definiste)\n",
        "    def _mk_y(d):\n",
        "        dnum = d.select_dtypes(include=np.number).copy()\n",
        "        q20, q80 = dnum.quantile(0.20), dnum.quantile(0.80)\n",
        "        lab = {}\n",
        "        if {\"amplitud_x\",\"amplitud_y\"} <= set(dnum.columns):\n",
        "            lab['amplitud_baja']     = ((dnum['amplitud_x']<=q20['amplitud_x'])|(dnum['amplitud_y']<=q20['amplitud_y'])).astype(int)\n",
        "            lab['amplitud_excesiva'] = ((dnum['amplitud_x']>=q80['amplitud_x'])|(dnum['amplitud_y']>=q80['amplitud_y'])).astype(int)\n",
        "            ratio = dnum['amplitud_x']/(dnum['amplitud_y']+1e-6)\n",
        "            r10,r90 = ratio.quantile(0.10), ratio.quantile(0.90)\n",
        "            lab['amplitud_desbalance'] = ((ratio<=r10)|(ratio>=r90)).astype(int)\n",
        "        for k in [\"velocidad_media\",\"simetria\",\"nivel_rango\",\"variedad_direcciones\",\"frames\"]:\n",
        "            if k in dnum:\n",
        "                lab[f\"{k}_low\"]  = (dnum[k]<=q20.get(k,np.nan)).fillna(False).astype(int)\n",
        "                lab[f\"{k}_high\"] = (dnum[k]>=q80.get(k,np.nan)).fillna(False).astype(int)\n",
        "        return pd.DataFrame(lab)\n",
        "    y_complete = _mk_y(df)\n",
        "\n",
        "label_names = y_complete.columns.tolist()\n",
        "\n",
        "# --- FEATURE ENGINEERING  ---\n",
        "def build_features(d: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = pd.DataFrame(index=d.index)\n",
        "    # básicos (usa los que existan)\n",
        "    base_cols = ['amplitud_x','amplitud_y','amplitud_z',\n",
        "                 'velocidad_media','simetria','nivel_rango',\n",
        "                 'variedad_direcciones','frames']\n",
        "    for c in base_cols:\n",
        "        if c in d.columns: out[c] = pd.to_numeric(d[c], errors='coerce')\n",
        "\n",
        "    # derivados (solo si hay datos fuente)\n",
        "    if {'amplitud_x','amplitud_y'} <= set(out.columns):\n",
        "        out['amp_diff']  = (out['amplitud_x'] - out['amplitud_y']).abs()\n",
        "        out['amp_ratio'] = out['amplitud_x'] / (out['amplitud_y'].abs() + 1e-6)\n",
        "    if 'velocidad_media' in out:\n",
        "        out['vel_log1p'] = np.log1p(out['velocidad_media'].clip(lower=0))\n",
        "    if 'nivel_rango' in out:\n",
        "        out['nivel_abs'] = out['nivel_rango'].abs()\n",
        "    if {'velocidad_media','variedad_direcciones'} <= set(out.columns):\n",
        "        out['vel_x_varDir'] = out['velocidad_media'] * out['variedad_direcciones']\n",
        "    if {'simetria','amplitud_x'} <= set(out.columns):\n",
        "        out['sim_x_ampx'] = out['simetria'] * out['amplitud_x']\n",
        "\n",
        "    return out\n",
        "\n",
        "X_df = build_features(df)\n",
        "feature_cols = X_df.columns.tolist()\n",
        "X_all = X_df.values\n",
        "Y_all = y_complete.values\n",
        "\n",
        "# --- Split train/val/test (70/15/15) ---\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X_all, Y_all, test_size=0.15, random_state=42\n",
        ")\n",
        "# val = 15% del total => relativo a trainval:\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval, y_trainval, test_size=0.1765, random_state=42\n",
        ")\n",
        "\n",
        "# --- Pipeline con LogisticRegressionCV dentro de OneVsRest ---\n",
        "pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\",  RobustScaler()),\n",
        "    (\"clf\", OneVsRestClassifier(\n",
        "        LogisticRegressionCV(\n",
        "            Cs=np.logspace(-2, 1, 6),   # 0.01..10\n",
        "            solver=\"liblinear\",\n",
        "            class_weight=\"balanced\",\n",
        "            max_iter=4000,\n",
        "            cv=3,\n",
        "            scoring=\"f1\",\n",
        "            n_jobs=-1,\n",
        "            refit=True\n",
        "        ),\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# --- Buscar UMBRALES por etiqueta en validación para maximizar F1 ---\n",
        "def find_optimal_thresholds(y_true, y_score, grid=None):\n",
        "    if grid is None:\n",
        "        grid = np.linspace(0.2, 0.8, 61)  # 0.2..0.8 paso 0.01\n",
        "    C = y_true.shape[1]\n",
        "    thr = np.full(C, 0.5, dtype=float)\n",
        "    for i in range(C):\n",
        "        yt = y_true[:, i]\n",
        "        # si en val hay solo una clase, dejamos 0.5\n",
        "        if np.unique(yt).size < 2:\n",
        "            thr[i] = 0.5\n",
        "            continue\n",
        "        best_f1, best_t = -1.0, 0.5\n",
        "        scores_i = y_score[:, i]\n",
        "        for t in grid:\n",
        "            yp = (scores_i >= t).astype(int)\n",
        "            f1 = f1_score(yt, yp, zero_division=0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1, best_t = f1, t\n",
        "        thr[i] = best_t\n",
        "    return thr\n",
        "\n",
        "# Probabilidades en validación\n",
        "try:\n",
        "    y_val_score = pipe.predict_proba(X_val)\n",
        "except Exception:\n",
        "    # fallback: usar decision_function y pasarlo por sigmoid\n",
        "    logits = pipe.decision_function(X_val)\n",
        "    y_val_score = 1.0 / (1.0 + np.exp(-logits))\n",
        "\n",
        "thresholds = find_optimal_thresholds(y_val, y_val_score)\n",
        "\n",
        "# --- Wrapper compatible con tu celda de evaluación ---\n",
        "class ThresholdedOVR:\n",
        "    \"\"\"Envuelve un pipeline y aplica umbral por etiqueta en predict.\"\"\"\n",
        "    def __init__(self, pipeline, thresholds, label_names=None, feature_cols=None):\n",
        "        self.pipeline = pipeline\n",
        "        self.thresholds_ = np.asarray(thresholds, dtype=float)\n",
        "        self.label_names = list(label_names) if label_names is not None else None\n",
        "        self.feature_cols = list(feature_cols) if feature_cols is not None else None\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.pipeline.predict_proba(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        P = self.predict_proba(X)\n",
        "        thr = self.thresholds_\n",
        "        # broadcast seguro\n",
        "        return (P >= thr.reshape(1, -1)).astype(int)\n",
        "\n",
        "# construir objeto final esperado por evaluación\n",
        "complete_model = ThresholdedOVR(\n",
        "    pipeline=pipe,\n",
        "    thresholds=thresholds,\n",
        "    label_names=label_names,\n",
        "    feature_cols=feature_cols\n",
        ")\n",
        "\n",
        "# --- Evaluación rápida en test para ver mejora (opcional) ---\n",
        "P_test = complete_model.predict_proba(X_test)\n",
        "Yp_test = (P_test >= thresholds.reshape(1, -1)).astype(int)\n",
        "print(\"F1 macro (test, con umbrales óptimos):\", f1_score(y_test, Yp_test, average=\"macro\", zero_division=0))\n",
        "print(\"F1 micro (test, con umbrales óptimos):\", f1_score(y_test, Yp_test, average=\"micro\", zero_division=0))\n",
        "\n",
        "# --- Guardado de artefactos ---\n",
        "bundle = {\n",
        "    \"pipeline\": pipe,\n",
        "    \"thresholds\": thresholds,\n",
        "    \"label_names\": label_names,\n",
        "    \"feature_cols\": feature_cols\n",
        "}\n",
        "joblib.dump(bundle, f\"{ART}/complete_model_thresholded_bundle.joblib\")\n",
        "pd.Series(label_names).to_csv(f\"{ART}/complete_label_names.csv\", index=False, header=False)\n",
        "pd.Series(feature_cols).to_csv(f\"{ART}/complete_feature_cols.csv\", index=False, header=False)\n",
        "with open(f\"{ART}/complete_thresholds.json\", \"w\") as f:\n",
        "    json.dump({label_names[i]: float(thresholds[i]) for i in range(len(label_names))}, f, indent=2)\n",
        "\n",
        "print(\"\\n✔️ Artefactos guardados en:\", ART)\n",
        "print(\" - complete_model_thresholded_bundle.joblib\")\n",
        "print(\" - complete_label_names.csv\")\n",
        "print(\" - complete_feature_cols.csv\")\n",
        "print(\" - complete_thresholds.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YRVkg30ROXqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e90b0f3-2e7d-4f7b-d402-ca76b67fbd97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUACIÓN MULTIETIQUETA – RESUMEN GLOBAL\n",
            "============================================================\n",
            "Subset accuracy (exact match): 0.8302\n",
            "Hamming loss:                 0.0142\n",
            "F1 micro:                     0.9682\n",
            "F1 macro:                     0.9682\n",
            "F1 weighted:                  0.9686\n",
            "Precisión micro:              0.9580\n",
            "Recall micro:                 0.9785\n",
            "Jaccard (samples):            0.8436\n",
            "Jaccard (macro):              0.9395\n",
            "\n",
            "------------------------------------------------------------\n",
            "Métricas por etiqueta (top 10 por menor F1 para priorizar mejoras):\n",
            "                    label  support  tp  fp  fn  tn  precision   recall       f1  roc_auc  avg_precision\n",
            "        amplitud_excesiva        0  44   7   1 160   0.862745 0.977778 0.916667 0.998536       0.995130\n",
            "            amplitud_baja        0  48   2   5 157   0.960000 0.905660 0.932039 0.993473       0.982968\n",
            "         nivel_rango_high        0  39   5   0 168   0.886364 1.000000 0.939759 0.999111       0.996152\n",
            "     velocidad_media_high        0  48   6   0 158   0.888889 1.000000 0.941176 0.999746       0.999167\n",
            "      amplitud_desbalance        0  33   0   2 177   1.000000 0.942857 0.970588 0.998386       0.992921\n",
            "               frames_low        0  64   2   1 145   0.969697 0.984615 0.977099 0.999477       0.998901\n",
            "      velocidad_media_low        0  43   1   1 167   0.977273 0.977273 0.977273 0.999729       0.999012\n",
            "variedad_direcciones_high        0  47   1   1 163   0.979167 0.979167 0.979167 0.978659       0.982090\n",
            "          nivel_rango_low        0  39   1   0 172   0.975000 1.000000 0.987342 1.000000       1.000000\n",
            "            simetria_high        0  40   0   1 171   1.000000 0.975610 0.987654 0.999144       0.996886\n",
            "\n",
            "Archivos de evaluación guardados en: /content/drive/MyDrive/asistente_coreografico/artifacts\n",
            " - eval_overall.json (métricas globales)\n",
            " - eval_per_label.csv (métricas por etiqueta)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# === EVALUACIÓN ===\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, hamming_loss, jaccard_score,\n",
        "    precision_recall_fscore_support, roc_auc_score, average_precision_score,\n",
        "    multilabel_confusion_matrix, f1_score # Import f1_score here\n",
        ")\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUACIÓN MULTIETIQUETA – RESUMEN GLOBAL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get predictions from the complete_model\n",
        "y_pred = complete_model.predict(X_test)\n",
        "\n",
        "# Attempt to get probabilities for AUC/AP if the estimator allows it\n",
        "y_score = None\n",
        "try:\n",
        "    # Check if the model is a OneVsRestClassifier and its estimators have predict_proba\n",
        "    if hasattr(complete_model, \"estimators_\") and all(hasattr(est, \"predict_proba\") for est in complete_model.estimators_):\n",
        "        y_score = np.column_stack([est.predict_proba(X_test)[:,1] for est in complete_model.estimators_])\n",
        "    elif hasattr(complete_model, \"predict_proba\"):\n",
        "         y_score = complete_model.predict_proba(X_test)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"[AVISO] No se pudieron obtener probabilidades: {e}\")\n",
        "\n",
        "# Métricas globales\n",
        "subset_acc   = (y_pred == y_test).all(axis=1).mean()\n",
        "hamm_loss    = hamming_loss(y_test, y_pred)\n",
        "f1_micro     = f1_score(y_test, y_pred, average=\"micro\", zero_division=0)\n",
        "f1_macro2    = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)  # ya mostrado antes, lo repetimos aquí\n",
        "f1_weighted  = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
        "prec_micro   = precision_recall_fscore_support(y_test, y_pred, average=\"micro\", zero_division=0)[0]\n",
        "rec_micro    = precision_recall_fscore_support(y_test, y_pred, average=\"micro\", zero_division=0)[1]\n",
        "jaccard_samples = jaccard_score(y_test, y_pred, average=\"samples\", zero_division=0) # Corrected variable name\n",
        "jaccard_macro   = jaccard_score(y_test, y_pred, average=\"macro\", zero_division=0)   # Corrected variable name\n",
        "\n",
        "\n",
        "print(f\"Subset accuracy (exact match): {subset_acc:0.4f}\")\n",
        "print(f\"Hamming loss:                 {hamm_loss:0.4f}\")\n",
        "print(f\"F1 micro:                     {f1_micro:0.4f}\")\n",
        "print(f\"F1 macro:                     {f1_macro2:0.4f}\")\n",
        "print(f\"F1 weighted:                  {f1_weighted:0.4f}\")\n",
        "print(f\"Precisión micro:              {prec_micro:0.4f}\")\n",
        "print(f\"Recall micro:                 {rec_micro:0.4f}\")\n",
        "print(f\"Jaccard (samples):            {jaccard_samples:0.4f}\")\n",
        "print(f\"Jaccard (macro):              {jaccard_macro:0.4f}\")\n",
        "\n",
        "# Métricas por etiqueta\n",
        "per_label_rows = []\n",
        "mcm = multilabel_confusion_matrix(y_test, y_pred)\n",
        "# Use y_complete.columns to get the correct label names\n",
        "label_names_eval = y_complete.columns.tolist()\n",
        "for i, label_name in enumerate(label_names_eval):\n",
        "    tn, fp, fn, tp = mcm[i].ravel()\n",
        "\n",
        "    # Precisión/Recall/F1 por etiqueta\n",
        "    p, r, f1, support = precision_recall_fscore_support(\n",
        "        y_test[:, i], y_pred[:, i], average=\"binary\", zero_division=0\n",
        "    )\n",
        "\n",
        "    # Handle support being None explicitly\n",
        "    support_int = int(support) if support is not None else 0\n",
        "\n",
        "    # ROC-AUC y AP si hay probabilidades y ambas clases están presentes\n",
        "    roc_auc = None\n",
        "    ap = None\n",
        "    if y_score is not None:\n",
        "        # Comprobamos que en test hay positivos y negativos\n",
        "        if len(np.unique(y_test[:, i])) == 2:\n",
        "            try:\n",
        "                roc_auc = roc_auc_score(y_test[:, i], y_score[:, i])\n",
        "            except Exception:\n",
        "                roc_auc = None\n",
        "            try:\n",
        "                ap = average_precision_score(y_test[:, i], y_score[:, i])\n",
        "            except Exception:\n",
        "                ap = None\n",
        "\n",
        "    per_label_rows.append({\n",
        "        \"label\": label_name,\n",
        "        \"support\": support_int, # Use the handled support value\n",
        "        \"tp\": int(tp), \"fp\": int(fp), \"fn\": int(fn), \"tn\": int(tn),\n",
        "        \"precision\": float(p), \"recall\": float(r), \"f1\": float(f1),\n",
        "        \"roc_auc\": (None if roc_auc is None else float(roc_auc)),\n",
        "        \"avg_precision\": (None if ap is None else float(ap)),\n",
        "    })\n",
        "\n",
        "per_label_df = pd.DataFrame(per_label_rows).sort_values(\"label\")\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Métricas por etiqueta (top 10 por menor F1 para priorizar mejoras):\")\n",
        "# Ensure that there are at least 10 rows before trying to print the head of 10\n",
        "if len(per_label_df) >= 10:\n",
        "  print(per_label_df.sort_values(\"f1\").head(10).to_string(index=False))\n",
        "else:\n",
        "  print(per_label_df.sort_values(\"f1\").to_string(index=False))\n",
        "\n",
        "\n",
        "# Guardado de resultados\n",
        "overall_metrics = {\n",
        "    \"subset_accuracy\": subset_acc,\n",
        "    \"hamming_loss\": hamm_loss,\n",
        "    \"f1_micro\": f1_micro,\n",
        "    \"f1_macro\": f1_macro2,\n",
        "    \"f1_weighted\": f1_weighted,\n",
        "    \"precision_micro\": prec_micro,\n",
        "    \"recall_micro\": rec_micro,\n",
        "    \"jaccard_samples\": jaccard_samples,\n",
        "    \"jaccard_macro\": jaccard_macro,\n",
        "    \"num_labels\": int(y_complete.shape[1]), # Use y_complete for total number of labels\n",
        "    \"num_samples_test\": int(y_test.shape[0])\n",
        "}\n",
        "with open(f\"{ART}/eval_overall.json\", \"w\") as f:\n",
        "    json.dump(overall_metrics, f, indent=2)\n",
        "\n",
        "per_label_df.to_csv(f\"{ART}/eval_per_label.csv\", index=False)\n",
        "\n",
        "print(\"\\nArchivos de evaluación guardados en:\", ART)\n",
        "print(\" - eval_overall.json (métricas globales)\")\n",
        "print(\" - eval_per_label.csv (métricas por etiqueta)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F1ilJrEaJiny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62047f7f-b84e-421a-e4aa-93156fd02e8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PROBABILIDADES (top 12) con umbral por etiqueta ===\n",
            "               label  prob   thr  passes_thr  selected_final                                          group\n",
            "         frase_corta 0.757 0.500        True            True                                               \n",
            "exceso_rango_niveles 0.139 0.500       False           False ['poco_rango_niveles', 'exceso_rango_niveles']\n",
            "  poco_rango_niveles 0.001 0.500       False           False ['poco_rango_niveles', 'exceso_rango_niveles']\n",
            "\n",
            "Estrategia: per-label thresholds\n",
            "Etiquetas finales: ['frase_corta']\n",
            "Sugerencias:\n",
            " - frase_corta\n"
          ]
        }
      ],
      "source": [
        "# === INFERENCIA: umbrales por etiqueta + exclusiones + diagnóstico ===\n",
        "import os, json, numpy as np, pandas as pd, joblib\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "ART  = f\"{BASE}/artifacts\"\n",
        "PROC = f\"{BASE}/data/processed/aistpp\"\n",
        "CSV  = f\"{PROC}/features_coreograficos.csv\"\n",
        "\n",
        "# === Cargar artefactos del preprocesado y modelo ===\n",
        "imp          = joblib.load(f\"{ART}/imputer.joblib\")\n",
        "scaler       = joblib.load(f\"{ART}/scaler.joblib\")\n",
        "model        = joblib.load(f\"{ART}/model_ovr_logreg.joblib\")\n",
        "feature_cols = pd.read_csv(f\"{ART}/feature_cols.csv\", header=None)[0].tolist()\n",
        "label_names  = pd.read_csv(f\"{ART}/label_names.csv\", header=None)[0].tolist()\n",
        "\n",
        "# --- Mapeo de sugerencias (puedes ampliar este diccionario) ---\n",
        "label_to_text = {\n",
        "    \"amplitud_baja\":      \"Aumentar amplitud (extensiones y desplazamientos más amplios).\",\n",
        "    \"variedad_baja\":      \"Introducir cambios de dirección y diagonales.\",\n",
        "    \"mucha_simetria\":     \"Explorar asimetrías entre izquierda y derecha.\",\n",
        "    \"poca_simetria\":      \"Equilibrar con momentos de simetría.\",\n",
        "    \"fluidez_baja\":       \"Trabajar transiciones para mayor continuidad/fluidez.\",\n",
        "    \"poco_rango_niveles\": \"Usar niveles alto y bajo además del medio.\",\n",
        "}\n",
        "\n",
        "# --- Grupos mutuamente excluyentes (mantener solo la más probable) ---\n",
        "mutex_groups = [\n",
        "    [\"amplitud_baja\", \"amplitud_excesiva\"],\n",
        "    [\"poco_rango_niveles\", \"exceso_rango_niveles\"],\n",
        "    [\"mucha_simetria\", \"poca_simetria\"],\n",
        "    [\"variedad_baja\", \"variedad_excesiva\"],\n",
        "    [\"sincronia_baja\", \"sincronia_alta\"],\n",
        "    [\"unisono_bajo\", \"unisono_alto\"],\n",
        "    [\"formacion_compacta\", \"formacion_dispers\"],\n",
        "    [\"area_formacion_peq\", \"area_formacion_gran\"],\n",
        "    [\"poco_tiempo_suelo\", \"mucho_tiempo_suelo\"],\n",
        "]\n",
        "\n",
        "# === Utilidades ===\n",
        "def _as_dataframe_one(feats_dict, cols):\n",
        "    row = {c: feats_dict.get(c, np.nan) for c in cols}\n",
        "    return pd.DataFrame([row], columns=cols)\n",
        "\n",
        "def _predict_proba_matrix(Xm):\n",
        "    \"\"\"Devuelve matriz de probabilidades shape (n_samples, n_labels).\"\"\"\n",
        "    # OneVsRest(LogisticRegression) -> estimators_ con predict_proba\n",
        "    if hasattr(model, \"estimators_\"):\n",
        "        cols = []\n",
        "        for est in model.estimators_:\n",
        "            if hasattr(est, \"predict_proba\"):\n",
        "                cols.append(est.predict_proba(Xm)[:, 1])\n",
        "            else:\n",
        "                # Fallback con decision_function -> sigmoide\n",
        "                from scipy.special import expit\n",
        "                cols.append(expit(est.decision_function(Xm)))\n",
        "        return np.column_stack(cols)\n",
        "    # Fallback genérico\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        yps = model.predict_proba(Xm)\n",
        "        # Algunos devuelven lista de arrays por etiqueta\n",
        "        if isinstance(yps, list):\n",
        "            return np.column_stack([p[:,1] for p in yps])\n",
        "        return yps\n",
        "    if hasattr(model, \"decision_function\"):\n",
        "        from scipy.special import expit\n",
        "        return expit(model.decision_function(Xm))\n",
        "    # Último recurso: binario a float\n",
        "    return model.predict(Xm).astype(float)\n",
        "\n",
        "def _enforce_mutex(selected_labels, prob_series):\n",
        "    \"\"\"Elimina conflictos en grupos mutuamente excluyentes, dejando la etiqueta con mayor prob.\"\"\"\n",
        "    selected = set(selected_labels)\n",
        "    for group in mutex_groups:\n",
        "        # Intersección con seleccionadas\n",
        "        present = [g for g in group if g in selected]\n",
        "        if len(present) > 1:\n",
        "            # Mantener la de mayor probabilidad\n",
        "            best = max(present, key=lambda k: prob_series.get(k, 0.0))\n",
        "            for g in present:\n",
        "                if g != best:\n",
        "                    selected.discard(g)\n",
        "    return list(selected)\n",
        "\n",
        "# === Cargar o aprender umbrales por etiqueta (maximizando F1) ===\n",
        "TH_FILE = f\"{ART}/thresholds_f1.json\"\n",
        "\n",
        "def _recreate_labels_proxy(df):\n",
        "    \"\"\"Reconstruye etiquetas proxy como en entrenamiento (cuantiles 20/80, low/high).\"\"\"\n",
        "    # Columnas candidatas conocidas\n",
        "    candidates = [\"amplitud_x\",\"amplitud_y\",\"velocidad_media\",\"simetria\",\n",
        "                  \"nivel_rango\",\"variedad_direcciones\",\"frames\",\"joints\",\"dims\",\n",
        "                  \"velocidad_std\",\"vel_std\",\"disp_total\",\"trayectoria_longitud\",\n",
        "                  \"gm_sync\",\"sync\",\"gm_unison\",\"unison\",\"gm_spread\",\"spread\",\n",
        "                  \"formation_area\",\"gm_formation_area\",\"onbeat_ratio\",\"bpm\",\n",
        "                  \"alineacion_postural\",\"equilibrio\",\"curvatura_trayectoria\",\n",
        "                  \"tiempo_en_suelo\",\"amp_x\",\"amp_y\",\"simetria_raw\",\"variedad_dir\"]\n",
        "    exist = [c for c in candidates if c in df.columns]\n",
        "    q20 = df[exist].quantile(0.20)\n",
        "    q80 = df[exist].quantile(0.80)\n",
        "\n",
        "    def low(col):  return (df[col] <= q20[col]).astype(int)\n",
        "    def high(col): return (df[col] >= q80[col]).astype(int)\n",
        "\n",
        "    labels_spec = {\n",
        "        \"amplitud_baja\":       ([\"amplitud_x\",\"amplitud_y\",\"amp_x\",\"amp_y\"], \"low_any\"),\n",
        "        \"amplitud_excesiva\":   ([\"amplitud_x\",\"amplitud_y\",\"amp_x\",\"amp_y\"], \"high_any\"),\n",
        "        \"poco_rango_niveles\":  ([\"nivel_rango\"], \"low\"),\n",
        "        \"exceso_rango_niveles\":([\"nivel_rango\"], \"high\"),\n",
        "        \"fluidez_baja\":        ([\"velocidad_media\",\"vel_media\"], \"low\"),\n",
        "        \"velocidad_excesiva\":  ([\"velocidad_media\",\"vel_media\"], \"high\"),\n",
        "        \"dinamica_monotona\":   ([\"velocidad_std\",\"vel_std\"], \"low\"),\n",
        "        \"dinamica_erratica\":   ([\"velocidad_std\",\"vel_std\"], \"high\"),\n",
        "        \"poca_simetria\":       ([\"simetria\",\"simetria_raw\"], \"low\"),\n",
        "        \"mucha_simetria\":      ([\"simetria\",\"simetria_raw\"], \"high\"),\n",
        "        \"variedad_baja\":       ([\"variedad_direcciones\",\"variedad_dir\"], \"low\"),\n",
        "        \"variedad_excesiva\":   ([\"variedad_direcciones\",\"variedad_dir\"], \"high\"),\n",
        "        \"ocupacion_reducida\":  ([\"disp_total\",\"trayectoria_longitud\"], \"low\"),\n",
        "        \"traslacion_excesiva\": ([\"disp_total\",\"trayectoria_longitud\"], \"high\"),\n",
        "        \"frase_corta\":         ([\"frames\"], \"low\"),\n",
        "        \"sincronia_baja\":      ([\"gm_sync\",\"sync\"], \"low\"),\n",
        "        \"sincronia_alta\":      ([\"gm_sync\",\"sync\"], \"high\"),\n",
        "        \"unisono_bajo\":        ([\"gm_unison\",\"unison\"], \"low\"),\n",
        "        \"unisono_alto\":        ([\"gm_unison\",\"unison\"], \"high\"),\n",
        "        \"formacion_compacta\":  ([\"gm_spread\",\"spread\"], \"low\"),\n",
        "        \"formacion_dispers\":   ([\"gm_spread\",\"spread\"], \"high\"),\n",
        "        \"area_formacion_peq\":  ([\"formation_area\",\"gm_formation_area\"], \"low\"),\n",
        "        \"area_formacion_gran\": ([\"formation_area\",\"gm_formation_area\"], \"high\"),\n",
        "        \"alineacion_musical_baja\": ([\"onbeat_ratio\"], \"low\"),\n",
        "        \"alineacion_musical_alta\": ([\"onbeat_ratio\"], \"high\"),\n",
        "        \"desfase_tempo_rapido\":    ([\"bpm\",\"velocidad_media\",\"vel_media\"], \"bpm_high_vel_low\"),\n",
        "        \"desfase_tempo_lento\":     ([\"bpm\",\"velocidad_media\",\"vel_media\"], \"bpm_low_vel_high\"),\n",
        "        \"alineacion_postural_baja\": ([\"alineacion_postural\"], \"low\"),\n",
        "        \"alineacion_postural_alta\": ([\"alineacion_postural\"], \"high\"),\n",
        "        \"equilibrio_bajo\":          ([\"equilibrio\"], \"low\"),\n",
        "        \"equilibrio_alto\":          ([\"equilibrio\"], \"high\"),\n",
        "        \"curvatura_baja\":           ([\"curvatura_trayectoria\"], \"low\"),\n",
        "        \"curvatura_alta\":           ([\"curvatura_trayectoria\"], \"high\"),\n",
        "        \"poco_tiempo_suelo\":        ([\"tiempo_en_suelo\"], \"low\"),\n",
        "        \"mucho_tiempo_suelo\":       ([\"tiempo_en_suelo\"], \"high\"),\n",
        "    }\n",
        "\n",
        "    ys = {}\n",
        "    for name, (cols, mode) in labels_spec.items():\n",
        "        missing = [c for c in cols if c not in df.columns]\n",
        "        if missing:\n",
        "            continue\n",
        "        if mode == \"low\":\n",
        "            ys[name] = low(cols[0])\n",
        "        elif mode == \"high\":\n",
        "            ys[name] = high(cols[0])\n",
        "        elif mode == \"low_any\":\n",
        "            v = np.zeros(len(df), dtype=int)\n",
        "            for c in cols:\n",
        "                v = np.maximum(v, low(c).values)\n",
        "            ys[name] = v\n",
        "        elif mode == \"high_any\":\n",
        "            v = np.zeros(len(df), dtype=int)\n",
        "            for c in cols:\n",
        "                v = np.maximum(v, high(c).values)\n",
        "            ys[name] = v\n",
        "        elif mode == \"bpm_high_vel_low\":\n",
        "            if \"bpm\" in df.columns and ((\"velocidad_media\" in df.columns) or (\"vel_media\" in df.columns)):\n",
        "                vlow = df[\"velocidad_media\"] if \"velocidad_media\" in df.columns else df[\"vel_media\"]\n",
        "                ys[name] = ((df[\"bpm\"] >= df[\"bpm\"].quantile(0.80)) & (vlow <= vlow.quantile(0.20))).astype(int)\n",
        "        elif mode == \"bpm_low_vel_high\":\n",
        "            if \"bpm\" in df.columns and ((\"velocidad_media\" in df.columns) or (\"vel_media\" in df.columns)):\n",
        "                vlow = df[\"velocidad_media\"] if \"velocidad_media\" in df.columns else df[\"vel_media\"]\n",
        "                ys[name] = ((df[\"bpm\"] <= df[\"bpm\"].quantile(0.20)) & (vlow >= vlow.quantile(0.80))).astype(int)\n",
        "    if not ys:\n",
        "        raise RuntimeError(\"No se pudieron recrear etiquetas proxy para aprender umbrales.\")\n",
        "    Y = pd.DataFrame(ys)\n",
        "    # Asegurar mismo orden que label_names (filtrando a las que existan)\n",
        "    kept = [l for l in label_names if l in Y.columns]\n",
        "    return Y[kept]\n",
        "\n",
        "def _learn_thresholds_f1(df, save_path=TH_FILE):\n",
        "    \"\"\"Aprende umbral por etiqueta maximizando F1 usando el CSV y el modelo actual.\"\"\"\n",
        "    # Preparar X\n",
        "    Xraw = df[feature_cols].copy()\n",
        "    Ximp = imp.transform(Xraw)\n",
        "    X    = scaler.transform(Ximp)\n",
        "    # Probabilidades\n",
        "    probs = _predict_proba_matrix(X)  # shape (N, L)\n",
        "    # Etiquetas proxy\n",
        "    Y = _recreate_labels_proxy(df)    # shape (N, L') con subset de labels\n",
        "    # Alinear dimensiones si falta alguna etiqueta\n",
        "    kept_labels = list(Y.columns)\n",
        "    kept_idx = [label_names.index(l) for l in kept_labels]\n",
        "    probs = probs[:, kept_idx]\n",
        "\n",
        "    thresholds = {}\n",
        "    for j, lbl in enumerate(kept_labels):\n",
        "        y_true = Y.values[:, j].astype(int)\n",
        "        p = probs[:, j]\n",
        "        # Si todas son 0/1 constantes, usar 0.5 por defecto\n",
        "        if len(np.unique(y_true)) < 2:\n",
        "            thresholds[lbl] = 0.5\n",
        "            continue\n",
        "        # Buscar umbral que maximiza F1\n",
        "        # Probamos umbrales únicos de p + bordes\n",
        "        cand = np.unique(np.r_[0.0, p, 1.0])\n",
        "        best_f1, best_t = -1.0, 0.5\n",
        "        for t in cand:\n",
        "            yhat = (p >= t).astype(int)\n",
        "            f1 = f1_score(y_true, yhat, zero_division=0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1, best_t = f1, t\n",
        "        thresholds[lbl] = float(best_t)\n",
        "    # Añadir las etiquetas sin proxy (usar 0.5)\n",
        "    for lbl in label_names:\n",
        "        if lbl not in thresholds:\n",
        "            thresholds[lbl] = 0.5\n",
        "    with open(save_path, \"w\") as f:\n",
        "        json.dump(thresholds, f, indent=2)\n",
        "    print(f\"[INFO] Umbrales por etiqueta guardados en {save_path}\")\n",
        "    return thresholds\n",
        "\n",
        "def _load_or_learn_thresholds():\n",
        "    if os.path.exists(TH_FILE):\n",
        "        with open(TH_FILE, \"r\") as f:\n",
        "            th = json.load(f)\n",
        "        # Asegurar que están todas las labels\n",
        "        for lbl in label_names:\n",
        "            th.setdefault(lbl, 0.5)\n",
        "        return th\n",
        "    # Aprender al vuelo desde el CSV\n",
        "    df_all = pd.read_csv(CSV)\n",
        "    return _learn_thresholds_f1(df_all, save_path=TH_FILE)\n",
        "\n",
        "label_thresholds = _load_or_learn_thresholds()\n",
        "\n",
        "def inferir_desde_features_dict(\n",
        "    feats: dict,\n",
        "    global_fallback_k=3,\n",
        "    global_fallback_min=0.25\n",
        "):\n",
        "    \"\"\"\n",
        "    - Aplica umbral ESPECÍFICO por etiqueta (aprendido o 0.5).\n",
        "    - Si ninguna supera su umbral -> fallback: top-k ≥ global_fallback_min (o top-1).\n",
        "    - Resuelve exclusiones mutuamente excluyentes conservando la etiqueta con mayor probabilidad.\n",
        "    Devuelve: (pred_labels, sugerencias, df_diag_ordenado)\n",
        "    \"\"\"\n",
        "    df1   = _as_dataframe_one(feats, feature_cols)\n",
        "    X_imp = imp.transform(df1)\n",
        "    X     = scaler.transform(X_imp)\n",
        "    probs = _predict_proba_matrix(X)[0]  # vector (L,)\n",
        "    prob_series = pd.Series(probs, index=label_names)\n",
        "\n",
        "    # Decisión por umbral específico\n",
        "    decided = []\n",
        "    for lbl, p in prob_series.items():\n",
        "        t = label_thresholds.get(lbl, 0.5)\n",
        "        if p >= t:\n",
        "            decided.append(lbl)\n",
        "\n",
        "    # Fallback si nada supera su umbral\n",
        "    strategy = \"per-label thresholds\"\n",
        "    if len(decided) == 0:\n",
        "        strategy = f\"fallback(top-{global_fallback_k}, min={global_fallback_min})\"\n",
        "        topk = prob_series.sort_values(ascending=False)\n",
        "        decided = topk[topk >= global_fallback_min].head(global_fallback_k).index.tolist()\n",
        "        if len(decided) == 0 and len(topk) > 0:\n",
        "            decided = [topk.index[0]]\n",
        "\n",
        "    # Resolver exclusiones\n",
        "    selected = _enforce_mutex(decided, prob_series.to_dict())\n",
        "\n",
        "    # Construir diagnóstico ordenado\n",
        "    rows = []\n",
        "    for lbl in label_names:\n",
        "        rows.append({\n",
        "            \"label\": lbl,\n",
        "            \"prob\": float(prob_series[lbl]),\n",
        "            \"thr\": float(label_thresholds.get(lbl, 0.5)),\n",
        "            \"passes_thr\": lbl in decided,\n",
        "            \"selected_final\": lbl in selected,\n",
        "            \"group\": next((str(g) for g in mutex_groups if lbl in g), \"\")\n",
        "        })\n",
        "    diag = pd.DataFrame(rows).sort_values(\"prob\", ascending=False)\n",
        "\n",
        "    # Sugerencias ordenadas por prob\n",
        "    sugerencias = [label_to_text.get(lbl, lbl) for lbl in sorted(selected, key=lambda k: prob_series[k], reverse=True)]\n",
        "\n",
        "    # Imprimir resumen legible\n",
        "    print(\"\\n=== PROBABILIDADES (top 12) con umbral por etiqueta ===\")\n",
        "    print(diag.head(12)[[\"label\",\"prob\",\"thr\",\"passes_thr\",\"selected_final\",\"group\"]].to_string(index=False,\n",
        "          formatters={\"prob\": lambda v: f\"{v:0.3f}\", \"thr\": lambda v: f\"{v:0.3f}\"}))\n",
        "    print(f\"\\nEstrategia: {strategy}\")\n",
        "    print(\"Etiquetas finales:\", selected)\n",
        "    print(\"Sugerencias:\")\n",
        "    for s in sugerencias:\n",
        "        print(\" -\", s)\n",
        "\n",
        "    return selected, sugerencias, diag\n",
        "\n",
        "# === Ejemplo con un registro del CSV (fila 0) ===\n",
        "df_all = pd.read_csv(CSV)\n",
        "row0 = df_all.iloc[0].to_dict()\n",
        "feats0 = {c: row0[c] for c in feature_cols if c in row0}\n",
        "\n",
        "preds, suggs, diag = inferir_desde_features_dict(\n",
        "    feats0,\n",
        "    global_fallback_k=3,\n",
        "    global_fallback_min=0.25\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G_6FyGNBVmbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a460c157-2bb0-4e70-d13a-70eef99125c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔️ Pipeline guardado en: /content/drive/MyDrive/asistente_coreografico/models/pipeline_ovr_logreg.joblib\n",
            "✔️ Metadatos guardados en: /content/drive/MyDrive/asistente_coreografico/models/pipeline_metadata.json\n",
            "Pred shape: (1408, 3) | Proba shape: (1408, 3)\n",
            "✔️ Inferencia realizada con columnas alineadas: ['amplitud_x', 'amplitud_y', 'velocidad_media', 'simetria', 'nivel_rango', 'variedad_direcciones', 'frames', 'joints', 'dims']\n"
          ]
        }
      ],
      "source": [
        "# === GUARDAR MODELO EN /models ===\n",
        "\n",
        "import os, json\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.pipeline import Pipeline as SkPipeline\n",
        "\n",
        "# Rutas base\n",
        "BASE        = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "ART         = f\"{BASE}/artifacts\"\n",
        "MODELS_DIR  = f\"{BASE}/models\"\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# Cargar objetos desde memoria o desde /artifacts\n",
        "try:\n",
        "    imp\n",
        "    scaler\n",
        "    clf\n",
        "except NameError:\n",
        "    # Si no están en el workspace actual, cargamos desde disco\n",
        "    imp_path    = f\"{ART}/imputer.joblib\"\n",
        "    scaler_path = f\"{ART}/scaler.joblib\"\n",
        "    clf_path    = f\"{ART}/model_ovr_logreg.joblib\"\n",
        "    assert os.path.exists(imp_path),    f\"Falta: {imp_path}\"\n",
        "    assert os.path.exists(scaler_path), f\"Falta: {scaler_path}\"\n",
        "    assert os.path.exists(clf_path),    f\"Falta: {clf_path}\"\n",
        "    imp    = joblib.load(imp_path)\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    clf    = joblib.load(clf_path)\n",
        "\n",
        "#  Cargar columnas de features y nombres de etiquetas\n",
        "try:\n",
        "    feature_cols\n",
        "except NameError:\n",
        "    feat_path = f\"{ART}/feature_cols.csv\"\n",
        "    assert os.path.exists(feat_path), f\"Falta: {feat_path}\"\n",
        "    feature_cols = pd.read_csv(feat_path, header=None)[0].tolist()\n",
        "\n",
        "try:\n",
        "    label_names\n",
        "except NameError:\n",
        "    labels_path = f\"{ART}/label_names.csv\"\n",
        "    assert os.path.exists(labels_path), f\"Falta: {labels_path}\"\n",
        "    label_names = pd.read_csv(labels_path, header=None)[0].tolist()\n",
        "\n",
        "# Construir Pipeline y guardar\n",
        "pipe = SkPipeline([\n",
        "    (\"imputer\", imp),     # SimpleImputer ya ajustado\n",
        "    (\"scaler\",  scaler),  # RobustScaler ya ajustado\n",
        "    (\"clf\",     clf)      # OneVsRest(LogisticRegression) ya ajustado\n",
        "])\n",
        "\n",
        "pipe_path = f\"{MODELS_DIR}/pipeline_ovr_logreg.joblib\"\n",
        "joblib.dump(pipe, pipe_path)\n",
        "print(f\"✔️ Pipeline guardado en: {pipe_path}\")\n",
        "\n",
        "# Metadatos del pipeline\n",
        "meta = {\n",
        "    \"sklearn_version\": sklearn.__version__,\n",
        "    \"feature_cols\": list(feature_cols),\n",
        "    \"label_names\": list(label_names),\n",
        "    \"notes\": \"Pipeline con SimpleImputer + RobustScaler + OneVsRest(LogReg) entrenado.\"\n",
        "}\n",
        "meta_path = f\"{MODELS_DIR}/pipeline_metadata.json\"\n",
        "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
        "print(f\"✔️ Metadatos guardados en: {meta_path}\")\n",
        "\n",
        "# Ejemplo de carga rápida (opcional)\n",
        "try:\n",
        "    _ = X_raw  # si existe en el entorno actual\n",
        "except NameError:\n",
        "    # Si no tienes X_raw en memoria, puedes construirlo así (descomenta y ajusta):\n",
        "    # CSV = f\"{BASE}/data/processed/aistpp/features_coreograficos.csv\"\n",
        "    # assert os.path.exists(CSV), f\"No existe el CSV de features: {CSV}\"\n",
        "    # df = pd.read_csv(CSV)\n",
        "    # feature_cols_to_use = ['amplitud_x', 'amplitud_y', 'amplitud_z', 'velocidad_media',\n",
        "    #                       'simetria', 'nivel_rango', 'variedad_direcciones', 'frames']\n",
        "    # feature_cols_to_use = [col for col in feature_cols_to_use if col in df.columns]\n",
        "    # X_raw = df[feature_cols_to_use].copy()\n",
        "    X_raw = None\n",
        "\n",
        "\n",
        "pipe = joblib.load(pipe_path)\n",
        "\n",
        "def _align_features_df(df_in: pd.DataFrame, required_cols: list) -> pd.DataFrame:\n",
        "    \"\"\"Devuelve un DF con exactamente las columnas usadas en entrenamiento, en el mismo orden.\n",
        "       Columnas faltantes se rellenan con NaN (las tratará el Imputer).\"\"\"\n",
        "    df = pd.DataFrame(index=df_in.index)\n",
        "    for c in required_cols:\n",
        "        if c in df_in.columns:\n",
        "            df[c] = pd.to_numeric(df_in[c], errors=\"coerce\")\n",
        "        else:\n",
        "            df[c] = np.nan\n",
        "    return df[required_cols]\n",
        "\n",
        "# Si no tienes X_raw en memoria, carga el CSV y alinéalo a feature_cols\n",
        "if 'X_raw' not in globals() or X_raw is None:\n",
        "    CSV = f\"{BASE}/data/processed/aistpp/features_coreograficos.csv\"\n",
        "    assert os.path.exists(CSV), f\"No existe el CSV de features: {CSV}\"\n",
        "    df_features = pd.read_csv(CSV)\n",
        "    X_raw = _align_features_df(df_features, feature_cols)\n",
        "else:\n",
        "    # Si ya existe X_raw, asegúrate de que es DataFrame y alinéalo\n",
        "    if isinstance(X_raw, np.ndarray):\n",
        "        # Si te pasaron un array, lo convertimos a DF sin nombres => no es válido para verificación de columnas\n",
        "        # Carga el CSV para obtener columnas y reemplaza X_raw por el dataset alineado\n",
        "        CSV = f\"{BASE}/data/processed/aistpp/features_coreograficos.csv\"\n",
        "        df_features = pd.read_csv(CSV)\n",
        "        X_raw = _align_features_df(df_features, feature_cols)\n",
        "    else:\n",
        "        X_raw = _align_features_df(X_raw, feature_cols)\n",
        "\n",
        "# Predicción ya con columnas correctas\n",
        "y_pred  = pipe.predict(X_raw)\n",
        "# predict_proba disponible con OneVsRest(LogReg)\n",
        "try:\n",
        "    y_proba = pipe.predict_proba(X_raw)\n",
        "except Exception:\n",
        "    # Fallback si no existiera predict_proba\n",
        "    if hasattr(pipe, \"decision_function\"):\n",
        "        logits = pipe.decision_function(X_raw)\n",
        "        y_proba = 1.0 / (1.0 + np.exp(-logits))\n",
        "    else:\n",
        "        y_proba = None\n",
        "\n",
        "print(\"Pred shape:\", y_pred.shape, \"| Proba shape:\", None if y_proba is None else y_proba.shape)\n",
        "print(\"✔️ Inferencia realizada con columnas alineadas:\", feature_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6XM-bPjNz__"
      },
      "source": [
        "# **Utilidades y carga de artefactos (features + ML)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "R3HyyJKxebgt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "29dda8ee-ccfb-43d9-cd5d-e5ee2178d847"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ultralytics'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3094081978.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NumPy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# 1.26.4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OpenCV:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# 4.10.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np, cv2, torch, torchvision\n",
        "from ultralytics import YOLO\n",
        "\n",
        "print(\"NumPy:\", np.__version__)             # 1.26.4\n",
        "print(\"OpenCV:\", cv2.__version__)           # 4.10.0\n",
        "print(\"Torch:\", torch.__version__)          # 2.4.1\n",
        "print(\"Torchvision:\", torchvision.__version__)  # 0.19.1\n",
        "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
        "\n",
        "# Smoke test de YOLO Pose (imagen en blanco)\n",
        "model = YOLO(\"yolov8n-pose.pt\")  # se descarga la primera vez\n",
        "dummy = np.zeros((480, 640, 3), dtype=np.uint8)\n",
        "res = model.predict(source=dummy, conf=0.25, iou=0.5, verbose=False)\n",
        "print(\"YOLO pose OK. Detecciones:\", len(res[0].boxes) if len(res) else 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr3C-F0iKrE4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "ART  = f\"{BASE}/artifacts\"\n",
        "os.makedirs(ART, exist_ok=True)\n",
        "\n",
        "# Cargar artefactos de entrenamiento\n",
        "imp          = joblib.load(f\"{ART}/imputer.joblib\")\n",
        "scaler       = joblib.load(f\"{ART}/scaler.joblib\")\n",
        "model        = joblib.load(f\"{ART}/model_ovr_logreg.joblib\")\n",
        "feature_cols = pd.read_csv(f\"{ART}/feature_cols.csv\", header=None)[0].tolist()\n",
        "label_names  = pd.read_csv(f\"{ART}/label_names.csv\", header=None)[0].tolist()\n",
        "\n",
        "label_to_text = {\n",
        "    \"amplitud_baja\":      \"Aumentar amplitud (extensiones y desplazamientos más amplios).\",\n",
        "    \"variedad_baja\":      \"Introducir cambios de dirección y diagonales.\",\n",
        "    \"mucha_simetria\":     \"Explorar asimetrías entre izquierda y derecha.\",\n",
        "    \"poca_simetria\":      \"Equilibrar con momentos de simetría.\",\n",
        "    \"fluidez_baja\":       \"Trabajar transiciones para mayor continuidad/fluidez.\",\n",
        "    \"poco_rango_niveles\": \"Usar niveles alto y bajo además del medio.\",\n",
        "}\n",
        "\n",
        "def get_pairs(J):\n",
        "    # COCO-17 (YOLO-Pose): hombros/codos/muñecas/caderas/rodillas/tobillos\n",
        "    return [(5,6),(7,8),(9,10),(11,12),(13,14),(15,16)]\n",
        "\n",
        "def center_of_mass(K):\n",
        "    # COCO: caderas 11 y 12\n",
        "    T,J,D = K.shape\n",
        "    if J >= 13:\n",
        "        return (K[:,11,:] + K[:,12,:]) / 2\n",
        "    return K.mean(axis=1)\n",
        "\n",
        "def features_coreograficos(K):\n",
        "    T,J,D = K.shape\n",
        "    amp_mean = (np.nanmax(K,axis=0) - np.nanmin(K,axis=0)).mean(axis=0)\n",
        "    amp_x = float(amp_mean[0]); amp_y = float(amp_mean[1])\n",
        "    amp_z = float(amp_mean[2]) if D==3 else np.nan\n",
        "    vel = np.linalg.norm(np.diff(K,axis=0),axis=2)\n",
        "    vel_media = float(np.nanmean(vel)) if vel.size else 0.0\n",
        "    pair_vals=[]\n",
        "    for a,b in get_pairs(J):\n",
        "        if a<J and b<J:\n",
        "            diff = K[:,a,:]-K[:,b,:]\n",
        "            if diff.ndim > 1:\n",
        "                d = np.linalg.norm(diff,axis=1).mean() # Correct axis for (T, D) -> (T,)\n",
        "                pair_vals.append(np.nanmean(d))\n",
        "            elif diff.ndim == 1: # Handle case where diff is 1D (T,)\n",
        "                 d = np.linalg.norm(diff).mean()\n",
        "                 pair_vals.append(np.nanmean(d))\n",
        "    sim_raw = float(np.nanmean(pair_vals)) if pair_vals else np.nan\n",
        "    com = center_of_mass(K)\n",
        "    nivel_p10 = float(np.nanpercentile(com[:,1],10))\n",
        "    nivel_p90 = float(np.nanpercentile(com[:,1],90))\n",
        "    nivel_rango = float(nivel_p10 - nivel_p90)\n",
        "    disp = np.diff(com,axis=0)\n",
        "    dirs = np.arctan2(disp[:,1], disp[:,0] + 1e-9)\n",
        "    cambios = np.abs(np.diff(dirs))\n",
        "    variedad_dir = float(np.nanmean(cambios)) if len(cambios)>0 else 0.0\n",
        "    return {\n",
        "        \"amp_x\": amp_x, \"amp_y\": amp_y, \"amp_z\": amp_z,\n",
        "        \"vel_media\": vel_media,\n",
        "        \"simetria_raw\": sim_raw,\n",
        "        \"nivel_rango\": nivel_rango,\n",
        "        \"variedad_dir\": variedad_dir,\n",
        "        \"frames\": int(T), \"joints\": int(J), \"dims\": int(D)\n",
        "    }\n",
        "\n",
        "def inferir_desde_features_dict(feats: dict):\n",
        "    x = np.array([[feats.get(c, np.nan) for c in feature_cols]], dtype=float)\n",
        "    x = scaler.transform(imp.transform(x))\n",
        "    yhat = model.predict(x)[0]\n",
        "    pred_labels = [label_names[i] for i,v in enumerate(yhat) if v==1]\n",
        "    sugerencias = [label_to_text.get(lbl, lbl) for lbl in pred_labels]\n",
        "    return pred_labels, sugerencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slsBbH6pN7Ia"
      },
      "source": [
        "# **Vídeo → Keypoints con YOLO-Pose**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMyHLOgekE7G"
      },
      "outputs": [],
      "source": [
        "import os, glob, shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Carpeta donde quieres guardar los pesos .pt\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "MODELS_DIR = f\"{BASE}/models\"\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "def _resolve_downloaded_weight_path(model_obj, model_name: str):\n",
        "    \"\"\"\n",
        "    Intenta localizar en disco el .pt que YOLO descargó al crear el modelo.\n",
        "    \"\"\"\n",
        "    # 1) Atributo típico en Ultralytics v8\n",
        "    p = getattr(model_obj, \"ckpt_path\", None)\n",
        "    if isinstance(p, str) and os.path.exists(p):\n",
        "        return p\n",
        "\n",
        "    # 2) Búsqueda en caches comunes\n",
        "    candidates_roots = [\n",
        "        os.path.expanduser(\"~/.cache/ultralytics\"),\n",
        "        os.path.expanduser(\"~/.cache/torch/hub\"),\n",
        "        os.path.expanduser(\"~/.cache\"),\n",
        "        \"/root/.cache\",\n",
        "        \"/content\",\n",
        "    ]\n",
        "    for root in candidates_roots:\n",
        "        try:\n",
        "            for hit in glob.glob(os.path.join(root, \"**\", model_name), recursive=True):\n",
        "                if os.path.isfile(hit):\n",
        "                    return hit\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 3) No encontrado\n",
        "    return None\n",
        "\n",
        "def _load_or_download_yolo(model_name: str, save_dir: str):\n",
        "    \"\"\"\n",
        "    Si el peso existe en save_dir, lo carga desde ahí.\n",
        "    Si no existe, instancia YOLO(model_name) para forzar descarga,\n",
        "    luego copia el .pt descargado a save_dir y devuelve el modelo cargado desde ahí.\n",
        "    \"\"\"\n",
        "    target_path = os.path.join(save_dir, model_name)\n",
        "    if os.path.exists(target_path):\n",
        "        # Cargar directamente desde el archivo ya guardado en Drive\n",
        "        return YOLO(target_path), target_path\n",
        "\n",
        "    # Forzar descarga (YOLO gestiona la descarga al instanciar con el nombre)\n",
        "    tmp_model = YOLO(model_name)\n",
        "    resolved = _resolve_downloaded_weight_path(tmp_model, model_name)\n",
        "\n",
        "    if resolved and os.path.exists(resolved):\n",
        "        # Copiar a tu carpeta de modelos en Drive\n",
        "        shutil.copy2(resolved, target_path)\n",
        "        print(f\"✔️ Peso descargado y copiado a: {target_path}\")\n",
        "        # Cargar desde la ruta destino (para que desde ahora siempre use ese .pt)\n",
        "        return YOLO(target_path), target_path\n",
        "    else:\n",
        "        # Si no logramos resolver el archivo, seguimos usando el objeto ya cargado\n",
        "        print(\" No se pudo resolver la ruta física del .pt descargado; \"\n",
        "              \"se usará el modelo en memoria. (Se intentó buscar en caches comunes).\")\n",
        "        return tmp_model, None\n",
        "\n",
        "def video_to_keypoints_yolo(\n",
        "    video_path,\n",
        "    model_name=\"yolov8n-pose.pt\",\n",
        "    conf=0.25,\n",
        "    iou=0.5,\n",
        "    stride=1,\n",
        "    max_people=1\n",
        "):\n",
        "    \"\"\"\n",
        "    Extrae keypoints COCO-17 (x,y,score) por frame usando YOLOv8 Pose.\n",
        "    Devuelve:\n",
        "      - kpts_main: (N, 17, 3) de la persona principal\n",
        "      - fps: frames por segundo del vídeo\n",
        "    \"\"\"\n",
        "    assert os.path.exists(video_path), f\"No existe el video: {video_path}\"\n",
        "\n",
        "    # Cargar/descargar y guardar en MODELS_DIR\n",
        "    model, weight_path = _load_or_download_yolo(model_name, MODELS_DIR)\n",
        "    if weight_path:\n",
        "        print(f\"Usando pesos desde: {weight_path}\")\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"No se pudo abrir el video: {video_path}\")\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "\n",
        "    frames = []\n",
        "    idx = 0\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        if (idx % max(1, stride)) != 0:\n",
        "            idx += 1\n",
        "            continue\n",
        "\n",
        "        # Inferencia por frame\n",
        "        res = model.predict(source=frame, conf=conf, iou=iou, verbose=False)\n",
        "        kpts_list = []\n",
        "        if len(res) and hasattr(res[0], \"keypoints\") and res[0].keypoints is not None:\n",
        "            xy = res[0].keypoints.xy\n",
        "            cf = res[0].keypoints.conf\n",
        "            xy = xy.cpu().numpy() if hasattr(xy, \"cpu\") else np.asarray(xy)\n",
        "            if cf is None:\n",
        "                cf = np.ones_like(xy[..., 0])\n",
        "            else:\n",
        "                cf = cf.cpu().numpy() if hasattr(cf, \"cpu\") else np.asarray(cf)\n",
        "\n",
        "            for i in range(xy.shape[0]):\n",
        "                # (17,2) + (17,1) -> (17,3)\n",
        "                k = np.concatenate([xy[i], cf[i][..., None]], axis=-1)\n",
        "                kpts_list.append(k)\n",
        "\n",
        "        if kpts_list:\n",
        "            # elige persona principal por mayor score medio\n",
        "            scores = []\n",
        "            for k in kpts_list:\n",
        "                # Calcular score medio, manejando posibles NaN\n",
        "                valid_scores = k[:, 2][~np.isnan(k[:, 2])]\n",
        "                if len(valid_scores) > 0:\n",
        "                    scores.append(np.mean(valid_scores))\n",
        "                else:\n",
        "                    scores.append(-np.inf)  # Si todos son NaN, usar -infinito\n",
        "\n",
        "            # Verificar que haya al menos un score válido\n",
        "            if not all(score == -np.inf for score in scores):\n",
        "                main = kpts_list[np.argmax(scores)]\n",
        "                frames.append(main)\n",
        "            else:\n",
        "                # Si todos los scores son inválidos, agregar array de NaN\n",
        "                frames.append(np.full((17, 3), np.nan))\n",
        "        else:\n",
        "            # Si no hay detecciones, agregar array de NaN\n",
        "            frames.append(np.full((17, 3), np.nan))\n",
        "\n",
        "        idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    if not frames:\n",
        "        return None, fps\n",
        "    return np.stack(frames, axis=0), fps\n",
        "\n",
        "\n",
        "# --- EJEMPLO DE USO ---\n",
        "demo_path = f\"{BASE}/demo/solo.mp4\"\n",
        "kpts, fps = video_to_keypoints_yolo(demo_path, model_name=\"yolov8n-pose.pt\", conf=0.25, stride=1)\n",
        "print(\"FPS:\", fps)\n",
        "print(\"Keypoints shape:\", None if kpts is None else kpts.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lq7pp9vyKutw"
      },
      "outputs": [],
      "source": [
        "# Selecciona una sola persona por frame (la de mayor área de bounding box).\n",
        "# Devuelve K con forma (T, 17, 2) en píxeles.\n",
        "# Si no detecta a nadie en un frame, deja NaN para ese frame (luego interpolamos).\n",
        "\n",
        "import cv2, numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def video_to_keypoints_yolo(video_path, model_name=\"yolov8n-pose.pt\", conf=0.25, stride=1):\n",
        "    \"\"\"\n",
        "    Devuelve K: (T, 17, 2) en píxeles (x,y) con NaNs si no hay detección.\n",
        "    Selecciona la persona con mayor caja por frame.\n",
        "    \"\"\"\n",
        "    model = YOLO(model_name)  # descarga automática del modelo\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    assert cap.isOpened(), f\"No se pudo abrir el vídeo: {video_path}\"\n",
        "\n",
        "    T = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    FPS = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "\n",
        "    J = 17  # COCO-17 en YOLO-Pose\n",
        "    K = np.full((T, J, 2), np.nan, dtype=np.float32)\n",
        "\n",
        "    t = 0\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        if t % stride != 0:\n",
        "            t += 1\n",
        "            continue\n",
        "\n",
        "        # Predicción\n",
        "        res = model.predict(frame, conf=conf, verbose=False)[0]\n",
        "        if res.boxes is not None and len(res.boxes) > 0 and res.keypoints is not None:\n",
        "            # Elegir persona con caja más grande\n",
        "            boxes = res.boxes.xyxy.cpu().numpy()  # (N,4)\n",
        "            areas = (boxes[:,2]-boxes[:,0]) * (boxes[:,3]-boxes[:,1])\n",
        "            idx = int(np.argmax(areas))\n",
        "            kps = res.keypoints.xy[idx].cpu().numpy()  # (17,2) en píxeles\n",
        "            # Algunos modelos devuelven flotantes fuera de imagen; clamp suave\n",
        "            kps[:,0] = np.clip(kps[:,0], 0, W-1)\n",
        "            kps[:,1] = np.clip(kps[:,1], 0, H-1)\n",
        "            K[t] = kps\n",
        "\n",
        "        t += 1\n",
        "\n",
        "    cap.release()\n",
        "    return K, FPS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h26Q2wJFPmUu"
      },
      "source": [
        "**Limpieza/Interpolación, “sliding window” + CSV + vídeo anotado**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3-R9-03KyOe"
      },
      "outputs": [],
      "source": [
        "# ===== limpieza NaNs + features + overlay + EJECUCIÓN =====\n",
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import joblib\n",
        "\n",
        "# --- Rutas base ---\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "ART  = f\"{BASE}/artifacts\"\n",
        "OUT  = f\"{BASE}/data/processed/aistpp\"\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "# --- Utilidades NaN ---\n",
        "def interpolate_nan_1d(y):\n",
        "    y = y.astype(np.float32)\n",
        "    T = len(y); idx = np.arange(T)\n",
        "    mask = np.isfinite(y)\n",
        "    if mask.sum() == 0: return y\n",
        "    last = np.nan\n",
        "    for i in range(T):\n",
        "        if not np.isfinite(y[i]) and np.isfinite(last): y[i] = last\n",
        "        else: last = y[i]\n",
        "    last = np.nan\n",
        "    for i in range(T-1, -1, -1):\n",
        "        if not np.isfinite(y[i]) and np.isfinite(last): y[i] = last\n",
        "        else: last = y[i]\n",
        "    mask = np.isfinite(y)\n",
        "    if 2 <= mask.sum() < T:\n",
        "        y[~mask] = np.interp(idx[~mask], idx[mask], y[mask])\n",
        "    return y\n",
        "\n",
        "def clean_nan_interpolate(K, min_valid_ratio=0.10):\n",
        "    if K is None or len(K) == 0:\n",
        "        return None, []\n",
        "    Kc = K.copy()\n",
        "    T,J,D = Kc.shape\n",
        "    used = []\n",
        "    for j in range(J):\n",
        "        valid = np.isfinite(Kc[:,j,:]).all(axis=1)\n",
        "        if valid.mean() < min_valid_ratio:\n",
        "            Kc[:,j,:] = np.nan\n",
        "            continue\n",
        "        for d in range(D):\n",
        "            Kc[:,j,d] = interpolate_nan_1d(Kc[:,j,d])\n",
        "        used.append(j)\n",
        "    if not used: return Kc, []\n",
        "    return Kc[:,used,:], used\n",
        "\n",
        "# --- Esqueleto COCO-17 (pares básicos brazo/torso/pierna) ---\n",
        "COCO_EDGES = [(5,7),(7,9),(6,8),(8,10),(11,13),(13,15),\n",
        "              (12,14),(14,16),(5,6),(11,12),(5,11),(6,12)]\n",
        "\n",
        "# --- Dibujo de overlay (corrige desempaquetado x,y,score) ---\n",
        "def overlay_suggestions_on_video(video_path, K, timeline, out_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    assert cap.isOpened(), f\"No se pudo abrir el vídeo: {video_path}\"\n",
        "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    FPS = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    vw = cv2.VideoWriter(out_path, fourcc, FPS, (W,H))\n",
        "\n",
        "    t = 0\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok: break\n",
        "\n",
        "        if K is not None and t < len(K):\n",
        "            P = K[t]  # (J,3) -> x,y,score\n",
        "            # esqueleto\n",
        "            for a,b in COCO_EDGES:\n",
        "                if a < P.shape[0] and b < P.shape[0]:\n",
        "                    xa,ya = P[a,:2]; xb,yb = P[b,:2]\n",
        "                    if np.isfinite(xa) and np.isfinite(ya) and np.isfinite(xb) and np.isfinite(yb):\n",
        "                        cv2.line(frame, (int(xa),int(ya)), (int(xb),int(yb)), (255,255,255), 2)\n",
        "            for j in range(P.shape[0]):\n",
        "                x,y = P[j,:2]\n",
        "                if np.isfinite(x) and np.isfinite(y):\n",
        "                    cv2.circle(frame, (int(x),int(y)), 2, (255,255,255), -1)\n",
        "\n",
        "        # sugerencias activas en frame t\n",
        "        active = []\n",
        "        for seg in timeline:\n",
        "            if seg[\"start_f\"] <= t <= seg[\"end_f\"]:\n",
        "                active = seg.get(\"sugerencias\", [])\n",
        "        y0 = 30\n",
        "        for s in active[:4]:\n",
        "            cv2.putText(frame, f\"• {s}\", (20, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 4, cv2.LINE_AA)\n",
        "            cv2.putText(frame, f\"• {s}\", (20, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1, cv2.LINE_AA)\n",
        "            y0 += 28\n",
        "\n",
        "        vw.write(frame); t += 1\n",
        "\n",
        "    cap.release(); vw.release()\n",
        "    return out_path\n",
        "\n",
        "# --- Features simples desde COCO-2D (coherentes con tu set básico) ---\n",
        "def features_coreograficos(Kw):\n",
        "    \"\"\"\n",
        "    Kw: (T,J,3) con NaNs posibles\n",
        "    Devuelve dict con claves usadas por tu pipeline clásico.\n",
        "    \"\"\"\n",
        "    T,J,D = Kw.shape\n",
        "    xy = Kw[:,:,:2]\n",
        "    # amplitudes globales (rango global X/Y)\n",
        "    amp_x = float(np.nanmax(xy[:,:,0]) - np.nanmin(xy[:,:,0])) if np.isfinite(xy[:,:,0]).any() else 0.0\n",
        "    amp_y = float(np.nanmax(xy[:,:,1]) - np.nanmin(xy[:,:,1])) if np.isfinite(xy[:,:,1]).any() else 0.0\n",
        "    # velocidad media (norma frame-to-frame)\n",
        "    if T > 1:\n",
        "        vel = np.linalg.norm(np.diff(xy, axis=0), axis=2)  # (T-1,J)\n",
        "        vel_media = float(np.nanmean(vel))\n",
        "    else:\n",
        "        vel_media = 0.0\n",
        "    # simetría (distancias medias entre pares L-R)\n",
        "    pairs = [(5,6),(7,8),(9,10),(11,12),(13,14),(15,16)]\n",
        "    dists = []\n",
        "    for a,b in pairs:\n",
        "        if a < J and b < J:\n",
        "            d = np.linalg.norm(xy[:,a,:] - xy[:,b,:], axis=1)\n",
        "            dists.append(np.nanmean(d))\n",
        "    simetria = float(np.nanmean(dists)) if len(dists) else np.nan\n",
        "    # centro de masa (cadera aprox: 11,12) y rango de niveles\n",
        "    if J > 12:\n",
        "        com = (xy[:,11,:] + xy[:,12,:]) / 2.0\n",
        "    else:\n",
        "        com = np.nanmean(xy, axis=1)\n",
        "    if np.isfinite(com[:,1]).any():\n",
        "        p10 = float(np.nanpercentile(com[:,1], 10))\n",
        "        p90 = float(np.nanpercentile(com[:,1], 90))\n",
        "        nivel_rango = float(p10 - p90)\n",
        "    else:\n",
        "        nivel_rango = 0.0\n",
        "    # variedad direcciones (cambios de ángulo de la trayectoria del COM)\n",
        "    if T > 2:\n",
        "        disp = np.diff(com, axis=0)\n",
        "        dirs = np.arctan2(disp[:,1], disp[:,0] + 1e-9)\n",
        "        cambios = np.abs(np.diff(dirs))\n",
        "        variedad_dir = float(np.nanmean(cambios)) if len(cambios)>0 else 0.0\n",
        "    else:\n",
        "        variedad_dir = 0.0\n",
        "\n",
        "    return {\n",
        "        \"amplitud_x\": amp_x,\n",
        "        \"amplitud_y\": amp_y,\n",
        "        \"velocidad_media\": vel_media,\n",
        "        \"simetria\": simetria,\n",
        "        \"nivel_rango\": nivel_rango,\n",
        "        \"variedad_direcciones\": variedad_dir,\n",
        "        \"frames\": int(T),\n",
        "        \"joints\": int(J),\n",
        "        \"dims\": int(D)\n",
        "    }\n",
        "\n",
        "# --- Inferencia con artefactos guardados (pipeline o piezas sueltas) ---\n",
        "def inferir_desde_features_dict(feats: dict):\n",
        "    \"\"\"\n",
        "    Carga pipeline_ovr_logreg.joblib si existe; si no, usa imp+scaler+clf.\n",
        "    Devuelve (labels_activas, sugerencias_texto)\n",
        "    \"\"\"\n",
        "    # Cargar metadatos\n",
        "    feature_cols_path = os.path.join(ART, \"feature_cols.csv\")\n",
        "    label_names_path  = os.path.join(ART, \"label_names.csv\")\n",
        "    if os.path.exists(feature_cols_path):\n",
        "        feat_cols = pd.read_csv(feature_cols_path, header=None)[0].tolist()\n",
        "    else:\n",
        "        feat_cols = list(feats.keys())\n",
        "    if os.path.exists(label_names_path):\n",
        "        labels_all = pd.read_csv(label_names_path, header=None)[0].tolist()\n",
        "    else:\n",
        "        labels_all = []\n",
        "\n",
        "    # Preparar vector en orden correcto\n",
        "    x = np.array([[feats.get(c, np.nan) for c in feat_cols]], dtype=float)\n",
        "\n",
        "    # Intentar pipeline completo\n",
        "    pipe_path = os.path.join(BASE, \"models\", \"pipeline_ovr_logreg.joblib\")\n",
        "    if os.path.exists(pipe_path):\n",
        "        pipe = joblib.load(pipe_path)\n",
        "        yhat = pipe.predict(x)[0]\n",
        "    else:\n",
        "        # Piezas sueltas\n",
        "        imp_path  = os.path.join(ART, \"imputer.joblib\")\n",
        "        scl_path  = os.path.join(ART, \"scaler.joblib\")\n",
        "        clf_path  = os.path.join(ART, \"model_ovr_logreg.joblib\")\n",
        "        assert all(os.path.exists(p) for p in [imp_path, scl_path, clf_path]), \\\n",
        "            \"No se encontraron artefactos de modelo en artifacts/ ni pipeline en models/.\"\n",
        "        imp    = joblib.load(imp_path)\n",
        "        scaler = joblib.load(scl_path)\n",
        "        clf    = joblib.load(clf_path)\n",
        "        x = scaler.transform(imp.transform(x))\n",
        "        yhat = clf.predict(x)[0]\n",
        "\n",
        "    # Mapear a texto (fallback simple si no hay mapa)\n",
        "    label_to_text = {\n",
        "        \"amplitud_baja\":      \"Aumentar amplitud (extensiones y desplazamientos más amplios).\",\n",
        "        \"variedad_baja\":      \"Introducir cambios de dirección y diagonales.\",\n",
        "        \"mucha_simetria\":     \"Explorar asimetrías entre izquierda y derecha.\",\n",
        "        \"poca_simetria\":      \"Equilibrar con momentos de simetría.\",\n",
        "        \"fluidez_baja\":       \"Trabajar transiciones para mayor continuidad/fluidez.\",\n",
        "        \"poco_rango_niveles\": \"Usar niveles alto y bajo además del medio.\",\n",
        "        \"exceso_rango_niveles\": \"Reducir rango vertical para mayor cohesión.\",\n",
        "        \"frase_corta\":        \"Frase corta: considerar repetición o desarrollo del material.\"\n",
        "    }\n",
        "    active_labels = []\n",
        "    suggs = []\n",
        "    names = labels_all if labels_all else [f\"label_{i}\" for i in range(len(yhat))]\n",
        "    for i, v in enumerate(yhat):\n",
        "        if v == 1:\n",
        "            lbl = names[i]\n",
        "            active_labels.append(lbl)\n",
        "            suggs.append(label_to_text.get(lbl, f\"Mejorar {lbl}\"))\n",
        "\n",
        "    return active_labels, suggs\n",
        "\n",
        "# --- Pipeline de vídeo con YOLO (usa tu función existente de keypoints) ---\n",
        "def run_inference_over_video_yolo(video_path, model_name=\"yolov8n-pose.pt\",\n",
        "                                  conf=0.25, stride=1, win_sec=5.0, hop_sec=2.5):\n",
        "    # Necesita que video_to_keypoints_yolo ya esté definida en una celda anterior\n",
        "    assert 'video_to_keypoints_yolo' in globals(), \"Define primero video_to_keypoints_yolo(...)\"\n",
        "    K, FPS = video_to_keypoints_yolo(video_path, model_name=model_name, conf=conf, stride=stride)\n",
        "\n",
        "    if K is None or len(K) == 0:\n",
        "        print(\" Sin detecciones válidas (K es None o vacío). Revisa conf/iluminación/escala.\")\n",
        "        return None\n",
        "\n",
        "    Kc, used = clean_nan_interpolate(K, min_valid_ratio=0.10)\n",
        "    if not used:\n",
        "        print(\" No hay articulaciones válidas tras limpieza. Prueba bajar 'conf' o un modelo más grande (yolov8s-pose).\")\n",
        "        return None\n",
        "\n",
        "    win = max(1, int(round(win_sec * FPS)))\n",
        "    hop = max(1, int(round(hop_sec * FPS)))\n",
        "    T = len(Kc)\n",
        "\n",
        "    rows = []; timeline=[]\n",
        "    for start in range(0, T-1, hop):\n",
        "        end = min(T-1, start + win)\n",
        "        if end - start < max(8, int(0.25*win)): break\n",
        "        Kw = Kc[start:end]\n",
        "        feats = features_coreograficos(Kw)\n",
        "        labels, suggs = inferir_desde_features_dict(feats)\n",
        "        timeline.append({\"start_f\":start, \"end_f\":end, \"sugerencias\":suggs, \"labels\":labels})\n",
        "        row = {\"start_f\":start, \"end_f\":end, \"start_s\":start/FPS, \"end_s\":end/FPS}\n",
        "        row.update(feats); row[\"labels\"]=\"|\".join(labels); row[\"sugerencias\"]=\"|\".join(suggs)\n",
        "        rows.append(row)\n",
        "\n",
        "    out_csv = os.path.join(OUT, \"timeline_sugerencias_yolo.csv\")\n",
        "    pd.DataFrame(rows).to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    out_mp4 = os.path.join(OUT, \"video_anotado_yolo.mp4\")\n",
        "    overlay_suggestions_on_video(video_path, Kc, timeline, out_mp4)\n",
        "\n",
        "    print(\"Listo\")\n",
        "    print(\"CSV:\", out_csv)\n",
        "    print(\"Vídeo:\", out_mp4)\n",
        "    return {\"csv\": out_csv, \"video\": out_mp4, \"timeline\": timeline}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuJhgbDnP1kT"
      },
      "source": [
        "**ejecutamos en video de muestra**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgT2snARK105"
      },
      "outputs": [],
      "source": [
        "VIDEO = \"/content/drive/MyDrive/asistente_coreografico/demo/solo.mp4\"\n",
        "res = run_inference_over_video_yolo(\n",
        "    video_path=VIDEO,\n",
        "    model_name=\"/content/drive/MyDrive/asistente_coreografico/models/yolov8n-pose.pt\",\n",
        "    conf=0.25,\n",
        "    stride=1,\n",
        "    win_sec=5.0,\n",
        "    hop_sec=2.5\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic7ZpKV13Pei"
      },
      "outputs": [],
      "source": [
        "def preview_primeras_anotaciones(res, n=10):\n",
        "    assert res is not None, \"res es None: ¿se ejecutó run_inference_over_video_yolo?\"\n",
        "    filas = []\n",
        "\n",
        "    # Intentar desde el timeline en memoria (si existe)\n",
        "    if \"timeline\" in res and isinstance(res[\"timeline\"], list) and len(res[\"timeline\"]) > 0:\n",
        "        tl = res[\"timeline\"][:n]\n",
        "        for i, seg in enumerate(tl):\n",
        "            start_f = seg.get(\"start_f\")\n",
        "            end_f   = seg.get(\"end_f\")\n",
        "            # Si además tienes start_s/end_s en CSV, lo mostraremos abajo.\n",
        "            suger = seg.get(\"sugerencias\") or seg.get(\"sugerencias_single\") or seg.get(\"sugerencias_global\") or []\n",
        "            labels = seg.get(\"labels\") or []\n",
        "            filas.append({\n",
        "                \"i\": i,\n",
        "                \"start_f\": start_f,\n",
        "                \"end_f\": end_f,\n",
        "                \"labels\": \" · \".join(labels) if isinstance(labels, list) else str(labels),\n",
        "                \"sugerencias\": \" · \".join(suger) if isinstance(suger, list) else str(suger)\n",
        "            })\n",
        "\n",
        "    # Completar con el CSV para tener tiempos en segundos y features\n",
        "    if \"csv\" in res and res[\"csv\"] and os.path.exists(res[\"csv\"]):\n",
        "        df = pd.read_csv(res[\"csv\"])\n",
        "        # Columnas interesantes si existen\n",
        "        cols_base = [c for c in [\"start_s\",\"end_s\",\"labels\",\"sugerencias\",\n",
        "                                 \"amp_x\",\"amp_y\",\"vel_media\",\"simetria_raw\",\"nivel_rango\",\"variedad_dir\"]\n",
        "                     if c in df.columns]\n",
        "        df_preview = df[cols_base].head(n).copy()\n",
        "\n",
        "        # Limpieza visual de listas en texto\n",
        "        for c in [\"labels\",\"sugerencias\"]:\n",
        "            if c in df_preview.columns:\n",
        "                df_preview[c] = df_preview[c].fillna(\"\").astype(str).apply(\n",
        "                    lambda s: \" · \".join([x.strip() for x in s.split(\"|\") if x.strip()])[:300]\n",
        "                )\n",
        "\n",
        "        print(\"=== Primeras ventanas (desde CSV) ===\")\n",
        "        display(df_preview)\n",
        "\n",
        "        # Además, un resumen legible en texto:\n",
        "        print(\"\\nResumen (texto):\")\n",
        "        for i, row in df_preview.iterrows():\n",
        "            st = row.get(\"start_s\", 0.0); et = row.get(\"end_s\", 0.0)\n",
        "            print(f\"[{i}] {st:6.2f}s → {et:6.2f}s\")\n",
        "            if \"sugerencias\" in df_preview.columns and str(row.get(\"sugerencias\",\"\")).strip():\n",
        "                for s in str(row[\"sugerencias\"]).split(\" · \"):\n",
        "                    if s.strip():\n",
        "                        print(\"   -\", s.strip())\n",
        "            elif \"labels\" in df_preview.columns and str(row.get(\"labels\",\"\")).strip():\n",
        "                for lab in str(row[\"labels\"]).split(\" · \"):\n",
        "                    if lab.strip():\n",
        "                        print(\"   -\", lab.strip())\n",
        "            print()\n",
        "\n",
        "    elif filas:\n",
        "        # Si no hay CSV, al menos muestra lo que haya en memoria\n",
        "        df_mem = pd.DataFrame(filas)\n",
        "        print(\"=== Primeras ventanas (desde res['timeline']) ===\")\n",
        "        display(df_mem)\n",
        "        print(\"\\nResumen (texto):\")\n",
        "        for _, r in df_mem.iterrows():\n",
        "            print(f\"[{int(r['i'])}] f{int(r['start_f'])} → f{int(r['end_f'])}\")\n",
        "            if str(r.get(\"sugerencias\",\"\")).strip():\n",
        "                for s in str(r[\"sugerencias\"]).split(\" · \"):\n",
        "                    if s.strip():\n",
        "                        print(\"   -\", s.strip())\n",
        "            elif str(r.get(\"labels\",\"\")).strip():\n",
        "                for lab in str(r[\"labels\"]).split(\" · \"):\n",
        "                    if lab.strip():\n",
        "                        print(\"   -\", lab.strip())\n",
        "            print()\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No encontré timeline en memoria ni CSV en res['csv'].\")\n",
        "\n",
        "# --- Usa la función con tu 'res' ---\n",
        "preview_primeras_anotaciones(res, n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qse5clex0i9"
      },
      "source": [
        "# **versión multipersona con decisión automática (unipersonal vs multipersona)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYod-Z09x1KB"
      },
      "outputs": [],
      "source": [
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "ART  = f\"{BASE}/artifacts\"\n",
        "os.makedirs(ART, exist_ok=True)\n",
        "\n",
        "# Carga artefactos (entrenados previamente con tu CSV)\n",
        "imp          = joblib.load(f\"{ART}/imputer.joblib\")\n",
        "scaler       = joblib.load(f\"{ART}/scaler.joblib\")\n",
        "model        = joblib.load(f\"{ART}/model_ovr_logreg.joblib\")\n",
        "feature_cols = pd.read_csv(f\"{ART}/feature_cols.csv\", header=None)[0].tolist()\n",
        "label_names  = pd.read_csv(f\"{ART}/label_names.csv\", header=None)[0].tolist()\n",
        "\n",
        "label_to_text = {\n",
        "    \"amplitud_baja\":      \"Aumentar amplitud (extensiones y desplazamientos más amplios).\",\n",
        "    \"variedad_baja\":      \"Introducir cambios de dirección y diagonales.\",\n",
        "    \"mucha_simetria\":     \"Explorar asimetrías entre izquierda y derecha.\",\n",
        "    \"poca_simetria\":      \"Equilibrar con momentos de simetría.\",\n",
        "    \"fluidez_baja\":       \"Trabajar transiciones para mayor continuidad/fluidez.\",\n",
        "    \"poco_rango_niveles\": \"Usar niveles alto y bajo además del medio.\",\n",
        "}\n",
        "\n",
        "# --- pares izquierda/derecha COCO-17 ---\n",
        "COCO_EDGES = [(5,7),(7,9),(6,8),(8,10),(11,13),(13,15),(12,14),(14,16),(5,6),(11,12),(5,11),(6,12)]\n",
        "COCO_PAIRS = [(5,6),(7,8),(9,10),(11,12),(13,14),(15,16)]\n",
        "\n",
        "def center_of_mass(K):\n",
        "    # COCO: caderas 11 y 12 si existen\n",
        "    T,J,D = K.shape\n",
        "    if J >= 13:\n",
        "        return (K[:,11,:] + K[:,12,:]) / 2\n",
        "    return K.mean(axis=1)\n",
        "\n",
        "def features_coreograficos(K):\n",
        "    \"\"\"K: (T,J,D) con NaNs posibles, devuelve dict de features.\"\"\"\n",
        "    T,J,D = K.shape\n",
        "    amp_mean = (np.nanmax(K,axis=0) - np.nanmin(K,axis=0)).mean(axis=0)\n",
        "    amp_x = float(amp_mean[0]); amp_y = float(amp_mean[1])\n",
        "    amp_z = float(amp_mean[2]) if D==3 else np.nan\n",
        "    vel = np.linalg.norm(np.diff(K,axis=0),axis=2)\n",
        "    vel_media = float(np.nanmean(vel)) if vel.size else 0.0\n",
        "    pair_vals=[]\n",
        "    for a,b in COCO_PAIRS:\n",
        "        if a<J and b<J:\n",
        "            diff = K[:,a,:]-K[:,b,:]\n",
        "            # Check number of dimensions before calculating norm\n",
        "            if diff.ndim > 1:\n",
        "                d = np.linalg.norm(diff,axis=1).mean() # Correct axis for (T, D) -> (T,)\n",
        "                pair_vals.append(np.nanmean(d))\n",
        "            elif diff.ndim == 1: # Handle case where diff is 1D (T,)\n",
        "                 d = np.linalg.norm(diff).mean()\n",
        "                 pair_vals.append(np.nanmean(d))\n",
        "    sim_raw = float(np.nanmean(pair_vals)) if pair_vals else np.nan\n",
        "    com = center_of_mass(K)\n",
        "    nivel_p10 = float(np.nanpercentile(com[:,1],10))\n",
        "    nivel_p90 = float(np.nanpercentile(com[:,1],90))\n",
        "    nivel_rango = float(nivel_p10 - nivel_p90)\n",
        "    disp = np.diff(com,axis=0)\n",
        "    dirs = np.arctan2(disp[:,1], disp[:,0] + 1e-9)\n",
        "    cambios = np.abs(np.diff(dirs))\n",
        "    variedad_dir = float(np.nanmean(cambios)) if len(cambios)>0 else 0.0\n",
        "    return {\n",
        "        \"amp_x\": amp_x, \"amp_y\": amp_y, \"amp_z\": amp_z,\n",
        "        \"vel_media\": vel_media,\n",
        "        \"simetria_raw\": sim_raw,\n",
        "        \"nivel_rango\": nivel_rango,\n",
        "        \"variedad_dir\": variedad_dir,\n",
        "        \"frames\": int(T), \"joints\": int(J), \"dims\": int(D)\n",
        "    }\n",
        "\n",
        "def inferir_desde_features_dict(feats: dict):\n",
        "    x = np.array([[feats.get(c, np.nan) for c in feature_cols]], dtype=float)\n",
        "    x = scaler.transform(imp.transform(x))\n",
        "    yhat = model.predict(x)[0]\n",
        "    pred_labels = [label_names[i] for i,v in enumerate(yhat) if v==1]\n",
        "    sugerencias = [label_to_text.get(lbl, lbl) for lbl in pred_labels]\n",
        "    return pred_labels, sugerencias\n",
        "\n",
        "def interpolate_nan_1d(y):\n",
        "    y = y.astype(np.float32)\n",
        "    T = len(y); idx = np.arange(T)\n",
        "    mask = np.isfinite(y)\n",
        "    if mask.sum() == 0: return y\n",
        "    # forward/backward\n",
        "    last = np.nan\n",
        "    for i in range(T):\n",
        "        if not np.isfinite(y[i]) and np.isfinite(last): y[i] = last\n",
        "        else: last = y[i]\n",
        "    last = np.nan\n",
        "    for i in range(T-1,-1,-1):\n",
        "        if not np.isfinite(y[i]) and np.isfinite(last): y[i] = last\n",
        "        else: last = y[i]\n",
        "    mask = np.isfinite(y)\n",
        "    if 2 <= mask.sum() < T:\n",
        "        y[~mask] = np.interp(idx[~mask], idx[mask], y[mask])\n",
        "    return y\n",
        "\n",
        "def clean_nan_interpolate(K, min_valid_ratio=0.10):\n",
        "    Kc = K.copy()\n",
        "    T,J,D = Kc.shape\n",
        "    used = []\n",
        "    for j in range(J):\n",
        "        valid = np.isfinite(Kc[:,j,:]).all(axis=1)\n",
        "        if valid.mean() < min_valid_ratio:\n",
        "            Kc[:,j,:] = np.nan\n",
        "            continue\n",
        "        for d in range(D):\n",
        "            Kc[:,j,d] = interpolate_nan_1d(Kc[:,j,d])\n",
        "        used.append(j)\n",
        "    if not used: return Kc, []\n",
        "    return Kc[:,used,:], used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfZLoDGZyIhh"
      },
      "outputs": [],
      "source": [
        "#Deteccion multipersona\n",
        "\n",
        "import cv2, numpy as np\n",
        "from ultralytics import YOLO\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def _centroid_from_kps(kps):\n",
        "    # kps: (J,2) con NaNs; devuelve centro válido\n",
        "    mask = np.isfinite(kps).all(axis=1)\n",
        "    if mask.any():\n",
        "        return np.nanmean(kps[mask], axis=0)\n",
        "    return np.array([np.nan, np.nan], dtype=np.float32)\n",
        "\n",
        "def assign_tracks(prev_centroids, det_centroids, max_dist=100.0):\n",
        "    \"\"\"\n",
        "    Asigna detecciones a tracks previos vía Hungarian.\n",
        "    Devuelve pares (prev_idx -> det_idx), y listas de no asignados.\n",
        "    \"\"\"\n",
        "    if len(prev_centroids)==0 or len(det_centroids)==0:\n",
        "        return [], list(range(len(prev_centroids))), list(range(len(det_centroids)))\n",
        "    C = np.zeros((len(prev_centroids), len(det_centroids)), dtype=np.float32)\n",
        "    for i, pc in enumerate(prev_centroids):\n",
        "        for j, dc in enumerate(det_centroids):\n",
        "            if np.any(np.isnan(pc)) or np.any(np.isnan(dc)):\n",
        "                C[i,j] = 1e6\n",
        "            else:\n",
        "                C[i,j] = np.linalg.norm(pc - dc)\n",
        "    rows, cols = linear_sum_assignment(C)\n",
        "    matches, prev_un, det_un = [], [], []\n",
        "    used_prev, used_det = set(), set()\n",
        "    for r,c in zip(rows, cols):\n",
        "        if C[r,c] <= max_dist:\n",
        "            matches.append((r,c))\n",
        "            used_prev.add(r); used_det.add(c)\n",
        "    for i in range(len(prev_centroids)):\n",
        "        if i not in used_prev: prev_un.append(i)\n",
        "    for j in range(len(det_centroids)):\n",
        "        if j not in used_det: det_un.append(j)\n",
        "    return matches, prev_un, det_un\n",
        "\n",
        "def video_to_tracks_yolo(\n",
        "    video_path, model_name=\"yolov8n-pose.pt\", conf=0.25, stride=1,\n",
        "    max_missed=30, max_tracks=12, assign_max_dist=120.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Devuelve:\n",
        "      tracks_K: dict track_id -> (T,J,2) con NaNs\n",
        "      FPS, W, H\n",
        "    \"\"\"\n",
        "    model = YOLO(model_name)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    assert cap.isOpened(), f\"No se pudo abrir el vídeo: {video_path}\"\n",
        "    T = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    FPS = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    J = 17\n",
        "\n",
        "    # Estructuras de tracking\n",
        "    tracks = {}           # track_id -> dict(last_centroid, last_frame, missed, kps_per_frame)\n",
        "    next_id = 0\n",
        "\n",
        "    t = 0\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        if t % stride != 0:\n",
        "            t += 1\n",
        "            continue\n",
        "\n",
        "        res = model.predict(frame, conf=conf, verbose=False)[0]\n",
        "        det_kps_list, det_centroids = [], []\n",
        "        if res.keypoints is not None and len(res.keypoints) > 0:\n",
        "            for i in range(len(res.keypoints)):\n",
        "                kps = res.keypoints.xy[i].cpu().numpy()  # (17,2)\n",
        "                kps[:,0] = np.clip(kps[:,0], 0, W-1)\n",
        "                kps[:,1] = np.clip(kps[:,1], 0, H-1)\n",
        "                det_kps_list.append(kps.astype(np.float32))\n",
        "                det_centroids.append(_centroid_from_kps(kps))\n",
        "\n",
        "        # Preparar centroids previos (de tracks activos)\n",
        "        active_ids = [tid for tid, tr in tracks.items() if tr[\"missed\"] < max_missed]\n",
        "        prev_centroids = [tracks[tid][\"last_centroid\"] for tid in active_ids]\n",
        "\n",
        "        # Asignación\n",
        "        matches, prev_un, det_un = assign_tracks(prev_centroids, det_centroids, max_dist=assign_max_dist)\n",
        "\n",
        "        # Actualizar tracks con matches\n",
        "        for (pi, di) in matches:\n",
        "            tid = active_ids[pi]\n",
        "            kps = det_kps_list[di]\n",
        "            c = det_centroids[di]\n",
        "            tracks[tid][\"kps\"].append((t, kps))\n",
        "            tracks[tid][\"last_centroid\"] = c\n",
        "            tracks[tid][\"last_frame\"] = t\n",
        "            tracks[tid][\"missed\"] = 0\n",
        "\n",
        "        # Los prev no asignados: incrementar missed\n",
        "        for pi in prev_un:\n",
        "            tid = active_ids[pi]\n",
        "            tracks[tid][\"missed\"] += 1\n",
        "\n",
        "        # Nuevas detecciones -> nuevos tracks\n",
        "        for di in det_un:\n",
        "            if len(tracks) >= max_tracks:\n",
        "                continue\n",
        "            kps = det_kps_list[di]\n",
        "            c = det_centroids[di]\n",
        "            tracks[next_id] = {\n",
        "                \"kps\": [(t, kps)],\n",
        "                \"last_centroid\": c,\n",
        "                \"last_frame\": t,\n",
        "                \"missed\": 0\n",
        "            }\n",
        "            next_id += 1\n",
        "\n",
        "        t += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Convertir a tensores (T,J,2) por track\n",
        "    tracks_K = {}\n",
        "    for tid, tr in tracks.items():\n",
        "        K = np.full((T, J, 2), np.nan, dtype=np.float32)\n",
        "        for (f, kps) in tr[\"kps\"]:\n",
        "            if 0 <= f < T:\n",
        "                K[f] = kps\n",
        "        tracks_K[tid] = K\n",
        "\n",
        "    return tracks_K, FPS, W, H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAkv9gbvyPOv"
      },
      "outputs": [],
      "source": [
        "#Métricas grupales y decisión automática\n",
        "\n",
        "def nancorr(a, b):\n",
        "    \"\"\"Correlación de Pearson nan-robusta.\"\"\"\n",
        "    mask = np.isfinite(a) & np.isfinite(b)\n",
        "    if mask.sum() < 3: return np.nan\n",
        "    aa = a[mask] - np.nanmean(a[mask])\n",
        "    bb = b[mask] - np.nanmean(b[mask])\n",
        "    denom = (np.sqrt(np.nanmean(aa**2))*np.sqrt(np.nanmean(bb**2))) + 1e-9\n",
        "    return float(np.nanmean(aa*bb)/denom)\n",
        "\n",
        "def group_metrics(COMs):\n",
        "    \"\"\"\n",
        "    COMs: lista de arrays (T,2) por persona en la ventana (NaNs posibles)\n",
        "    Devuelve: sincronía, dispersión, unísono, área de formación\n",
        "    \"\"\"\n",
        "    n = len(COMs)\n",
        "    if n < 2:\n",
        "        return {\"sync\": np.nan, \"spread\": np.nan, \"unison\": np.nan, \"formation_area\": np.nan}\n",
        "\n",
        "    # Velocidades\n",
        "    vels = [np.linalg.norm(np.diff(c,axis=0),axis=1) for c in COMs]\n",
        "    # Direcciones (unitarios)\n",
        "    dirs = []\n",
        "    for c in COMs:\n",
        "        d = np.diff(c,axis=0)\n",
        "        norm = np.linalg.norm(d,axis=1) + 1e-9\n",
        "        dirs.append(d / norm[:,None])\n",
        "\n",
        "    # Sincronía = corr de velocidades promedio por pares\n",
        "    pair_sync = []\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            pair_sync.append(nancorr(vels[i], vels[j]))\n",
        "    sync = float(np.nanmean(pair_sync)) if pair_sync else np.nan\n",
        "\n",
        "    # Unísono = media de coseno(dir_i, dir_j) > 0.8 (proporción)\n",
        "    pair_unison = []\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            doti = (dirs[i]*dirs[j]).sum(axis=1)  # cosenos\n",
        "            mask = np.isfinite(doti)\n",
        "            if mask.sum() == 0: continue\n",
        "            pair_unison.append(np.nanmean((doti[mask] > 0.8).astype(np.float32)))\n",
        "    unison = float(np.nanmean(pair_unison)) if pair_unison else np.nan\n",
        "\n",
        "    # Dispersión = distancia media por pares de COM (promedio en ventana)\n",
        "    pair_dist = []\n",
        "    for t in range(COMs[0].shape[0]):\n",
        "        pts = []\n",
        "        for c in COMs:\n",
        "            if np.all(np.isfinite(c[t])):\n",
        "                pts.append(c[t])\n",
        "        if len(pts) >= 2:\n",
        "            pts = np.stack(pts)\n",
        "            # media de distancias por pares\n",
        "            D = np.linalg.norm(pts[:,None,:]-pts[None,:,:], axis=2)\n",
        "            pair_dist.append(np.nanmean(D[np.triu_indices(len(pts),1)]))\n",
        "    spread = float(np.nanmean(pair_dist)) if pair_dist else np.nan\n",
        "\n",
        "    # Área de formación = área del convex hull promedio (aprox usando todos los puntos)\n",
        "    all_pts = []\n",
        "    for c in COMs:\n",
        "        mask = np.all(np.isfinite(c), axis=1)\n",
        "        all_pts += [c[m] for m in np.where(mask)[0]]\n",
        "    if len(all_pts) >= 3:\n",
        "        pts = np.vstack(all_pts)\n",
        "        try:\n",
        "            hull = ConvexHull(pts)\n",
        "            area = float(hull.area)  # perímetro 3D en 2D? En 2D mejor usar hull.volume\n",
        "            area = float(hull.volume)  # en 2D, 'volume' es el área\n",
        "        except Exception:\n",
        "            # fallback: bbox\n",
        "            xmin, ymin = np.nanmin(pts, axis=0)\n",
        "            xmax, ymax = np.nanmax(pts, axis=0)\n",
        "            area = float((xmax-xmin)*(ymax-ymin))\n",
        "    else:\n",
        "        area = np.nan\n",
        "\n",
        "    return {\"sync\": sync, \"spread\": spread, \"unison\": unison, \"formation_area\": area}\n",
        "\n",
        "def auto_decide_mode(tracks_K, min_frames_active=0.5, prop_single_threshold=0.8, multi_threshold=2):\n",
        "    \"\"\"\n",
        "    Decide 'single' vs 'multi':\n",
        "      - Cuenta personas activas por frame (≥ Joints válidos/ NaNs bajos).\n",
        "      - Si >80% de frames tienen exactamente 1 → 'single', si no → 'multi'.\n",
        "    \"\"\"\n",
        "    if not tracks_K:\n",
        "        return \"single\", None\n",
        "    T = next(iter(tracks_K.values())).shape[0]\n",
        "    active_counts = np.zeros(T, dtype=int)\n",
        "    for K in tracks_K.values():\n",
        "        valid = np.isfinite(K).all(axis=2)  # (T,J)\n",
        "        frames_valid = valid.mean(axis=1) >= min_frames_active   # proporción de joints válidos\n",
        "        active_counts += frames_valid.astype(int)\n",
        "\n",
        "    prop_single = (active_counts == 1).mean()\n",
        "    if prop_single >= prop_single_threshold:\n",
        "        # single: elige el track con más frames válidos\n",
        "        best_id, best_frames = None, -1\n",
        "        for tid, K in tracks_K.items():\n",
        "            frames_valid = np.isfinite(K).all(axis=2).mean(axis=1) >= min_frames_active\n",
        "            cnt = int(frames_valid.sum())\n",
        "            if cnt > best_frames:\n",
        "                best_frames = cnt; best_id = tid\n",
        "        return \"single\", best_id\n",
        "    else:\n",
        "        # multi si en una proporción relevante hay >=2 personas\n",
        "        if (active_counts >= multi_threshold).mean() >= 0.2:\n",
        "            return \"multi\", None\n",
        "        # default\n",
        "        return \"single\", max(tracks_K, key=lambda tid: np.isfinite(tracks_K[tid]).all(axis=2).mean(axis=1).sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61E-7VcCyZX-"
      },
      "outputs": [],
      "source": [
        "# Timeline por ventanas + overlay simple o multiple\n",
        "\n",
        "def overlay_video_multi(video_path, tracks_K, timeline, out_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    assert cap.isOpened(), f\"No se pudo abrir el vídeo: {video_path}\"\n",
        "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    FPS = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    vw = cv2.VideoWriter(out_path, fourcc, FPS, (W,H))\n",
        "\n",
        "    # colores fijos por track\n",
        "    palette = [(255,255,255),(0,255,255),(255,0,255),(255,255,0),(0,200,255),(255,200,0),(200,255,0),(200,200,255)]\n",
        "    track_ids = sorted(tracks_K.keys())\n",
        "    color_of = {tid: palette[i%len(palette)] for i,tid in enumerate(track_ids)}\n",
        "\n",
        "    t = 0\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok: break\n",
        "\n",
        "        # dibujar cada track si hay puntos\n",
        "        for tid, K in tracks_K.items():\n",
        "            if t < len(K):\n",
        "                P = K[t]\n",
        "                col = color_of[tid]\n",
        "                for a,b in COCO_EDGES:\n",
        "                    if a < P.shape[0] and b < P.shape[0]:\n",
        "                        xa,ya = P[a]; xb,yb = P[b]\n",
        "                        if np.isfinite(xa) and np.isfinite(ya) and np.isfinite(xb) and np.isfinite(yb):\n",
        "                            cv2.line(frame, (int(xa),int(ya)), (int(xb),int(yb)), col, 2)\n",
        "                for (x,y) in P:\n",
        "                    if np.isfinite(x) and np.isfinite(y):\n",
        "                        cv2.circle(frame, (int(x),int(y)), 2, col, -1)\n",
        "\n",
        "        # texto: sugerencias activas (globales)\n",
        "        active = []\n",
        "        for seg in timeline:\n",
        "            if seg[\"start_f\"] <= t <= seg[\"end_f\"]:\n",
        "                active = seg.get(\"sugerencias_global\", []) or seg.get(\"sugerencias_single\", [])\n",
        "        y0 = 30\n",
        "        for s in active[:4]:\n",
        "            cv2.putText(frame, f\"• {s}\", (20, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 4, cv2.LINE_AA)\n",
        "            cv2.putText(frame, f\"• {s}\", (20, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1, cv2.LINE_AA)\n",
        "            y0 += 28\n",
        "\n",
        "        vw.write(frame); t += 1\n",
        "\n",
        "    cap.release(); vw.release()\n",
        "\n",
        "def run_inference_auto(video_path, model_name=\"yolov8n-pose.pt\",\n",
        "                       conf=0.25, stride=1, win_sec=5.0, hop_sec=2.5):\n",
        "    # 1) detección + tracking\n",
        "    tracks_K, FPS, W, H = video_to_tracks_yolo(video_path, model_name=model_name, conf=conf, stride=stride)\n",
        "\n",
        "    # 2) decisión auto\n",
        "    mode, best_id = auto_decide_mode(tracks_K)\n",
        "    print(f\"Modo decidido: {mode}  | best_id: {best_id}\")\n",
        "\n",
        "    # 3) limpiar/interpolar cada track\n",
        "    tracks_clean = {}\n",
        "    for tid, K in tracks_K.items():\n",
        "        Kc, used = clean_nan_interpolate(K, min_valid_ratio=0.10)\n",
        "        tracks_clean[tid] = Kc\n",
        "\n",
        "    win = max(1, int(round(win_sec * FPS)))\n",
        "    hop = max(1, int(round(hop_sec * FPS)))\n",
        "    T = next(iter(tracks_clean.values())).shape[0]\n",
        "\n",
        "    rows = []\n",
        "    timeline = []\n",
        "\n",
        "    if mode == \"single\":\n",
        "        # elegir K del best_id\n",
        "        K_sel = tracks_clean[best_id]\n",
        "        for start in range(0, T-1, hop):\n",
        "            end = min(T-1, start + win)\n",
        "            if end - start < max(8, int(0.25*win)): break\n",
        "            Kw = K_sel[start:end]\n",
        "            feats = features_coreograficos(Kw)\n",
        "            feats_ml = {c: feats.get(c, np.nan) for c in feature_cols}\n",
        "            labels, suggs = inferir_desde_features_dict(feats_ml)\n",
        "            timeline.append({\"start_f\":start, \"end_f\":end, \"sugerencias_single\":suggs})\n",
        "            row = {\"start_f\":start, \"end_f\":end, \"start_s\":start/FPS, \"end_s\":end/FPS, \"mode\":\"single\", \"track_id\":best_id}\n",
        "            row.update(feats); row[\"labels\"]=\"|\".join(labels); row[\"sugerencias\"]=\"|\".join(suggs)\n",
        "            rows.append(row)\n",
        "\n",
        "        out_dir = os.path.join(BASE, \"data/processed/aistpp\")\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        out_csv = os.path.join(out_dir, \"timeline_auto_single.csv\")\n",
        "        pd.DataFrame(rows).to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
        "        out_mp4 = os.path.join(out_dir, \"video_auto_single.mp4\")\n",
        "        overlay_video_multi(video_path, {best_id: tracks_clean[best_id]}, timeline, out_mp4)\n",
        "        print(\"CSV:\", out_csv); print(\"Video:\", out_mp4)\n",
        "        return {\"mode\": mode, \"csv\": out_csv, \"video\": out_mp4, \"timeline\": timeline}\n",
        "\n",
        "    else:\n",
        "        # MULTI: por ventana, computar features individuales + métricas grupales\n",
        "        track_ids = sorted(tracks_clean.keys())\n",
        "        for start in range(0, T-1, hop):\n",
        "            end = min(T-1, start + win)\n",
        "            if end - start < max(8, int(0.25*win)): break\n",
        "\n",
        "            feats_per_track = {}\n",
        "            suggs_per_track = {}\n",
        "            COMs = []\n",
        "\n",
        "            for tid in track_ids:\n",
        "                Kw = tracks_clean[tid][start:end]\n",
        "                # si la ventana de ese track está totalmente NaN, saltar\n",
        "                if not np.isfinite(Kw).any():\n",
        "                    continue\n",
        "                feats = features_coreograficos(Kw)\n",
        "                feats_per_track[tid] = feats\n",
        "                feats_ml = {c: feats.get(c, np.nan) for c in feature_cols}\n",
        "                labels, suggs = inferir_desde_features_dict(feats_ml)\n",
        "                suggs_per_track[tid] = suggs\n",
        "                COMs.append(center_of_mass(Kw))\n",
        "\n",
        "            # métricas grupales\n",
        "            gm = group_metrics(COMs) if len(COMs) >= 2 else {\"sync\":np.nan,\"spread\":np.nan,\"unison\":np.nan,\"formation_area\":np.nan}\n",
        "\n",
        "            # sugerencias globales (reglas simples sobre métricas grupales)\n",
        "            sglob = []\n",
        "            if np.isfinite(gm[\"sync\"]) and gm[\"sync\"] < 0.2:\n",
        "                sglob.append(\"Sincronía grupal baja: ensayar cues de entrada/salida y respiración común.\")\n",
        "            if np.isfinite(gm[\"unison\"]) and gm[\"unison\"] < 0.5:\n",
        "                sglob.append(\"Poco unísono: trabajar frases en espejo y acentos compartidos.\")\n",
        "            if np.isfinite(gm[\"spread\"]) and gm[\"spread\"] < 60:\n",
        "                sglob.append(\"Formación muy compacta: explorar aperturas y diagonales.\")\n",
        "            if np.isfinite(gm[\"spread\"]) and gm[\"spread\"] > 300:\n",
        "                sglob.append(\"Demasiada dispersión: introducir agrupamientos o centros de atención.\")\n",
        "            # (opcional) área de formación poco variable entre ventanas → sugerir cambios de formación\n",
        "\n",
        "            # fila CSV (agregada)\n",
        "            row = {\"start_f\":start, \"end_f\":end, \"start_s\":start/FPS, \"end_s\":end/FPS, \"mode\":\"multi\"}\n",
        "            row.update({f\"gm_{k}\":v for k,v in gm.items()})\n",
        "            row[\"sugerencias_global\"] = \"|\".join(sglob)\n",
        "            # puedes agregar resumidas por track si quieres\n",
        "            rows.append(row)\n",
        "\n",
        "            timeline.append({\n",
        "                \"start_f\":start, \"end_f\":end,\n",
        "                \"sugerencias_global\": sglob,\n",
        "                \"sugerencias_por_track\": {int(t): suggs_per_track[t] for t in suggs_per_track}\n",
        "            })\n",
        "\n",
        "        out_dir = os.path.join(BASE, \"data/processed/aistpp\")\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        out_csv = os.path.join(out_dir, \"timeline_auto_multi.csv\")\n",
        "        pd.DataFrame(rows).to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
        "        out_mp4 = os.path.join(out_dir, \"video_auto_multi.mp4\")\n",
        "        overlay_video_multi(video_path, tracks_clean, timeline, out_mp4)\n",
        "        print(\"CSV:\", out_csv); print(\"Video:\", out_mp4)\n",
        "        return {\"mode\": \"multi\", \"csv\": out_csv, \"video\": out_mp4, \"timeline\": timeline}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8Jkn6X1ykgZ"
      },
      "outputs": [],
      "source": [
        "#Ejecución de autoswitch\n",
        "\n",
        "VIDEO = \"/content/drive/MyDrive/asistente_coreografico/demo/estío.mp4\"\n",
        "res = run_inference_auto(\n",
        "    video_path=VIDEO,\n",
        "    model_name=\"yolov8s-pose.pt\",\n",
        "    conf=0.25,\n",
        "    stride=4,        # se puede bajar a 1 si se quiere ir más lento\n",
        "    win_sec=5.0,\n",
        "    hop_sec=2.5\n",
        ")\n",
        "res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wat0WfNF-znz"
      },
      "source": [
        "En este punto hemos utilizado un modelo existente y pre-entrenado para realizar un primer proceso de lo que sería el asistente coreografico, este modelo YOLOv8 que a partir de los datos de AIST+++ reconoce tanto a una persona o realiza la seleccion multipersona segun el video que este trabajando y realiza el análisis para dar las sugerencias necesarias y las anota por secuencia de tiempo.\n",
        "A partir de aqui vamos a realizar mejoras al proyecto para lograr la máxima efectividad en vista al desarrollo de una aplicacion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pUHG1IlyfLR"
      },
      "source": [
        "Pipeline para Procesar Videos Custom (Subidos por Usuarios)\n",
        "Objetivo\n",
        "\n",
        "Crear un sistema que permita a usuarios subir sus propios videos de danza para obtener:\n",
        "\n",
        "Extracción de keypoints (pose estimation)\n",
        "Cálculo de métricas coreográficas\n",
        "Generación de sugerencias personalizadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP4hw7woyhjO"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "def upload_video():\n",
        "    \"\"\"Permite al usuario subir un video desde su dispositivo\"\"\"\n",
        "    uploaded = files.upload()\n",
        "    video_name = list(uploaded.keys())[0]\n",
        "    return video_name, uploaded[video_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWi5ZFvPzoiC"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "\n",
        "def process_video_with_mediapipe(video_path, output_dir):\n",
        "    \"\"\"\n",
        "    Procesa video con MediaPipe Pose y guarda keypoints\n",
        "    \"\"\"\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose(\n",
        "        static_image_mode=False,\n",
        "        model_complexity=2,\n",
        "        enable_segmentation=False,\n",
        "        min_detection_confidence=0.5\n",
        "    )\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    all_keypoints = []\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convertir BGR a RGB\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose.process(rgb_frame)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            frame_keypoints = []\n",
        "            for landmark in results.pose_landmarks.landmark:\n",
        "                frame_keypoints.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "            all_keypoints.append(frame_keypoints)\n",
        "        else:\n",
        "            # Si no se detecta pose, añadir array de NaN\n",
        "            all_keypoints.append([np.nan] * 33 * 4)\n",
        "\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Convertir a array numpy y guardar\n",
        "    keypoints_array = np.array(all_keypoints)\n",
        "    output_path = os.path.join(output_dir, \"custom_video_keypoints.npy\")\n",
        "    np.save(output_path, keypoints_array)\n",
        "\n",
        "    return keypoints_array, output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97fe88f4"
      },
      "source": [
        "## Procesamos video con mediapipe\n",
        "\n",
        "###\n",
        "Se procesa un video subido por el usuario para que sea procesado por mediapipe\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====  RESET + PIN DE VERSIONES COMPATIBLES PARA MEDIAPIPE EN COLAB (Py 3.12) =====\n",
        "# realizamos este reset porque mediapipe nos da conflictos en su instalacion y hemos buscado la opción mas sostenible.\n",
        "import sys\n",
        "\n",
        "#  Limpia imports a medio cargar (evita estados inconsistentes en esta sesión)\n",
        "for m in list(sys.modules):\n",
        "    if m.startswith((\"mediapipe\", \"cv2\", \"google.protobuf\")):\n",
        "        sys.modules.pop(m, None)\n",
        "\n",
        "# Desinstala paquetes que chocan con los pines que vamos a poner\n",
        "!pip -q uninstall -y mediapipe opencv-python opencv-contrib-python opencv-python-headless protobuf ydf tensorflow-decision-forests grpcio-status || true\n",
        "\n",
        "# Instala versiones compatibles\n",
        "#    - NumPy 2.1.x satisface OpenCV ≥4.12 y TF 2.19 (que pide <2.2)\n",
        "#    - pandas 2.2.2 es la que pide google-colab\n",
        "#    - protobuf 4.25.3 es compatible con TF 2.19 y con MediaPipe 0.10.21\n",
        "#    - opencv-python-headless 4.12.0.88 requiere NumPy ≥2 y <2.3 (ok con 2.1.x)\n",
        "#    - mediapipe 0.10.21 funciona en Py 3.12 y pide protobuf <5 (4.25.3 cumple)\n",
        "!pip -q install --upgrade pip setuptools wheel jedi\n",
        "!pip -q install \"numpy==2.1.3\" \"pandas==2.2.2\"\n",
        "!pip -q install \"protobuf==4.25.3\"\n",
        "!pip -q install \"opencv-python-headless==4.12.0.88\"\n",
        "!pip -q install \"mediapipe==0.10.21\"\n",
        "\n",
        "import platform, importlib, pkg_resources, numpy, pandas\n",
        "print(\"Python :\", sys.version)\n",
        "print(\"NumPy  :\", numpy.__version__)\n",
        "print(\"Pandas :\", pandas.__version__)\n",
        "print(\"Proto  :\", pkg_resources.get_distribution('protobuf').version)\n",
        "print(\"MP spec:\", importlib.util.find_spec('mediapipe') is not None)\n",
        "\n",
        "print(\"\\n➡️  IMPORTANTE: Reinicia ahora el runtime (Entorno de ejecución → Reiniciar ejecución).\")\n"
      ],
      "metadata": {
        "id": "AmZHLRJRVmvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== SUBIR VÍDEO + EXTRAER KEYPOINTS (MediaPipe Pose) =====\n",
        "import os, tempfile\n",
        "import numpy as np\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from google.colab import files, drive\n",
        "\n",
        "# Montar Drive si no está montado\n",
        "if not os.path.isdir(\"/content/drive/MyDrive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "OUT_DIR = os.path.join(BASE, \"data/processed/custom_videos\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def upload_video():\n",
        "    up = files.upload()\n",
        "    if not up:\n",
        "        raise RuntimeError(\"No se subió ningún archivo.\")\n",
        "    name = list(up.keys())[0]\n",
        "    return name, up[name]\n",
        "\n",
        "def process_video_with_mediapipe(video_path, output_dir):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"No se pudo abrir el vídeo: {video_path}\")\n",
        "\n",
        "    all_keypoints, frame_count = [], 0\n",
        "    # Pose con FP32; segmentación desactivada para ir más ligero\n",
        "    with mp.solutions.pose.Pose(\n",
        "        static_image_mode=False,\n",
        "        model_complexity=2,\n",
        "        enable_segmentation=False,\n",
        "        min_detection_confidence=0.5\n",
        "    ) as pose:\n",
        "        while True:\n",
        "            ok, frame = cap.read()\n",
        "            if not ok:\n",
        "                break\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            res = pose.process(rgb)\n",
        "\n",
        "            if res.pose_landmarks:\n",
        "                # 33 landmarks × [x, y, z, visibility]\n",
        "                flat = []\n",
        "                for lm in res.pose_landmarks.landmark:\n",
        "                    flat.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
        "                all_keypoints.append(flat)\n",
        "            else:\n",
        "                all_keypoints.append([np.nan] * (33 * 4))\n",
        "\n",
        "            frame_count += 1\n",
        "            if frame_count % 30 == 0:\n",
        "                print(f\"Procesados {frame_count} frames...\")\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    arr = np.asarray(all_keypoints, dtype=np.float32)  # (T, 132)\n",
        "    out_name = os.path.splitext(os.path.basename(video_path))[0] + \"_keypoints.npy\"\n",
        "    out_path = os.path.join(output_dir, out_name)\n",
        "    np.save(out_path, arr)\n",
        "    return arr, out_path\n",
        "\n",
        "# Flujo: subir → temporal → procesar → guardar\n",
        "video_tmp = None\n",
        "try:\n",
        "    name, data = upload_video()\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(name)[1]) as tmp:\n",
        "        tmp.write(data)\n",
        "        video_tmp = tmp.name\n",
        "    print(f\"▶ Vídeo '{name}' guardado temporalmente en: {video_tmp}\")\n",
        "\n",
        "    arr, saved = process_video_with_mediapipe(video_tmp, OUT_DIR)\n",
        "    print(f\"\\n✅ Keypoints extraídos: shape={arr.shape}  (frames x 132)\")\n",
        "    print(f\"📁 Guardado en: {saved}\")\n",
        "\n",
        "finally:\n",
        "    try:\n",
        "        if video_tmp and os.path.exists(video_tmp):\n",
        "            os.remove(video_tmp)\n",
        "            print(f\"🧹 Temporal eliminado: {video_tmp}\")\n",
        "    except Exception:\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "QbEciIDBVvWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4127728"
      },
      "source": [
        "## Analisis y visualizacion de los resultados\n",
        "\n",
        "Calcula las características coreográficas a partir de los puntos clave extraídos, genera sugerencias basadas en estas características utilizando el modelo entrenado y crea un video anotado con las sugerencias superpuestas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSI7N_hr2CRl"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import os\n",
        "import tempfile\n",
        "import mediapipe as mp  # Mantener el import: se usa en process_video_with_mediapipe si hace falta\n",
        "\n",
        "# Cargar artefactos (entrenados previamente con tu CSV)\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "ART  = f\"{BASE}/artifacts\"\n",
        "imp          = joblib.load(f\"{ART}/imputer.joblib\")\n",
        "scaler       = joblib.load(f\"{ART}/scaler.joblib\")\n",
        "model        = joblib.load(f\"{ART}/model_ovr_logreg.joblib\")\n",
        "feature_cols = pd.read_csv(f\"{ART}/feature_cols.csv\", header=None)[0].tolist()\n",
        "label_names  = pd.read_csv(f\"{ART}/label_names.csv\", header=None)[0].tolist()\n",
        "\n",
        "label_to_text = {\n",
        "    \"amplitud_baja\":      \"Aumentar amplitud (extensiones y desplazamientos más amplios).\",\n",
        "    \"variedad_baja\":      \"Introducir cambios de dirección y diagonales.\",\n",
        "    \"mucha_simetria\":     \"Explorar asimetrías entre izquierda y derecha.\",\n",
        "    \"poca_simetria\":      \"Equilibrar con momentos de simetría.\",\n",
        "    \"fluidez_baja\":       \"Trabajar transiciones para mayor continuidad/fluidez.\",\n",
        "    \"poco_rango_niveles\": \"Usar niveles alto y bajo además del medio.\",\n",
        "    \"exceso_rango_niveles\": \"Reducir rango vertical para mayor cohesión.\",\n",
        "    \"frase_corta\": \"Frase corta: considerar repetición o desarrollo del material.\"\n",
        "    # Añade aquí el resto de etiquetas de label_names y su sugerencia correspondiente\n",
        "}\n",
        "\n",
        "# --- pares izquierda/derecha COCO-17 ---\n",
        "# Nota: MediaPipe proporciona 33 puntos, pero el modelo fue entrenado con COCO-17.\n",
        "# Se necesita mapear los puntos de MediaPipe a COCO-17 o reentrenar el modelo.\n",
        "# Para simplificar en este ejemplo, asumimos un mapeo o un subconjunto\n",
        "# de puntos MediaPipe que se alinea con COCO-17 para el cálculo de features.\n",
        "# Este mapeo es simplificado a efectos demostrativos.\n",
        "# Un mapeo correcto requeriría revisar los índices de MediaPipe y COCO.\n",
        "# Suponemos que los primeros 17 puntos de MediaPipe se alinean con COCO-17 para estas features.\n",
        "MEDIAPIPE_TO_COCO_MAP = {\n",
        "    0: 0,  # Nariz\n",
        "    1: 1,  # Ojo izquierdo (interno)\n",
        "    2: 2,  # Ojo izquierdo\n",
        "    3: 3,  # Ojo izquierdo (externo)\n",
        "    4: 4,  # Ojo derecho (interno)\n",
        "    5: 5,  # Ojo derecho\n",
        "    6: 6,  # Ojo derecho (externo)\n",
        "    7: 7,  # Oreja izquierda\n",
        "    8: 8,  # Oreja derecha\n",
        "    9: 9,  # Boca izquierda\n",
        "    10: 10,  # Boca derecha\n",
        "    11: 11,  # Hombro izquierdo\n",
        "    12: 12,  # Hombro derecho\n",
        "    13: 13,  # Codo izquierdo\n",
        "    14: 14,  # Codo derecho\n",
        "    15: 15,  # Muñeca izquierda\n",
        "    16: 16,  # Muñeca derecha\n",
        "    17: None,  # Meñique izquierdo\n",
        "    18: None,  # Meñique derecho\n",
        "    19: None,  # Índice izquierdo\n",
        "    20: None,  # Índice derecho\n",
        "    21: None,  # Pulgar izquierdo\n",
        "    22: None,  # Pulgar derecho\n",
        "    23: None,  # Cadera izquierda\n",
        "    24: None,  # Cadera derecha\n",
        "    25: None,  # Rodilla izquierda\n",
        "    26: None,  # Rodilla derecha\n",
        "    27: None,  # Tobillo izquierdo\n",
        "    28: None,  # Tobillo derecho\n",
        "    29: None,  # Talón izquierdo\n",
        "    30: None,  # Talón derecho\n",
        "    31: None,  # Punta pie izquierdo\n",
        "    32: None,  # Punta pie derecho\n",
        "}\n",
        "\n",
        "# Pares COCO simplificados basados en los primeros 17 puntos de MediaPipe\n",
        "MEDIAPIPE_COCO_PAIRS = [(11,12), (13,14), (15,16)]  # Hombros, codos, muñecas\n",
        "\n",
        "def center_of_mass_mediapipe(K):\n",
        "    # En MediaPipe las caderas son 23 y 24\n",
        "    T,J,D = K.shape\n",
        "    if J >= 25:  # Asumiendo que se usan puntos de MediaPipe\n",
        "        return (K[:,23,:2] + K[:,24,:2]) / 2  # Solo x, y para el centro de masas\n",
        "    return K[:,:,:2].mean(axis=1)  # Solo x, y promediando todos los puntos\n",
        "\n",
        "def features_coreograficos_mediapipe(K):\n",
        "    \"\"\"K: (T,J,D) con posibles NaNs; devuelve un dict de features.\"\"\"\n",
        "    T,J,D = K.shape\n",
        "    # Usar únicamente x, y para features 2D\n",
        "    K_2d = K[:,:,:2]\n",
        "\n",
        "    amp_mean = (np.nanmax(K_2d, axis=0) - np.nanmin(K_2d, axis=0)).mean(axis=0)\n",
        "    amp_x = float(amp_mean[0]); amp_y = float(amp_mean[1])\n",
        "\n",
        "    vel = np.linalg.norm(np.diff(K_2d, axis=0), axis=2)\n",
        "    vel_media = float(np.nanmean(vel)) if vel.size else 0.0\n",
        "\n",
        "    pair_vals=[]\n",
        "    for a,b in MEDIAPIPE_COCO_PAIRS:\n",
        "        if a < J and b < J:\n",
        "            diff = K_2d[:,a,:] - K_2d[:,b,:]\n",
        "            if diff.ndim > 1:\n",
        "                d = np.linalg.norm(diff, axis=1).mean()  # Eje correcto para (T, D) -> (T,)\n",
        "                pair_vals.append(np.nanmean(d))\n",
        "            elif diff.ndim == 1:  # Caso en el que diff es 1D (T,)\n",
        "                d = np.linalg.norm(diff).mean()\n",
        "                pair_vals.append(np.nanmean(d))\n",
        "    sim_raw = float(np.nanmean(pair_vals)) if pair_vals else np.nan\n",
        "\n",
        "    com = center_of_mass_mediapipe(K)\n",
        "    nivel_p10 = float(np.nanpercentile(com[:,1], 10))\n",
        "    nivel_p90 = float(np.nanpercentile(com[:,1], 90))\n",
        "    nivel_rango = float(nivel_p10 - nivel_p90)\n",
        "\n",
        "    disp = np.diff(com, axis=0)\n",
        "    # Evitar división por cero en arctan2 añadiendo un pequeño epsilon\n",
        "    dirs = np.arctan2(disp[:,1], disp[:,0] + 1e-9)\n",
        "    cambios = np.abs(np.diff(dirs))\n",
        "    variedad_dir = float(np.nanmean(cambios)) if len(cambios) > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"amplitud_x\": amp_x,                  # Mantener nombres originales para consistencia con el modelo\n",
        "        \"amplitud_y\": amp_y,\n",
        "        \"velocidad_media\": vel_media,\n",
        "        \"simetria\": sim_raw,                  # Mantener nombres originales para consistencia con el modelo\n",
        "        \"nivel_rango\": nivel_rango,\n",
        "        \"variedad_direcciones\": variedad_dir, # Mantener nombres originales para consistencia con el modelo\n",
        "        \"frames\": int(T),\n",
        "        \"joints\": int(J),\n",
        "        \"dims\": int(D)\n",
        "    }\n",
        "\n",
        "def inferir_desde_features_dict(feats: dict):\n",
        "    # Asegurar que el orden de las features coincide con el de entrenamiento\n",
        "    x = np.array([[feats.get(c, np.nan) for c in feature_cols]], dtype=float)\n",
        "    x = scaler.transform(imp.transform(x))\n",
        "    yhat = model.predict(x)[0]\n",
        "    # Obtener etiquetas predichas (1 = predicha, 0 = no)\n",
        "    predicted_labels = [label_names[i] for i, v in enumerate(yhat) if v == 1]\n",
        "    sugerencias = [label_to_text.get(lbl, lbl) for lbl in predicted_labels]\n",
        "    return predicted_labels, sugerencias\n",
        "\n",
        "def interpolate_nan_1d(y):\n",
        "    y = y.astype(np.float32)\n",
        "    T = len(y); idx = np.arange(T)\n",
        "    mask = np.isfinite(y)\n",
        "    if mask.sum() == 0: return y\n",
        "    # Relleno hacia delante/atrás\n",
        "    last = np.nan\n",
        "    for i in range(T):\n",
        "        if not np.isfinite(y[i]) and np.isfinite(last): y[i] = last\n",
        "        else: last = y[i]\n",
        "    last = np.nan\n",
        "    for i in range(T-1, -1, -1):\n",
        "        if not np.isfinite(y[i]) and np.isfinite(last): y[i] = last\n",
        "        else: last = y[i]\n",
        "    mask = np.isfinite(y)\n",
        "    if 2 <= mask.sum() < T:\n",
        "        y[~mask] = np.interp(idx[~mask], idx[mask], y[mask])\n",
        "    return y\n",
        "\n",
        "def clean_nan_interpolate(K, min_valid_ratio=0.10):\n",
        "    # Limpia NaNs por articulación y dimension, e interpola si hay datos suficientes\n",
        "    Kc = K.copy()\n",
        "    T,J,D = Kc.shape\n",
        "    used = []\n",
        "    for j in range(J):\n",
        "        # Validez: frames donde todas las dimensiones del punto son finitas\n",
        "        valid = np.isfinite(Kc[:,j,:]).all(axis=1)\n",
        "        if valid.mean() < min_valid_ratio:\n",
        "            Kc[:,j,:] = np.nan\n",
        "            continue\n",
        "        for d in range(D):\n",
        "            # Interpolar cada dimensión por separado\n",
        "            Kc[:,j,d] = interpolate_nan_1d(Kc[:,j,d])\n",
        "        # Comprobar si sigue habiendo algún valor válido tras la interpolación\n",
        "        if np.isfinite(Kc[:,j,:]).any():\n",
        "            used.append(j)\n",
        "\n",
        "    if not used:\n",
        "        print(\"No hay articulaciones válidas tras limpieza e interpolación.\")\n",
        "        return Kc, []  # Devuelve con NaNs si no hay puntos utilizables\n",
        "\n",
        "    # Filtrar para incluir solo las articulaciones usadas\n",
        "    return Kc[:,used,:], used\n",
        "\n",
        "# Aristas COCO simplificadas para dibujado basadas en los primeros 17 puntos de MediaPipe\n",
        "# Es una simplificación; para dibujo preciso se necesitaría un mapeo riguroso.\n",
        "MEDIAPIPE_COCO_EDGES_SIMPLIFIED = [\n",
        "    (11,13), (13,15),      # Brazo izquierdo (hombro a muñeca)\n",
        "    (12,14), (14,16),      # Brazo derecho (hombro a muñeca)\n",
        "    (11,12),               # Hombros\n",
        "    (23,24),               # Caderas\n",
        "    (23,25), (25,27),      # Pierna izquierda (cadera a tobillo)\n",
        "    (24,26), (26,28),      # Pierna derecha (cadera a tobillo)\n",
        "    (11,23), (12,24)       # Tronco (hombro a cadera)\n",
        "]\n",
        "\n",
        "def overlay_suggestions_on_video(video_path, K, timeline, out_path):\n",
        "    # Pinta esqueleto y sugerencias activas en cada frame y exporta vídeo anotado\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    assert cap.isOpened(), f\"No se pudo abrir el vídeo: {video_path}\"\n",
        "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    FPS = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    vw = cv2.VideoWriter(out_path, fourcc, FPS, (W,H))\n",
        "\n",
        "    t = 0\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok: break\n",
        "\n",
        "        if t < len(K):\n",
        "            P = K[t][:,:2]  # Solo x, y para dibujar\n",
        "            # Dibujar esqueleto (simplificado)\n",
        "            for a,b in MEDIAPIPE_COCO_EDGES_SIMPLIFIED:\n",
        "                if a < P.shape[0] and b < P.shape[0]:\n",
        "                    xa,ya = P[a]; xb,yb = P[b]\n",
        "                    if np.isfinite(xa) and np.isfinite(ya) and np.isfinite(xb) and np.isfinite(yb):\n",
        "                        cv2.line(frame, (int(xa),int(ya)), (int(xb),int(yb)), (255,255,255), 2)\n",
        "            for (x,y) in P:\n",
        "                if np.isfinite(x) and np.isfinite(y):\n",
        "                    cv2.circle(frame, (int(x),int(y)), 2, (255,255,255), -1)\n",
        "\n",
        "        # Sugerencias activas para el frame t\n",
        "        active = []\n",
        "        for seg in timeline:\n",
        "            if seg[\"start_f\"] <= t <= seg[\"end_f\"]:\n",
        "                active = seg[\"sugerencias\"]\n",
        "        y0 = 30\n",
        "        for s in active[:4]:\n",
        "            cv2.putText(frame, f\"• {s}\", (20, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 4, cv2.LINE_AA)\n",
        "            cv2.putText(frame, f\"• {s}\", (20, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1, cv2.LINE_AA)\n",
        "            y0 += 28\n",
        "\n",
        "        vw.write(frame); t += 1\n",
        "\n",
        "    cap.release(); vw.release()\n",
        "\n",
        "def run_inference_on_custom_video(video_path, keypoints_array, output_dir, win_sec=5.0, hop_sec=2.5):\n",
        "    \"\"\"\n",
        "    Ejecuta la inferencia sobre un vídeo custom usando keypoints ya extraídos.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Ruta del vídeo original.\n",
        "        keypoints_array (np.ndarray): Array de keypoints de MediaPipe (T, J*D).\n",
        "        output_dir (str): Carpeta para guardar las salidas.\n",
        "        win_sec (float): Tamaño de ventana en segundos para calcular features.\n",
        "        hop_sec (float): Salto entre ventanas en segundos.\n",
        "\n",
        "    Returns:\n",
        "        dict: Rutas al CSV y al vídeo anotado, y la estructura 'timeline'.\n",
        "    \"\"\"\n",
        "    # FPS del vídeo original\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    FPS = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    cap.release()\n",
        "\n",
        "    # Reajustar array de keypoints a (T, J, D)\n",
        "    # MediaPipe devuelve 33 puntos con (x, y, z, visibility)\n",
        "    T = keypoints_array.shape[0]\n",
        "    J = 33  # MediaPipe Pose: 33 puntos\n",
        "    D = 4   # x, y, z, visibility\n",
        "    K_raw = keypoints_array.reshape(T, J, D)\n",
        "\n",
        "    # 2) Limpieza e interpolación de keypoints\n",
        "    Kc, used = clean_nan_interpolate(K_raw, min_valid_ratio=0.10)\n",
        "    if not used:\n",
        "        print(\"No hay articulaciones válidas tras limpieza e interpolación. No se puede continuar.\")\n",
        "        return None\n",
        "\n",
        "    win = max(1, int(round(win_sec * FPS)))\n",
        "    hop = max(1, int(round(hop_sec * FPS)))\n",
        "    T_clean = len(Kc)\n",
        "\n",
        "    rows = []\n",
        "    timeline = []\n",
        "\n",
        "    # 3) Calcular features y generar sugerencias con ventana deslizante\n",
        "    for start in range(0, T_clean - 1, hop):\n",
        "        end = min(T_clean - 1, start + win)\n",
        "        if end - start < max(8, int(0.25 * win)):\n",
        "             break  # Garantizar ventana suficientemente grande\n",
        "\n",
        "        Kw = Kc[start:end]\n",
        "\n",
        "        # Calcular features adaptadas a puntos MediaPipe\n",
        "        feats = features_coreograficos_mediapipe(Kw)\n",
        "\n",
        "        # Preparar features para la inferencia ML\n",
        "        feats_ml = {c: feats.get(c, np.nan) for c in feature_cols}\n",
        "\n",
        "        # Generar sugerencias\n",
        "        labels, suggs = inferir_desde_features_dict(feats_ml)\n",
        "\n",
        "        timeline.append({\"start_f\": start, \"end_f\": end, \"sugerencias\": suggs})\n",
        "\n",
        "        row = {\"start_f\": start, \"end_f\": end, \"start_s\": start / FPS, \"end_s\": end / FPS}\n",
        "        row.update(feats)\n",
        "        row[\"labels\"] = \"|\".join(labels)\n",
        "        row[\"sugerencias\"] = \"|\".join(suggs)\n",
        "        rows.append(row)\n",
        "\n",
        "    # 4) Guardar timeline a CSV\n",
        "    out_csv = os.path.join(output_dir, \"custom_video_suggestions_timeline.csv\")\n",
        "    pd.DataFrame(rows).to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
        "    print(f\"Sugerencias (timeline) guardadas en: {out_csv}\")\n",
        "\n",
        "    # 5) Crear vídeo anotado\n",
        "    out_mp4 = os.path.join(output_dir, \"custom_video_annotated.mp4\")\n",
        "    overlay_suggestions_on_video(video_path, Kc, timeline, out_mp4)\n",
        "    print(f\"Vídeo anotado guardado en: {out_mp4}\")\n",
        "\n",
        "    return {\"csv\": out_csv, \"video\": out_mp4, \"timeline\": timeline}\n",
        "\n",
        "# --- Ejecutar el proceso sobre el vídeo custom ---\n",
        "# Se asume que video_path y keypoints_array existen desde la celda previa.\n",
        "\n",
        "# Para demostración, asumimos que las variables están definidas por la ejecución anterior.\n",
        "# Comprobamos que el vídeo original aún está accesible para la superposición.\n",
        "# Si el archivo temporal se eliminó, habrá que volver a subir o usar una ruta persistente.\n",
        "# Asumimos que video_path apunta a una ubicación persistente o re-subimos si es necesario.\n",
        "\n",
        "# Re-subir el vídeo si el archivo temporal ya no existe\n",
        "try:\n",
        "    # Verificar si el archivo temporal previo sigue existiendo\n",
        "    # Assuming 'video_path' variable exists and has the path from previous execution\n",
        "    if 'video_path' not in locals() or not os.path.exists(video_path):\n",
        "         print(\"No se encontró el vídeo anterior o la ruta no está definida. Vuelve a subir el vídeo, por favor.\")\n",
        "         video_name, video_content = upload_video()\n",
        "         with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(video_name)[1]) as temp_video_file:\n",
        "             temp_video_file.write(video_content)\n",
        "             video_path = temp_video_file.name\n",
        "         print(f\"Vídeo '{video_name}' re-subido y guardado temporalmente en: {video_path}\")\n",
        "\n",
        "    # Asegurar que keypoints_array también esté disponible\n",
        "    if 'keypoints_array' not in locals() or keypoints_array is None:\n",
        "        print(\"No se encontró el array de keypoints. Ejecuta primero el paso de MediaPipe.\")\n",
        "        # cargar desde el .npy guardado\n",
        "        keypoints_path = os.path.join(output_dir, \"custom_video_keypoints.npy\")\n",
        "        if os.path.exists(keypoints_path):\n",
        "            keypoints_array = np.load(keypoints_path)\n",
        "        else:\n",
        "            raise FileNotFoundError(\"No se encontró el archivo de keypoints.\")\n",
        "        # Para este ejemplo, asumimos que keypoints_array estará disponible tras la subida.\n",
        "\n",
        "\n",
        "    if 'video_path' in locals() and video_path and os.path.exists(video_path) and 'keypoints_array' in locals() and keypoints_array is not None:\n",
        "        output_dir = os.path.join(BASE, \"data/processed/custom_videos\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"Procesando vídeo: {video_path}\")\n",
        "        print(f\"Usando array de keypoints con forma: {keypoints_array.shape}\")\n",
        "        res_custom = run_inference_on_custom_video(video_path, keypoints_array, output_dir)\n",
        "        print(\"\\nProcesamiento del vídeo custom finalizado.\")\n",
        "        print(res_custom)\n",
        "\n",
        "        # Limpieza: eliminar el archivo temporal del vídeo al terminar\n",
        "        if 'video_path' in locals() and video_path and os.path.exists(video_path) and video_path.startswith(tempfile.gettempdir()):\n",
        "             os.remove(video_path)\n",
        "             print(f\"Archivo temporal de vídeo eliminado: {video_path}\")\n",
        "\n",
        "    else:\n",
        "        print(\"No se encontró el vídeo, la ruta del vídeo o el array de keypoints. No se puede continuar.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrió un error durante el procesamiento del vídeo custom: {e}\")\n",
        "    # Asegurar que el archivo temporal se elimine también en caso de error\n",
        "    if 'video_path' in locals() and video_path and os.path.exists(video_path) and video_path.startswith(tempfile.gettempdir()):\n",
        "         os.remove(video_path)\n",
        "         print(f\"Archivo temporal eliminado por error: {video_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bCuEms55-Dx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Visualización de frames de un vídeo\n",
        "\n",
        "\n",
        "import os, math\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Opcional (para modo slider)\n",
        "try:\n",
        "    from ipywidgets import interact, IntSlider\n",
        "    HAS_WIDGETS = True\n",
        "except Exception:\n",
        "    HAS_WIDGETS = False\n",
        "\n",
        "# -----------------------------\n",
        "# Config: intenta usar el vídeo anotado por defecto\n",
        "# -----------------------------\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "DEFAULT_DIR = os.path.join(BASE, \"data/processed/custom_videos\")\n",
        "DEFAULT_ANNOTATED = os.path.join(DEFAULT_DIR, \"custom_video_annotated.mp4\")\n",
        "\n",
        "# Cambia esta ruta si quieres otro vídeo\n",
        "video_path = DEFAULT_ANNOTATED if os.path.exists(DEFAULT_ANNOTATED) else None\n",
        "print(\"Vídeo detectado:\", video_path)\n",
        "\n",
        "# -----------------------------\n",
        "# Utilidades de vídeo\n",
        "# -----------------------------\n",
        "def get_video_info(path):\n",
        "    \"\"\"Devuelve diccionario con info básica del vídeo.\"\"\"\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    assert cap.isOpened(), f\"No se pudo abrir el vídeo: {path}\"\n",
        "    fps   = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    nfrm  = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if cap.get(cv2.CAP_PROP_FRAME_COUNT) else 0\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height= int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    cap.release()\n",
        "    duration = nfrm / fps if fps > 0 and nfrm > 0 else 0\n",
        "    return {\"fps\": fps, \"frames\": nfrm, \"width\": width, \"height\": height, \"duration\": duration}\n",
        "\n",
        "def read_frame_at(path, t_sec=None, frame_index=None):\n",
        "    \"\"\"\n",
        "    Lee un frame en 't_sec' (segundos) o en 'frame_index'.\n",
        "    Devuelve (frame_rgb, idx, t_real_sec).\n",
        "    \"\"\"\n",
        "    assert os.path.exists(path), f\"No existe el archivo: {path}\"\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    assert cap.isOpened(), f\"No se pudo abrir el vídeo: {path}\"\n",
        "\n",
        "    fps  = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    nfrm = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if cap.get(cv2.CAP_PROP_FRAME_COUNT) else 0\n",
        "\n",
        "    if frame_index is None:\n",
        "        if t_sec is None:\n",
        "            t_sec = 0.0\n",
        "        frame_index = int(round(t_sec * fps))\n",
        "\n",
        "    frame_index = int(np.clip(frame_index, 0, max(0, nfrm-1)))\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
        "    ok, bgr = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    if not ok or bgr is None:\n",
        "        return None, frame_index, (frame_index / fps if fps > 0 else 0.0)\n",
        "\n",
        "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "    t_real = frame_index / fps if fps > 0 else 0.0\n",
        "    return rgb, frame_index, t_real\n",
        "\n",
        "def show_frame_at_time(path, t_sec):\n",
        "    \"\"\"Muestra un frame en el segundo 't_sec'.\"\"\"\n",
        "    info = get_video_info(path)\n",
        "    img, idx, t_real = read_frame_at(path, t_sec=t_sec)\n",
        "    if img is None:\n",
        "        print(\"No se pudo leer el frame.\")\n",
        "        return\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.imshow(img); plt.axis('off')\n",
        "    plt.title(f\"Frame {idx} @ {t_real:.2f}s  |  {info['width']}x{info['height']}  |  fps={info['fps']:.2f}\")\n",
        "    plt.show()\n",
        "\n",
        "def show_frames_grid(path, num_frames=12, cols=4, margin=0.02):\n",
        "    \"\"\"\n",
        "    Muestra una rejilla de 'num_frames' frames muestreados uniformemente.\n",
        "    \"\"\"\n",
        "    info = get_video_info(path)\n",
        "    n = info[\"frames\"]\n",
        "    if n <= 0:\n",
        "        print(\"No se pudo determinar el número de frames.\")\n",
        "        return\n",
        "\n",
        "    # ÍNDICES muestreados uniformemente (evita extremos por si hay negro)\n",
        "    idxs = np.linspace(int(n*0.02), max(int(n*0.98)-1, 0), num_frames).astype(int)\n",
        "    rows = int(math.ceil(num_frames / cols))\n",
        "    plt.figure(figsize=(4*cols, 3*rows))\n",
        "    for i, idx in enumerate(idxs):\n",
        "        img, _, t_real = read_frame_at(path, frame_index=idx)\n",
        "        plt.subplot(rows, cols, i+1)\n",
        "        if img is None:\n",
        "            plt.text(0.5, 0.5, f\"Frame {idx}\\nNo leído\", ha='center', va='center')\n",
        "            plt.axis('off')\n",
        "        else:\n",
        "            plt.imshow(img); plt.axis('off')\n",
        "            plt.title(f\"{idx} ({t_real:.2f}s)\", fontsize=10)\n",
        "        plt.subplots_adjust(wspace=margin, hspace=margin)\n",
        "    plt.suptitle(f\"Rejilla de frames | total={n} | fps={info['fps']:.2f}\", y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "def show_slider(path):\n",
        "    \"\"\"\n",
        "    Slider interactivo para navegar frames.\n",
        "    Requiere ipywidgets; si no está disponible, avisa.\n",
        "    \"\"\"\n",
        "    if not HAS_WIDGETS:\n",
        "        print(\"ipywidgets no disponible. Instala con: !pip install ipywidgets y reinicia.\")\n",
        "        return\n",
        "    info = get_video_info(path)\n",
        "    n = info[\"frames\"]\n",
        "    def _show(idx):\n",
        "        img, _, t_real = read_frame_at(path, frame_index=idx)\n",
        "        plt.figure(figsize=(6,4))\n",
        "        if img is None:\n",
        "            plt.text(0.5, 0.5, f\"Frame {idx}\\nNo leído\", ha='center', va='center')\n",
        "            plt.axis('off')\n",
        "        else:\n",
        "            plt.imshow(img); plt.axis('off')\n",
        "            plt.title(f\"Frame {idx} @ {t_real:.2f}s\")\n",
        "        plt.show()\n",
        "    interact(_show, idx=IntSlider(min=0, max=max(0,n-1), step=1, value=min(0, n//2)))\n",
        "\n",
        "\n",
        "# Ejemplos de uso\n",
        "if video_path:\n",
        "    info = get_video_info(video_path)\n",
        "    print(f\"Info: {info}\")\n",
        "    show_frames_grid(video_path, num_frames=12, cols=4)\n",
        "else:\n",
        "    print(\"No se encontró el vídeo por defecto. Ajusta la variable 'video_path' manualmente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56m3M4-V7Bvs"
      },
      "source": [
        "Red LSTM para Análisis Secuencial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaqgrCNbLmib"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Bootstrap robusto (imports + checks + carga artefactos)\n",
        "# =========================\n",
        "import os, glob, warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# Imports que faltaban\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import load\n",
        "\n",
        "\n",
        "\n",
        "#  Rutas base\n",
        "BASE        = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "ART         = f\"{BASE}/artifacts\"\n",
        "MODELS_DIR  = f\"{BASE}/models\"\n",
        "REPORTS_DIR = f\"{BASE}/reports\"\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
        "\n",
        "# 3) Verificación de artefactos requeridos\n",
        "required = [\n",
        "    f\"{ART}/imputer.joblib\",\n",
        "    f\"{ART}/scaler.joblib\",\n",
        "    f\"{ART}/model_ovr_logreg.joblib\",\n",
        "    f\"{ART}/feature_cols.csv\",\n",
        "    f\"{ART}/label_names.csv\",\n",
        "]\n",
        "missing = [p for p in required if not os.path.exists(p)]\n",
        "if missing:\n",
        "    raise FileNotFoundError(\n",
        "        \"Faltan artefactos necesarios en ART:\\n  - \" + \"\\n  - \".join(missing)\n",
        "    )\n",
        "\n",
        "# 4) Carga artefactos (ya con pandas importado)\n",
        "imp          = load(f\"{ART}/imputer.joblib\")\n",
        "scaler       = load(f\"{ART}/scaler.joblib\")\n",
        "model_basic  = load(f\"{ART}/model_ovr_logreg.joblib\")\n",
        "feature_cols = pd.read_csv(f\"{ART}/feature_cols.csv\", header=None)[0].tolist()\n",
        "label_names  = pd.read_csv(f\"{ART}/label_names.csv\", header=None)[0].tolist()\n",
        "\n",
        "# 5) Diccionario de textos (si no existe)\n",
        "try:\n",
        "    label_to_text\n",
        "except NameError:\n",
        "    label_to_text = {ln: f\"Sugerencia para {ln}\" for ln in label_names}\n",
        "\n",
        "# 6) Utilidades de datos (con glob y Path ya disponibles)\n",
        "SEARCH_ROOTS = [\n",
        "    f\"{BASE}/data/aistppp\",\n",
        "    f\"{BASE}/data/AIST_ppp\",\n",
        "    f\"{BASE}/data/aistppp_features\",\n",
        "    f\"{BASE}/data/processed\",\n",
        "    f\"{BASE}/data/custom_videos/processed\",\n",
        "    f\"{BASE}/data/demo\",\n",
        "]\n",
        "\n",
        "def _align_features_df_any(df_like, cols):\n",
        "    \"\"\"Asegura DataFrame con columnas EXACTAS (ordenadas); rellena faltantes con NaN.\"\"\"\n",
        "    if isinstance(df_like, pd.DataFrame):\n",
        "        for c in cols:\n",
        "            if c not in df_like.columns:\n",
        "                df_like[c] = np.nan\n",
        "        return df_like[cols]\n",
        "    elif isinstance(df_like, (dict, pd.Series)):\n",
        "        return pd.DataFrame([{c: df_like.get(c, np.nan) for c in cols}])[cols]\n",
        "    else:\n",
        "        raise ValueError(\"Se esperaba DataFrame o dict/Series con nombres de columnas.\")\n",
        "\n",
        "def find_feature_files(roots=SEARCH_ROOTS):\n",
        "    files = []\n",
        "    for r in roots:\n",
        "        if not os.path.isdir(r):\n",
        "            continue\n",
        "        files += glob.glob(os.path.join(r, \"**\", \"features.parquet\"), recursive=True)\n",
        "        files += glob.glob(os.path.join(r, \"**\", \"features.csv\"), recursive=True)\n",
        "    return sorted(set(files))\n",
        "\n",
        "def load_video_features(path):\n",
        "    if path.endswith(\".parquet\"):\n",
        "        # Si falla por motor de parquet, instala pyarrow (ver arriba)\n",
        "        df = pd.read_parquet(path)\n",
        "    else:\n",
        "        df = pd.read_csv(path)\n",
        "    # normalizar variantes de tiempo\n",
        "    ren = {}\n",
        "    if \"start_sec\" not in df.columns and \"start_s\" in df.columns: ren[\"start_s\"] = \"start_sec\"\n",
        "    if \"end_sec\"   not in df.columns and \"end_s\"   in df.columns: ren[\"end_s\"]   = \"end_sec\"\n",
        "    if ren: df = df.rename(columns=ren)\n",
        "    return df, Path(path).parent.name\n",
        "\n",
        "def basic_predict_proba_df(df_feat):\n",
        "    \"\"\"imp + scaler + modelo básico → probabilidades multilabel (N, C).\"\"\"\n",
        "    Xdf  = _align_features_df_any(df_feat, feature_cols)\n",
        "    Ximp = imp.transform(Xdf)\n",
        "    Xs   = scaler.transform(Ximp)\n",
        "    proba = model_basic.predict_proba(Xs)\n",
        "    proba = np.asarray(proba, dtype=np.float32)\n",
        "    if proba.ndim == 1:\n",
        "        proba = proba[:, None]\n",
        "    return proba\n",
        "\n",
        "def build_sequence_dataset_from_features(files, seq_len=12, hop=6):\n",
        "    X_list, y_list, vids = [], [], []\n",
        "    for f in files:\n",
        "        df, vid = load_video_features(f)\n",
        "        # asegurar columnas del modelo básico\n",
        "        for c in feature_cols:\n",
        "            if c not in df.columns:\n",
        "                df[c] = np.nan\n",
        "        Xw = df[feature_cols].astype(np.float32)\n",
        "        Yw = basic_predict_proba_df(Xw)  # (Nw, C)\n",
        "\n",
        "        N = len(Xw)\n",
        "        if N < seq_len:\n",
        "            continue\n",
        "        for s in range(0, N - seq_len + 1, hop):\n",
        "            e = s + seq_len\n",
        "            X_list.append(Xw.iloc[s:e].to_numpy(np.float32))\n",
        "            y_list.append(Yw[s:e].mean(axis=0).astype(np.float32))\n",
        "            vids.append(vid)\n",
        "\n",
        "    if not X_list:\n",
        "        return None\n",
        "\n",
        "    X = np.stack(X_list, 0)  # (Nseq, T, F)\n",
        "    y = np.stack(y_list, 0)  # (Nseq, C)\n",
        "    vids = np.array(vids)\n",
        "\n",
        "    uniq = np.unique(vids)\n",
        "    if len(uniq) >= 2:\n",
        "        np.random.shuffle(uniq)\n",
        "        n_val = max(1, int(round(0.2 * len(uniq))))\n",
        "        val_set = set(uniq[:n_val])\n",
        "        tr = np.array([v not in val_set for v in vids]); va = ~tr\n",
        "    else:\n",
        "        n = len(X)\n",
        "        cut = max(1, int(0.8 * n))\n",
        "        tr = np.zeros(n, bool); tr[:cut] = True; va = ~tr\n",
        "\n",
        "    return {\n",
        "        \"X_train\": X[tr], \"y_train\": y[tr],\n",
        "        \"X_val\":   X[va], \"y_val\":   y[va],\n",
        "        \"vids\": vids\n",
        "    }\n",
        "\n",
        "print(\" Artefactos y utilidades cargados correctamente.\")\n",
        "print(\"   - #feature_cols:\", len(feature_cols), \"| #labels:\", len(label_names))\n",
        "print(\"   - MODELS_DIR:\", MODELS_DIR)\n",
        "print(\"   - REPORTS_DIR:\", REPORTS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4cIkQX-rw8e"
      },
      "source": [
        "# **INTEGRACION FINAL: PIPELINE COMPLETO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2CvqrrOPUBd",
        "outputId": "2556fe9d-0e7d-4add-945b-05dbff28b39e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Entorno inicializado\n",
            "TF: 2.19.0\n",
            "DATA_DIR: /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final\n"
          ]
        }
      ],
      "source": [
        "#  Setup, imports, rutas, hiperparámetros y flags\n",
        "\n",
        "import os, re, json, pickle, math, random, warnings, time, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    _HAS_SNS = True\n",
        "except Exception:\n",
        "    _HAS_SNS = False\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---- Rutas ----\n",
        "BASE        = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "DATA_DIR    = f\"{BASE}/data/annotations/aistpp/aist_plusplus_final\"\n",
        "MODELS_DIR  = f\"{BASE}/models\"\n",
        "REPORTS_DIR = f\"{BASE}/reports\"\n",
        "ART         = f\"{BASE}/artifacts\"\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
        "os.makedirs(ART, exist_ok=True)\n",
        "\n",
        "# ---- Hiperparámetros ----\n",
        "SEQ_LEN    = 30\n",
        "HOP        = 15\n",
        "MODEL_TYPE = \"lstm\"    # 'lstm' | 'bilstm' | 'transformer'\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS     = 50\n",
        "PATIENCE   = 10\n",
        "SEED       = 42\n",
        "\n",
        "# ---- Flags de ejecución ----\n",
        "USE_CACHE      = True     # usa/crea cache de X,y,label_names en ART\n",
        "CLEAR_CACHE    = False    # fuerza ignorar caches\n",
        "LIMIT_FILES    = None     # p.ej. 200 para desarrollo rápido; None = todos\n",
        "VERBOSE_DATA   = True\n",
        "\n",
        "# ---- Semillas y GPU ---\n",
        "np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
        "try:\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    for g in gpus:\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "    if gpus: print(f\"[GPU] {len(gpus)} GPU(s) detectadas.\")\n",
        "except Exception as e:\n",
        "    print(\"[GPU] Mem growth no aplicado:\", e)\n",
        "\n",
        "print(\"[OK] Entorno inicializado\")\n",
        "print(\"TF:\", tf.__version__)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEQTS36-PYgt",
        "outputId": "8b98095f-304a-495b-c6e6-e43280411ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] classes.json cargado (21 clases).\n"
          ]
        }
      ],
      "source": [
        "# Utilidades de clases (mapeo estable con classes.json)\n",
        "\n",
        "_label_to_idx = {}\n",
        "_label_names  = []\n",
        "\n",
        "def _load_existing_classes():\n",
        "    global _label_to_idx, _label_names\n",
        "    path = os.path.join(ART, \"classes.json\")\n",
        "    if os.path.exists(path):\n",
        "        try:\n",
        "            _label_names = list(json.load(open(path)))\n",
        "            _label_to_idx = {name: i for i, name in enumerate(_label_names)}\n",
        "            print(f\"[INFO] classes.json cargado ({len(_label_names)} clases).\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] No se pudo leer classes.json:\", e)\n",
        "    return False\n",
        "\n",
        "def _save_classes():\n",
        "    path = os.path.join(ART, \"classes.json\")\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(_label_names, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"[OK] classes.json guardado ({len(_label_names)} clases).\")\n",
        "\n",
        "def _register_label_stable(label):\n",
        "    global _label_to_idx, _label_names\n",
        "    if label in _label_to_idx:\n",
        "        return _label_to_idx[label]\n",
        "    _label_to_idx[label] = len(_label_to_idx)\n",
        "    _label_names = [\"\"] * len(_label_to_idx)\n",
        "    for k, v in _label_to_idx.items():\n",
        "        _label_names[v] = k\n",
        "    return _label_to_idx[label]\n",
        "\n",
        "def one_hot(idx, C):\n",
        "    v = np.zeros((C,), dtype=np.float32); v[idx] = 1.0\n",
        "    return v\n",
        "\n",
        "_ = _load_existing_classes()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NHshzjMPbnh",
        "outputId": "66673306-e619-40d9-f26e-e53262193d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Funciones KP listas\n"
          ]
        }
      ],
      "source": [
        "# Conversión y limpieza de keypoints\n",
        "\n",
        "def to_TJD_from_any(obj):\n",
        "    \"\"\"Devuelve (T,J,D) con D in {2,3} o None.\"\"\"\n",
        "    arr = None\n",
        "    if isinstance(obj, dict):\n",
        "        for k in [\"keypoints3d\", \"joints3d\", \"poses_3d\", \"kp3d\", \"keypoints\"]:\n",
        "            if k in obj:\n",
        "                arr = np.array(obj[k]); break\n",
        "        if arr is None:\n",
        "            for v in obj.values():\n",
        "                if isinstance(v, np.ndarray):\n",
        "                    arr = v; break\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        arr = obj\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    arr = np.asarray(arr)\n",
        "    if arr.ndim != 3:\n",
        "        return None\n",
        "\n",
        "    T, A, B = arr.shape\n",
        "    for c in [arr, np.transpose(arr, (1,0,2)), np.transpose(arr, (0,2,1))]:\n",
        "        if c.shape[-1] in (2,3):\n",
        "            if c.shape[0] >= c.shape[1]:\n",
        "                return c.astype(np.float32)\n",
        "            else:\n",
        "                return np.transpose(c, (1,0,2)).astype(np.float32)\n",
        "    if A in (17,24,25) and B in (2,3):\n",
        "        return np.transpose(arr, (1,0,2)).astype(np.float32)\n",
        "    if B in (17,24,25) and A in (2,3):\n",
        "        return np.transpose(arr, (0,2,1)).astype(np.float32)\n",
        "    return None\n",
        "\n",
        "def clean_nan_interpolate(kp_TJD, min_valid_ratio=0.10):\n",
        "    \"\"\"Interpola NaNs temporalmente; descarta si ratio finitos < umbral.\"\"\"\n",
        "    x = kp_TJD.copy().astype(np.float32)\n",
        "    valid_ratio = np.isfinite(x).mean()\n",
        "    if valid_ratio < min_valid_ratio:\n",
        "        return None, valid_ratio\n",
        "    T,J,D = x.shape\n",
        "    for j in range(J):\n",
        "        for d in range(D):\n",
        "            series = x[:,j,d]\n",
        "            idx = np.where(np.isfinite(series))[0]\n",
        "            if len(idx) == 0:\n",
        "                x[:,j,d] = 0.0; continue\n",
        "            first, last = idx[0], idx[-1]\n",
        "            series[:first] = series[first]\n",
        "            series[last+1:] = series[last]\n",
        "            miss = np.where(~np.isfinite(series))[0]\n",
        "            if len(miss) > 0:\n",
        "                good = np.where(np.isfinite(series))[0]\n",
        "                x[miss,j,d] = np.interp(miss, good, series[good])\n",
        "    return x, valid_ratio\n",
        "\n",
        "def extract_label_from_filename(fname):\n",
        "    base = os.path.splitext(fname)[0]\n",
        "    m = re.search(r\"(ch\\d+)\", base)\n",
        "    return m.group(1) if m else base\n",
        "\n",
        "print(\"[OK] Funciones KP listas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmRUyW95PeYo",
        "outputId": "6cae535d-b18f-47c1-9f88-8912b4d7777f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CHECK] DATA_DIR: /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final | keypoints3d existe: True\n"
          ]
        }
      ],
      "source": [
        "# Carga AIST++ (keypoints3d) -> lista de (T,J,D) + labels\n",
        "\n",
        "def load_aist_data(limit_files=LIMIT_FILES):\n",
        "    print(\"Cargando datos de AIST++...\")\n",
        "    keypoints_dir = os.path.join(DATA_DIR, \"keypoints3d\")\n",
        "    assert os.path.isdir(keypoints_dir), f\"No existe: {keypoints_dir}\"\n",
        "    files = sorted([f for f in os.listdir(keypoints_dir) if f.endswith(\".pkl\")])\n",
        "    if limit_files is not None:\n",
        "        files = files[:int(limit_files)]\n",
        "        print(f\"[INFO] LIMIT_FILES activo: {len(files)} ficheros.\")\n",
        "\n",
        "    assert len(files) > 0, f\"No hay .pkl en {keypoints_dir}\"\n",
        "\n",
        "    have_fixed = _load_existing_classes()\n",
        "\n",
        "    all_kp, all_idx = [], []\n",
        "    ok, bad = 0, 0\n",
        "    t0 = time.time()\n",
        "    for i, fname in enumerate(files, 1):\n",
        "        try:\n",
        "            with open(os.path.join(keypoints_dir, fname), \"rb\") as f:\n",
        "                obj = pickle.load(f)\n",
        "            kp = to_TJD_from_any(obj)\n",
        "            if kp is None or not np.isfinite(kp).any():\n",
        "                bad += 1; continue\n",
        "            kp, vr = clean_nan_interpolate(kp, min_valid_ratio=0.10)\n",
        "            if kp is None:\n",
        "                bad += 1; continue\n",
        "            lab = extract_label_from_filename(fname)\n",
        "            idx = _register_label_stable(lab)\n",
        "            all_kp.append(kp); all_idx.append(idx); ok += 1\n",
        "        except Exception as e:\n",
        "            print(f\"[ERR] {fname}: {e}\"); bad += 1\n",
        "        if VERBOSE_DATA and i % 200 == 0:\n",
        "            print(f\"  - procesados {i}/{len(files)}\")\n",
        "\n",
        "    print(f\"[DATA] OK={ok} | Bad={bad} | Total={len(files)} | t={time.time()-t0:.1f}s\")\n",
        "    assert ok > 0, \"No se cargaron clips válidos.\"\n",
        "\n",
        "    if not have_fixed:\n",
        "        _save_classes()\n",
        "\n",
        "    C = len(_label_names)\n",
        "    labels_oh = [one_hot(i, C) for i in all_idx]\n",
        "    return all_kp, labels_oh, list(_label_names)\n",
        "\n",
        "print(\"[CHECK] DATA_DIR:\", DATA_DIR, \"| keypoints3d existe:\", os.path.isdir(os.path.join(DATA_DIR, \"keypoints3d\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-Lxqe6YPhpg",
        "outputId": "8049c120-5b8a-491e-aaef-a0992ec493d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Ventanas + cache listo\n"
          ]
        }
      ],
      "source": [
        "def prepare_sequences(kp_list, y_list, seq_len=SEQ_LEN, hop=HOP):\n",
        "    \"\"\"Devuelve X:[N,seq_len,J*D], y:[N,C].\"\"\"\n",
        "    X, Y = [], []\n",
        "    for kp, y in zip(kp_list, y_list):\n",
        "        T,J,D = kp.shape\n",
        "        if T <= 0: continue\n",
        "        for s in range(0, max(1, T - seq_len + 1), hop):\n",
        "            e = s + seq_len\n",
        "            if e <= T:\n",
        "                win = kp[s:e]\n",
        "            else:\n",
        "                pad = np.zeros((e - T, J, D), dtype=kp.dtype)\n",
        "                win = np.concatenate([kp[s:], pad], axis=0)\n",
        "            X.append(win.reshape(seq_len, J*D))\n",
        "            Y.append(y)\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    Y = np.asarray(Y, dtype=np.float32)\n",
        "    print(f\"[SEQ] X={X.shape} y={Y.shape} (seq_len={seq_len}, hop={hop})\")\n",
        "    return X, Y\n",
        "\n",
        "def dataset_cache_paths():\n",
        "    tag = f\"seq{SEQ_LEN}_hop{HOP}\"\n",
        "    return (os.path.join(ART, f\"aist_{tag}_X.npz\"),\n",
        "            os.path.join(ART, f\"aist_{tag}_y.npz\"),\n",
        "            os.path.join(ART, f\"aist_{tag}_label_names.json\"))\n",
        "\n",
        "def load_or_build_dataset():\n",
        "    X_path, y_path, ln_path = dataset_cache_paths()\n",
        "    if CLEAR_CACHE:\n",
        "        for p in [X_path, y_path, ln_path]:\n",
        "            if os.path.exists(p): os.remove(p)\n",
        "        print(\"[CACHE] Limpiado.\")\n",
        "\n",
        "    if USE_CACHE and all(os.path.exists(p) for p in [X_path, y_path, ln_path]):\n",
        "        print(\"[CACHE] Cargando dataset desde cache...\")\n",
        "        X = np.load(X_path)[\"X\"]; y = np.load(y_path)[\"y\"]\n",
        "        label_names = list(json.load(open(ln_path)))\n",
        "        print(f\"[CACHE] X:{X.shape} y:{y.shape} | clases={len(label_names)}\")\n",
        "        return X, y, label_names\n",
        "\n",
        "    print(\"[BUILD] Generando dataset (esto puede tardar)...\")\n",
        "    kp_list, y_list, label_names = load_aist_data()\n",
        "    X, y = prepare_sequences(kp_list, y_list, SEQ_LEN, HOP)\n",
        "\n",
        "    if USE_CACHE:\n",
        "        np.savez_compressed(X_path, X=X)\n",
        "        np.savez_compressed(y_path, y=y)\n",
        "        with open(ln_path, \"w\") as f: json.dump(label_names, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"[CACHE] Guardado: {X_path} | {y_path} | {ln_path}\")\n",
        "\n",
        "    # libera memoria\n",
        "    del kp_list, y_list; gc.collect()\n",
        "    return X, y, label_names\n",
        "\n",
        "print(\"[OK] Ventanas + cache listo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atuaBRZ6PkXv",
        "outputId": "8889d4cc-fd01-4e11-d244-68df180700da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Modelos listos\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def create_lstm_model(input_shape, num_labels):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Masking(mask_value=0., input_shape=input_shape),\n",
        "        tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2),\n",
        "        tf.keras.layers.LSTM(64, dropout=0.3, recurrent_dropout=0.2),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(num_labels, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', 'binary_accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_bilstm_model(input_shape, num_labels):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Masking(mask_value=0., input_shape=input_shape),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.3, recurrent_dropout=0.2)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(num_labels, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', 'binary_accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_transformer_model(input_shape, num_labels):\n",
        "    \"\"\"Transformer simple con proyección para residual.\"\"\"\n",
        "    seq_len, d_model = input_shape\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    pos = tf.range(start=0, limit=seq_len, delta=1)\n",
        "    pos_emb = tf.keras.layers.Embedding(input_dim=seq_len, output_dim=d_model)(pos)  # [T,d_model]\n",
        "    x = inputs + pos_emb  # broadcast sobre batch\n",
        "\n",
        "    attn = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=max(1, d_model//8))(x, x)\n",
        "    attn = tf.keras.layers.Dense(d_model)(attn)  # asegura misma dim para residual\n",
        "    x = tf.keras.layers.Add()([x, attn])\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    ffn = tf.keras.Sequential([tf.keras.layers.Dense(128, activation='relu'),\n",
        "                               tf.keras.layers.Dense(d_model)])(x)\n",
        "    x = tf.keras.layers.Add()([x, ffn])\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.4)(x)\n",
        "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    outputs = tf.keras.layers.Dense(num_labels, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', 'binary_accuracy'])\n",
        "    return model\n",
        "\n",
        "print(\"[OK] Modelos listos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52QxRMDIPnSK"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history, model_type):\n",
        "    hist = history.history\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    ax1.plot(hist.get('loss', []), label='Train')\n",
        "    if 'val_loss' in hist: ax1.plot(hist['val_loss'], label='Val')\n",
        "    ax1.set_title(f'Pérdida - {model_type.upper()}'); ax1.set_xlabel('Época'); ax1.set_ylabel('Loss'); ax1.legend()\n",
        "\n",
        "    mkey = 'binary_accuracy' if 'binary_accuracy' in hist else ('accuracy' if 'accuracy' in hist else None)\n",
        "    if mkey:\n",
        "        ax2.plot(hist[mkey], label='Train')\n",
        "        if f\"val_{mkey}\" in hist: ax2.plot(hist[f\"val_{mkey}\"], label='Val')\n",
        "        ax2.set_title(f'{mkey} - {model_type.upper()}'); ax2.set_xlabel('Época'); ax2.set_ylabel(mkey); ax2.legend()\n",
        "    else:\n",
        "        ax2.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    out_png = os.path.join(REPORTS_DIR, f\"{MODEL_TYPE}_training_curves.png\")\n",
        "    plt.savefig(out_png, dpi=150); plt.show()\n",
        "    print(f\"[OK] Curvas guardadas en: {out_png}\")\n",
        "\n",
        "def _safe_evaluate(model, X, y):\n",
        "    try:\n",
        "        out = model.evaluate(X, y, verbose=0, return_dict=True)\n",
        "        if isinstance(out, dict):\n",
        "            return out\n",
        "    except TypeError:\n",
        "        pass\n",
        "    vals = model.evaluate(X, y, verbose=0)\n",
        "    names = getattr(model, \"metrics_names\", None)\n",
        "    if names and isinstance(vals, (list, tuple)):\n",
        "        return {n: float(v) for n, v in zip(names, vals)}\n",
        "    return {\"loss\": float(vals) if np.isscalar(vals) else float(vals[0])}\n",
        "\n",
        "def generate_evaluation_report(model, X_test, y_test, model_type, label_names):\n",
        "    y_pred = model.predict(X_test, verbose=0)\n",
        "    y_bin  = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    if len(label_names) != y_test.shape[1]:\n",
        "        print(\"[WARN] target_names ≠ nº clases; usaré nombres genéricos.\")\n",
        "        label_names = [f\"cls_{i}\" for i in range(y_test.shape[1])]\n",
        "\n",
        "    report = classification_report(y_test, y_bin, target_names=label_names, output_dict=True, zero_division=0)\n",
        "    rp_path = os.path.join(REPORTS_DIR, f\"{model_type}_classification_report.csv\")\n",
        "    pd.DataFrame(report).transpose().to_csv(rp_path)\n",
        "    print(f\"[OK] Reporte de clasificación en: {rp_path}\")\n",
        "\n",
        "    # Matriz de confusión para UNA etiqueta (binaria)\n",
        "    if _HAS_SNS and y_test.shape[1] > 0:\n",
        "        present = np.where(np.any(y_test, axis=0) | np.any(y_bin, axis=0))[0]\n",
        "        if len(present) > 0:\n",
        "            idx0 = int(present[0])\n",
        "            cm = confusion_matrix(y_test[:, idx0], y_bin[:, idx0])\n",
        "            plt.figure(figsize=(7,6))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                        xticklabels=[\"Pred 0\",\"Pred 1\"],\n",
        "                        yticklabels=[\"Real 0\",\"Real 1\"])\n",
        "            plt.title(f'Confusión - {model_type.upper()} - {label_names[idx0]}')\n",
        "            plt.ylabel('Real'); plt.xlabel('Predicha')\n",
        "            cm_path = os.path.join(REPORTS_DIR, f\"{model_type}_confusion_matrix.png\")\n",
        "            plt.savefig(cm_path, dpi=150); plt.show()\n",
        "            print(f\"[OK] Matriz de confusión en: {cm_path}\")\n",
        "        else:\n",
        "            print(\"[INFO] Sin clases presentes para matriz de confusión.\")\n",
        "    else:\n",
        "        print(\"[INFO] seaborn no disponible o sin clases -> sin matriz de confusión.\")\n",
        "\n",
        "    metrics = _safe_evaluate(model, X_test, y_test)\n",
        "    metrics.update({\n",
        "        \"model_type\": model_type,\n",
        "        \"input_shape\": tuple(int(x) for x in X_test.shape[1:]),\n",
        "        \"num_samples\": int(len(X_test))\n",
        "    })\n",
        "    with open(os.path.join(REPORTS_DIR, f\"{model_type}_metrics.json\"), \"w\") as f:\n",
        "        json.dump({k: float(v) if not isinstance(v, str) else v for k, v in metrics.items()}, f, indent=2)\n",
        "    print(f\"[OK] Métricas guardadas en: {os.path.join(REPORTS_DIR, f'{model_type}_metrics.json')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTXk0kVoZ46z",
        "outputId": "b9be996c-765e-4f22-83fe-52940f04a7cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] train_and_evaluate listo\n"
          ]
        }
      ],
      "source": [
        "#Entrenamiento + Evaluación\n",
        "\n",
        "def train_and_evaluate():\n",
        "    X, y, label_names = load_or_build_dataset()\n",
        "    assert len(X) > 0 and len(y) > 0, \"No hay datos preparados.\"\n",
        "\n",
        "    unique_classes = np.unique(np.argmax(y, axis=1))\n",
        "    if len(unique_classes) > 1:\n",
        "        y_int = np.argmax(y, axis=1)\n",
        "        try:\n",
        "            X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y_int)\n",
        "        except ValueError as e:\n",
        "            print(\"[WARN] Stratify falló:\", e)\n",
        "            X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "        try:\n",
        "            y_tr_int = np.argmax(y_tr, axis=1)\n",
        "            X_tr, X_va, y_tr, y_va = train_test_split(X_tr, y_tr, test_size=0.2, random_state=SEED, stratify=y_tr_int)\n",
        "        except ValueError as e:\n",
        "            print(\"[WARN] Stratify val falló:\", e)\n",
        "            X_tr, X_va, y_tr, y_va = train_test_split(X_tr, y_tr, test_size=0.2, random_state=SEED)\n",
        "    else:\n",
        "        print(\"[WARN] Solo una clase. Split sin estratificar.\")\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "        X_tr, X_va, y_tr, y_va = train_test_split(X_tr, y_tr, test_size=0.2, random_state=SEED)\n",
        "\n",
        "    print(f\"[SPLIT] Train={X_tr.shape} | Val={X_va.shape} | Test={X_te.shape}\")\n",
        "\n",
        "    input_shape = X_tr.shape[1:]\n",
        "    num_labels  = y_tr.shape[1]\n",
        "\n",
        "    if MODEL_TYPE == \"lstm\":\n",
        "        model = create_lstm_model(input_shape, num_labels)\n",
        "    elif MODEL_TYPE == \"bilstm\":\n",
        "        model = create_bilstm_model(input_shape, num_labels)\n",
        "    elif MODEL_TYPE == \"transformer\":\n",
        "        model = create_transformer_model(input_shape, num_labels)\n",
        "    else:\n",
        "        raise ValueError(\"MODEL_TYPE debe ser 'lstm' | 'bilstm' | 'transformer'.\")\n",
        "\n",
        "    ckpt_path = os.path.join(MODELS_DIR, f\"best_{MODEL_TYPE}_model.h5\")\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=max(1, PATIENCE//2), min_lr=1e-7, verbose=1),\n",
        "        ModelCheckpoint(ckpt_path, monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=1)\n",
        "    ]\n",
        "\n",
        "    print(f\"[TRAIN] {MODEL_TYPE.upper()} ...\")\n",
        "    history = model.fit(\n",
        "        X_tr, y_tr,\n",
        "        validation_data=(X_va, y_va),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    metrics = _safe_evaluate(model, X_te, y_te)\n",
        "    loss = metrics.get(\"loss\", np.nan)\n",
        "    acc  = metrics.get(\"accuracy\", np.nan)\n",
        "    bacc = metrics.get(\"binary_accuracy\", np.nan)\n",
        "    print(f\"[TEST] loss={loss:.4f} | acc={acc:.4f} | bin_acc={bacc:.4f}\")\n",
        "\n",
        "    final_path = os.path.join(MODELS_DIR, f\"final_{MODEL_TYPE}_model.h5\")\n",
        "    model.save(final_path)\n",
        "    print(f\"[OK] Modelo final guardado en: {final_path}\")\n",
        "\n",
        "    hist_path = os.path.join(REPORTS_DIR, f\"{MODEL_TYPE}_training_history.csv\")\n",
        "    pd.DataFrame(history.history).to_csv(hist_path, index=False)\n",
        "    print(f\"[OK] Historial guardado en: {hist_path}\")\n",
        "\n",
        "    plot_training_history(history, MODEL_TYPE)\n",
        "    generate_evaluation_report(model, X_te, y_te, MODEL_TYPE, label_names)\n",
        "\n",
        "    return model, history, metrics, label_names\n",
        "\n",
        "print(\"[OK] train_and_evaluate listo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCDlwlFyZ_eQ",
        "outputId": "7693a506-0cd6-4d90-80b8-a14cc1c2ab7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Demo listo\n"
          ]
        }
      ],
      "source": [
        "# Demo con best model\n",
        "\n",
        "def run_demo_with_best_model(label_names):\n",
        "    best_path = os.path.join(MODELS_DIR, f\"best_{MODEL_TYPE}_model.h5\")\n",
        "    if not os.path.exists(best_path):\n",
        "        print(\"[INFO] No hay best model; salto demo.\")\n",
        "        return\n",
        "    print(f\"[DEMO] Cargando: {best_path}\")\n",
        "    model = tf.keras.models.load_model(best_path)\n",
        "\n",
        "    X, y, _ = load_or_build_dataset()\n",
        "    if len(X) == 0:\n",
        "        print(\"[DEMO] No hay datos.\"); return\n",
        "    demo_idx = 0\n",
        "    y_pred = model.predict(X[demo_idx:demo_idx+1], verbose=0)[0]\n",
        "    top = np.argsort(y_pred)[::-1][:min(5, len(label_names))]\n",
        "    print(\"\\n=== DEMO: Top predicciones ===\")\n",
        "    for i, j in enumerate(top, 1):\n",
        "        print(f\"{i}. {label_names[j]}: {y_pred[j]:.3f}\")\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.bar(range(len(y_pred)), y_pred)\n",
        "    plt.xlabel(\"Clases\"); plt.ylabel(\"Prob.\"); plt.title(f\"Predicciones ({MODEL_TYPE.upper()})\")\n",
        "    plt.xticks(range(len(label_names)), label_names, rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    demo_png = os.path.join(REPORTS_DIR, f\"{MODEL_TYPE}_demo_predictions.png\")\n",
        "    plt.savefig(demo_png, dpi=150); plt.show()\n",
        "    print(f\"[OK] Demo guardada en: {demo_png}\")\n",
        "\n",
        "print(\"[OK] Demo listo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U9rQvhovaCcs",
        "outputId": "926ccdf7-528e-4a8f-ce5d-71f2e4ccf3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CHECK] DATA_DIR: /content/drive/MyDrive/asistente_coreografico/data/annotations/aistpp/aist_plusplus_final\n",
            "[CACHE] Cargando dataset desde cache...\n",
            "[CACHE] X:(72887, 30, 51) y:(72887, 21) | clases=21\n",
            "[SPLIT] Train=(46647, 30, 51) | Val=(11662, 30, 51) | Test=(14578, 30, 51)\n",
            "[TRAIN] LSTM ...\n",
            "Epoch 1/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.0750 - binary_accuracy: 0.9171 - loss: 0.2525\n",
            "Epoch 1: val_loss improved from inf to 0.18009, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 146ms/step - accuracy: 0.0750 - binary_accuracy: 0.9171 - loss: 0.2524 - val_accuracy: 0.0853 - val_binary_accuracy: 0.9524 - val_loss: 0.1801 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.0833 - binary_accuracy: 0.9524 - loss: 0.1859\n",
            "Epoch 2: val_loss improved from 0.18009 to 0.17988, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 137ms/step - accuracy: 0.0833 - binary_accuracy: 0.9524 - loss: 0.1859 - val_accuracy: 0.0853 - val_binary_accuracy: 0.9524 - val_loss: 0.1799 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.0829 - binary_accuracy: 0.9524 - loss: 0.1830\n",
            "Epoch 3: val_loss improved from 0.17988 to 0.17908, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 133ms/step - accuracy: 0.0829 - binary_accuracy: 0.9524 - loss: 0.1830 - val_accuracy: 0.0853 - val_binary_accuracy: 0.9524 - val_loss: 0.1791 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.0875 - binary_accuracy: 0.9524 - loss: 0.1820\n",
            "Epoch 4: val_loss improved from 0.17908 to 0.17845, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 136ms/step - accuracy: 0.0875 - binary_accuracy: 0.9524 - loss: 0.1820 - val_accuracy: 0.0879 - val_binary_accuracy: 0.9524 - val_loss: 0.1784 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.0934 - binary_accuracy: 0.9524 - loss: 0.1814\n",
            "Epoch 5: val_loss improved from 0.17845 to 0.17821, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 131ms/step - accuracy: 0.0934 - binary_accuracy: 0.9524 - loss: 0.1814 - val_accuracy: 0.0971 - val_binary_accuracy: 0.9524 - val_loss: 0.1782 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.0989 - binary_accuracy: 0.9524 - loss: 0.1808\n",
            "Epoch 6: val_loss improved from 0.17821 to 0.17786, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 141ms/step - accuracy: 0.0989 - binary_accuracy: 0.9524 - loss: 0.1808 - val_accuracy: 0.1048 - val_binary_accuracy: 0.9524 - val_loss: 0.1779 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.0969 - binary_accuracy: 0.9524 - loss: 0.1806\n",
            "Epoch 7: val_loss improved from 0.17786 to 0.17777, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 134ms/step - accuracy: 0.0969 - binary_accuracy: 0.9524 - loss: 0.1806 - val_accuracy: 0.1019 - val_binary_accuracy: 0.9524 - val_loss: 0.1778 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.1026 - binary_accuracy: 0.9524 - loss: 0.1801\n",
            "Epoch 8: val_loss improved from 0.17777 to 0.17722, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 136ms/step - accuracy: 0.1026 - binary_accuracy: 0.9524 - loss: 0.1801 - val_accuracy: 0.1088 - val_binary_accuracy: 0.9524 - val_loss: 0.1772 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.1040 - binary_accuracy: 0.9524 - loss: 0.1798\n",
            "Epoch 9: val_loss improved from 0.17722 to 0.17720, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 131ms/step - accuracy: 0.1040 - binary_accuracy: 0.9524 - loss: 0.1798 - val_accuracy: 0.1126 - val_binary_accuracy: 0.9524 - val_loss: 0.1772 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.1058 - binary_accuracy: 0.9524 - loss: 0.1795\n",
            "Epoch 10: val_loss improved from 0.17720 to 0.17704, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 132ms/step - accuracy: 0.1058 - binary_accuracy: 0.9524 - loss: 0.1795 - val_accuracy: 0.1051 - val_binary_accuracy: 0.9524 - val_loss: 0.1770 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.1047 - binary_accuracy: 0.9524 - loss: 0.1793\n",
            "Epoch 11: val_loss improved from 0.17704 to 0.17667, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 131ms/step - accuracy: 0.1047 - binary_accuracy: 0.9524 - loss: 0.1793 - val_accuracy: 0.1090 - val_binary_accuracy: 0.9524 - val_loss: 0.1767 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.1027 - binary_accuracy: 0.9524 - loss: 0.1791\n",
            "Epoch 12: val_loss improved from 0.17667 to 0.17664, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 136ms/step - accuracy: 0.1027 - binary_accuracy: 0.9524 - loss: 0.1791 - val_accuracy: 0.1086 - val_binary_accuracy: 0.9524 - val_loss: 0.1766 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.1042 - binary_accuracy: 0.9524 - loss: 0.1788\n",
            "Epoch 13: val_loss improved from 0.17664 to 0.17630, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 131ms/step - accuracy: 0.1042 - binary_accuracy: 0.9524 - loss: 0.1788 - val_accuracy: 0.1113 - val_binary_accuracy: 0.9524 - val_loss: 0.1763 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.1057 - binary_accuracy: 0.9524 - loss: 0.1787\n",
            "Epoch 14: val_loss improved from 0.17630 to 0.17623, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 130ms/step - accuracy: 0.1057 - binary_accuracy: 0.9524 - loss: 0.1787 - val_accuracy: 0.1154 - val_binary_accuracy: 0.9524 - val_loss: 0.1762 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.1078 - binary_accuracy: 0.9524 - loss: 0.1786\n",
            "Epoch 15: val_loss improved from 0.17623 to 0.17610, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 132ms/step - accuracy: 0.1078 - binary_accuracy: 0.9524 - loss: 0.1786 - val_accuracy: 0.1163 - val_binary_accuracy: 0.9524 - val_loss: 0.1761 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.1092 - binary_accuracy: 0.9524 - loss: 0.1784\n",
            "Epoch 16: val_loss improved from 0.17610 to 0.17597, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 133ms/step - accuracy: 0.1092 - binary_accuracy: 0.9524 - loss: 0.1784 - val_accuracy: 0.1156 - val_binary_accuracy: 0.9524 - val_loss: 0.1760 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.1120 - binary_accuracy: 0.9524 - loss: 0.1783\n",
            "Epoch 17: val_loss did not improve from 0.17597\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 146ms/step - accuracy: 0.1120 - binary_accuracy: 0.9524 - loss: 0.1783 - val_accuracy: 0.1152 - val_binary_accuracy: 0.9524 - val_loss: 0.1760 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.1077 - binary_accuracy: 0.9524 - loss: 0.1782\n",
            "Epoch 18: val_loss improved from 0.17597 to 0.17588, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 150ms/step - accuracy: 0.1077 - binary_accuracy: 0.9524 - loss: 0.1782 - val_accuracy: 0.1140 - val_binary_accuracy: 0.9524 - val_loss: 0.1759 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.1090 - binary_accuracy: 0.9524 - loss: 0.1781\n",
            "Epoch 19: val_loss improved from 0.17588 to 0.17577, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 137ms/step - accuracy: 0.1090 - binary_accuracy: 0.9524 - loss: 0.1781 - val_accuracy: 0.1229 - val_binary_accuracy: 0.9524 - val_loss: 0.1758 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.1085 - binary_accuracy: 0.9524 - loss: 0.1779\n",
            "Epoch 20: val_loss improved from 0.17577 to 0.17568, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 146ms/step - accuracy: 0.1085 - binary_accuracy: 0.9524 - loss: 0.1779 - val_accuracy: 0.1255 - val_binary_accuracy: 0.9524 - val_loss: 0.1757 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.1142 - binary_accuracy: 0.9524 - loss: 0.1779\n",
            "Epoch 21: val_loss improved from 0.17568 to 0.17566, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 142ms/step - accuracy: 0.1142 - binary_accuracy: 0.9524 - loss: 0.1779 - val_accuracy: 0.1185 - val_binary_accuracy: 0.9524 - val_loss: 0.1757 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.1129 - binary_accuracy: 0.9524 - loss: 0.1777\n",
            "Epoch 22: val_loss improved from 0.17566 to 0.17555, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 143ms/step - accuracy: 0.1129 - binary_accuracy: 0.9524 - loss: 0.1777 - val_accuracy: 0.1212 - val_binary_accuracy: 0.9524 - val_loss: 0.1755 - learning_rate: 0.0010\n",
            "Epoch 23/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.1147 - binary_accuracy: 0.9524 - loss: 0.1776\n",
            "Epoch 23: val_loss improved from 0.17555 to 0.17549, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 142ms/step - accuracy: 0.1147 - binary_accuracy: 0.9524 - loss: 0.1776 - val_accuracy: 0.1218 - val_binary_accuracy: 0.9524 - val_loss: 0.1755 - learning_rate: 0.0010\n",
            "Epoch 24/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.1144 - binary_accuracy: 0.9524 - loss: 0.1777\n",
            "Epoch 24: val_loss improved from 0.17549 to 0.17534, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 134ms/step - accuracy: 0.1144 - binary_accuracy: 0.9524 - loss: 0.1777 - val_accuracy: 0.1253 - val_binary_accuracy: 0.9524 - val_loss: 0.1753 - learning_rate: 0.0010\n",
            "Epoch 25/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.1134 - binary_accuracy: 0.9524 - loss: 0.1773\n",
            "Epoch 25: val_loss improved from 0.17534 to 0.17516, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 137ms/step - accuracy: 0.1134 - binary_accuracy: 0.9524 - loss: 0.1773 - val_accuracy: 0.1229 - val_binary_accuracy: 0.9524 - val_loss: 0.1752 - learning_rate: 0.0010\n",
            "Epoch 26/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.1164 - binary_accuracy: 0.9524 - loss: 0.1774\n",
            "Epoch 26: val_loss improved from 0.17516 to 0.17482, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 134ms/step - accuracy: 0.1164 - binary_accuracy: 0.9524 - loss: 0.1774 - val_accuracy: 0.1263 - val_binary_accuracy: 0.9524 - val_loss: 0.1748 - learning_rate: 0.0010\n",
            "Epoch 27/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.1143 - binary_accuracy: 0.9524 - loss: 0.1772\n",
            "Epoch 27: val_loss improved from 0.17482 to 0.17467, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 131ms/step - accuracy: 0.1143 - binary_accuracy: 0.9524 - loss: 0.1772 - val_accuracy: 0.1243 - val_binary_accuracy: 0.9526 - val_loss: 0.1747 - learning_rate: 0.0010\n",
            "Epoch 28/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.1169 - binary_accuracy: 0.9525 - loss: 0.1768\n",
            "Epoch 28: val_loss did not improve from 0.17467\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 129ms/step - accuracy: 0.1169 - binary_accuracy: 0.9525 - loss: 0.1768 - val_accuracy: 0.1280 - val_binary_accuracy: 0.9526 - val_loss: 0.1748 - learning_rate: 0.0010\n",
            "Epoch 29/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.1170 - binary_accuracy: 0.9525 - loss: 0.1767\n",
            "Epoch 29: val_loss did not improve from 0.17467\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 132ms/step - accuracy: 0.1170 - binary_accuracy: 0.9525 - loss: 0.1767 - val_accuracy: 0.1334 - val_binary_accuracy: 0.9526 - val_loss: 0.1747 - learning_rate: 0.0010\n",
            "Epoch 30/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.1192 - binary_accuracy: 0.9525 - loss: 0.1767\n",
            "Epoch 30: val_loss improved from 0.17467 to 0.17450, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 136ms/step - accuracy: 0.1192 - binary_accuracy: 0.9525 - loss: 0.1767 - val_accuracy: 0.1327 - val_binary_accuracy: 0.9526 - val_loss: 0.1745 - learning_rate: 0.0010\n",
            "Epoch 31/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.1229 - binary_accuracy: 0.9525 - loss: 0.1765\n",
            "Epoch 31: val_loss did not improve from 0.17450\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 133ms/step - accuracy: 0.1229 - binary_accuracy: 0.9525 - loss: 0.1765 - val_accuracy: 0.1314 - val_binary_accuracy: 0.9525 - val_loss: 0.1745 - learning_rate: 0.0010\n",
            "Epoch 32/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.1204 - binary_accuracy: 0.9525 - loss: 0.1765\n",
            "Epoch 32: val_loss improved from 0.17450 to 0.17412, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 134ms/step - accuracy: 0.1204 - binary_accuracy: 0.9525 - loss: 0.1765 - val_accuracy: 0.1358 - val_binary_accuracy: 0.9526 - val_loss: 0.1741 - learning_rate: 0.0010\n",
            "Epoch 33/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.1188 - binary_accuracy: 0.9525 - loss: 0.1765\n",
            "Epoch 33: val_loss did not improve from 0.17412\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 133ms/step - accuracy: 0.1188 - binary_accuracy: 0.9525 - loss: 0.1765 - val_accuracy: 0.1320 - val_binary_accuracy: 0.9526 - val_loss: 0.1743 - learning_rate: 0.0010\n",
            "Epoch 34/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.1196 - binary_accuracy: 0.9525 - loss: 0.1764\n",
            "Epoch 34: val_loss did not improve from 0.17412\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 132ms/step - accuracy: 0.1196 - binary_accuracy: 0.9525 - loss: 0.1764 - val_accuracy: 0.1379 - val_binary_accuracy: 0.9526 - val_loss: 0.1741 - learning_rate: 0.0010\n",
            "Epoch 35/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.1244 - binary_accuracy: 0.9525 - loss: 0.1763\n",
            "Epoch 35: val_loss improved from 0.17412 to 0.17400, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 140ms/step - accuracy: 0.1244 - binary_accuracy: 0.9525 - loss: 0.1763 - val_accuracy: 0.1387 - val_binary_accuracy: 0.9526 - val_loss: 0.1740 - learning_rate: 0.0010\n",
            "Epoch 36/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.1248 - binary_accuracy: 0.9525 - loss: 0.1762\n",
            "Epoch 36: val_loss improved from 0.17400 to 0.17380, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 133ms/step - accuracy: 0.1248 - binary_accuracy: 0.9525 - loss: 0.1762 - val_accuracy: 0.1405 - val_binary_accuracy: 0.9526 - val_loss: 0.1738 - learning_rate: 0.0010\n",
            "Epoch 37/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.1212 - binary_accuracy: 0.9525 - loss: 0.1760\n",
            "Epoch 37: val_loss improved from 0.17380 to 0.17372, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 141ms/step - accuracy: 0.1212 - binary_accuracy: 0.9525 - loss: 0.1760 - val_accuracy: 0.1436 - val_binary_accuracy: 0.9526 - val_loss: 0.1737 - learning_rate: 0.0010\n",
            "Epoch 38/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.1236 - binary_accuracy: 0.9525 - loss: 0.1759\n",
            "Epoch 38: val_loss did not improve from 0.17372\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 133ms/step - accuracy: 0.1236 - binary_accuracy: 0.9525 - loss: 0.1759 - val_accuracy: 0.1407 - val_binary_accuracy: 0.9526 - val_loss: 0.1739 - learning_rate: 0.0010\n",
            "Epoch 39/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.1240 - binary_accuracy: 0.9525 - loss: 0.1760\n",
            "Epoch 39: val_loss improved from 0.17372 to 0.17371, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 136ms/step - accuracy: 0.1240 - binary_accuracy: 0.9525 - loss: 0.1760 - val_accuracy: 0.1400 - val_binary_accuracy: 0.9526 - val_loss: 0.1737 - learning_rate: 0.0010\n",
            "Epoch 40/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.1244 - binary_accuracy: 0.9525 - loss: 0.1756\n",
            "Epoch 40: val_loss did not improve from 0.17371\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 140ms/step - accuracy: 0.1244 - binary_accuracy: 0.9525 - loss: 0.1756 - val_accuracy: 0.1459 - val_binary_accuracy: 0.9526 - val_loss: 0.1738 - learning_rate: 0.0010\n",
            "Epoch 41/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.1240 - binary_accuracy: 0.9525 - loss: 0.1757\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\n",
            "Epoch 41: val_loss did not improve from 0.17371\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 134ms/step - accuracy: 0.1240 - binary_accuracy: 0.9525 - loss: 0.1757 - val_accuracy: 0.1458 - val_binary_accuracy: 0.9526 - val_loss: 0.1737 - learning_rate: 0.0010\n",
            "Epoch 42/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.1280 - binary_accuracy: 0.9526 - loss: 0.1754\n",
            "Epoch 42: val_loss improved from 0.17371 to 0.17333, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 136ms/step - accuracy: 0.1280 - binary_accuracy: 0.9526 - loss: 0.1754 - val_accuracy: 0.1489 - val_binary_accuracy: 0.9526 - val_loss: 0.1733 - learning_rate: 2.0000e-04\n",
            "Epoch 43/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.1281 - binary_accuracy: 0.9525 - loss: 0.1751\n",
            "Epoch 43: val_loss improved from 0.17333 to 0.17325, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 133ms/step - accuracy: 0.1281 - binary_accuracy: 0.9525 - loss: 0.1751 - val_accuracy: 0.1447 - val_binary_accuracy: 0.9526 - val_loss: 0.1733 - learning_rate: 2.0000e-04\n",
            "Epoch 44/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.1278 - binary_accuracy: 0.9525 - loss: 0.1752\n",
            "Epoch 44: val_loss improved from 0.17325 to 0.17320, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 131ms/step - accuracy: 0.1278 - binary_accuracy: 0.9525 - loss: 0.1752 - val_accuracy: 0.1441 - val_binary_accuracy: 0.9526 - val_loss: 0.1732 - learning_rate: 2.0000e-04\n",
            "Epoch 45/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.1301 - binary_accuracy: 0.9526 - loss: 0.1752\n",
            "Epoch 45: val_loss improved from 0.17320 to 0.17300, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 131ms/step - accuracy: 0.1301 - binary_accuracy: 0.9526 - loss: 0.1752 - val_accuracy: 0.1488 - val_binary_accuracy: 0.9526 - val_loss: 0.1730 - learning_rate: 2.0000e-04\n",
            "Epoch 46/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.1301 - binary_accuracy: 0.9526 - loss: 0.1750\n",
            "Epoch 46: val_loss did not improve from 0.17300\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 133ms/step - accuracy: 0.1301 - binary_accuracy: 0.9526 - loss: 0.1750 - val_accuracy: 0.1495 - val_binary_accuracy: 0.9526 - val_loss: 0.1730 - learning_rate: 2.0000e-04\n",
            "Epoch 47/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.1367 - binary_accuracy: 0.9525 - loss: 0.1750\n",
            "Epoch 47: val_loss improved from 0.17300 to 0.17292, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 130ms/step - accuracy: 0.1367 - binary_accuracy: 0.9525 - loss: 0.1750 - val_accuracy: 0.1534 - val_binary_accuracy: 0.9527 - val_loss: 0.1729 - learning_rate: 2.0000e-04\n",
            "Epoch 48/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.1338 - binary_accuracy: 0.9525 - loss: 0.1749\n",
            "Epoch 48: val_loss did not improve from 0.17292\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 129ms/step - accuracy: 0.1338 - binary_accuracy: 0.9525 - loss: 0.1749 - val_accuracy: 0.1525 - val_binary_accuracy: 0.9526 - val_loss: 0.1729 - learning_rate: 2.0000e-04\n",
            "Epoch 49/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.1328 - binary_accuracy: 0.9525 - loss: 0.1750\n",
            "Epoch 49: val_loss improved from 0.17292 to 0.17273, saving model to /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 137ms/step - accuracy: 0.1328 - binary_accuracy: 0.9525 - loss: 0.1750 - val_accuracy: 0.1510 - val_binary_accuracy: 0.9527 - val_loss: 0.1727 - learning_rate: 2.0000e-04\n",
            "Epoch 50/50\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.1315 - binary_accuracy: 0.9526 - loss: 0.1747\n",
            "Epoch 50: val_loss did not improve from 0.17273\n",
            "\u001b[1m1458/1458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 135ms/step - accuracy: 0.1315 - binary_accuracy: 0.9526 - loss: 0.1747 - val_accuracy: 0.1523 - val_binary_accuracy: 0.9527 - val_loss: 0.1728 - learning_rate: 2.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] loss=0.1728 | acc=0.1503 | bin_acc=0.9527\n",
            "[OK] Modelo final guardado en: /content/drive/MyDrive/asistente_coreografico/models/final_lstm_model.h5\n",
            "[OK] Historial guardado en: /content/drive/MyDrive/asistente_coreografico/reports/lstm_training_history.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Curvas guardadas en: /content/drive/MyDrive/asistente_coreografico/reports/lstm_training_curves.png\n",
            "[OK] Reporte de clasificación en: /content/drive/MyDrive/asistente_coreografico/reports/lstm_classification_report.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Matriz de confusión en: /content/drive/MyDrive/asistente_coreografico/reports/lstm_confusion_matrix.png\n",
            " Error durante train_and_evaluate(): float() argument must be a string or a real number, not 'tuple'\n",
            " Continuo con demo usando best model existente: /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n",
            "[DEMO] Cargando: /content/drive/MyDrive/asistente_coreografico/models/best_lstm_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1039579478.py\", line 47, in <cell line: 0>\n",
            "    model, history, test_metrics, label_names = train_and_evaluate()\n",
            "                                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-519205845.py\", line 72, in train_and_evaluate\n",
            "    generate_evaluation_report(model, X_te, y_te, MODEL_TYPE, label_names)\n",
            "  File \"/tmp/ipython-input-3473756780.py\", line 75, in generate_evaluation_report\n",
            "    json.dump({k: float(v) if not isinstance(v, str) else v for k, v in metrics.items()}, f, indent=2)\n",
            "                  ^^^^^^^^\n",
            "TypeError: float() argument must be a string or a real number, not 'tuple'\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CACHE] Cargando dataset desde cache...\n",
            "[CACHE] X:(72887, 30, 51) y:(72887, 21) | clases=21\n",
            "\n",
            "=== DEMO: Top predicciones ===\n",
            "1. ch03: 0.139\n",
            "2. ch02: 0.110\n",
            "3. ch01: 0.109\n",
            "4. ch04: 0.102\n",
            "5. ch09: 0.082\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Demo guardada en: /content/drive/MyDrive/asistente_coreografico/reports/lstm_demo_predictions.png\n",
            "\n",
            "============================================================\n",
            "PROCESO COMPLETADO\n",
            "============================================================\n",
            "Modelo: LSTM\n",
            "Modelos guardados en: /content/drive/MyDrive/asistente_coreografico/models\n",
            "Reportes guardados en: /content/drive/MyDrive/asistente_coreografico/reports\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Ejecutar (checks + entrenamiento + demo)\n",
        "\n",
        "import os, json, numpy as np\n",
        "\n",
        "try:\n",
        "    BASE\n",
        "except NameError:\n",
        "    BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "try:\n",
        "    DATA_DIR\n",
        "except NameError:\n",
        "    DATA_DIR = f\"{BASE}/data/annotations/aistpp/aist_plusplus_final\"\n",
        "try:\n",
        "    MODELS_DIR\n",
        "except NameError:\n",
        "    MODELS_DIR = f\"{BASE}/models\"\n",
        "try:\n",
        "    REPORTS_DIR\n",
        "except NameError:\n",
        "    REPORTS_DIR = f\"{BASE}/reports\"\n",
        "try:\n",
        "    MODEL_TYPE\n",
        "except NameError:\n",
        "    MODEL_TYPE = \"lstm\"  # 'lstm' | 'bilstm' | 'transformer'\n",
        "\n",
        "print(\"[CHECK] DATA_DIR:\", DATA_DIR)\n",
        "if not os.path.isdir(DATA_DIR):\n",
        "    raise FileNotFoundError(\n",
        "        f\"No existe DATA_DIR: {DATA_DIR}\\n\"\n",
        "        \"Verifica el montaje de Drive y la ruta.\"\n",
        "    )\n",
        "\n",
        "kp_root = os.path.join(DATA_DIR, \"keypoints3d\")\n",
        "if not os.path.isdir(kp_root):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Falta carpeta keypoints3d en: {kp_root}\\n\"\n",
        "        \"Asegúrate de tener los .pkl de AIST++ en esa carpeta.\"\n",
        "    )\n",
        "\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
        "\n",
        "model = history = test_metrics = label_names = None\n",
        "\n",
        "# Entrenamiento + evaluación con manejo de errores\n",
        "try:\n",
        "    model, history, test_metrics, label_names = train_and_evaluate()\n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    print(\" Error durante train_and_evaluate():\", e)\n",
        "    traceback.print_exc()\n",
        "\n",
        "    # Si existe un best model previo, intenta continuar con la demo\n",
        "    best_path = os.path.join(MODELS_DIR, f\"best_{MODEL_TYPE}_model.h5\")\n",
        "    if os.path.exists(best_path):\n",
        "        print(f\" Continuo con demo usando best model existente: {best_path}\")\n",
        "        if not label_names:\n",
        "            classes_json = os.path.join(BASE, \"artifacts\", \"classes.json\")\n",
        "            if os.path.exists(classes_json):\n",
        "                try:\n",
        "                    label_names = json.load(open(classes_json))\n",
        "                except Exception:\n",
        "                    label_names = []\n",
        "    else:\n",
        "        # No hay best model para continuar\n",
        "        raise\n",
        "\n",
        "\n",
        "def _pp_metrics(m):\n",
        "    if m is None:\n",
        "        return\n",
        "    try:\n",
        "        items = m.items() if isinstance(m, dict) else enumerate(m)\n",
        "        print(\"[TEST METRICS]\")\n",
        "        for k, v in items:\n",
        "            if isinstance(v, (list, tuple, np.ndarray)):\n",
        "                # imprime solo el primer valor si es iterable/largo\n",
        "                try:\n",
        "                    v0 = float(v[0]) if len(v) > 0 else v\n",
        "                    print(f\" - {k}: {v0} (…)\")\n",
        "                except Exception:\n",
        "                    print(f\" - {k}: {v} (…)\")\n",
        "            else:\n",
        "                try:\n",
        "                    print(f\" - {k}: {float(v):.4f}\")\n",
        "                except Exception:\n",
        "                    print(f\" - {k}: {v}\")\n",
        "    except Exception as e:\n",
        "        print(\"No se pudieron formatear métricas:\", e, \"| raw:\", m)\n",
        "\n",
        "_pp_metrics(test_metrics)\n",
        "\n",
        "# Demo con el best model disponible\n",
        "try:\n",
        "    if not label_names:\n",
        "        classes_json = os.path.join(BASE, \"artifacts\", \"classes.json\")\n",
        "        if os.path.exists(classes_json):\n",
        "            try:\n",
        "                label_names = json.load(open(classes_json))\n",
        "            except Exception:\n",
        "                label_names = []\n",
        "    run_demo_with_best_model(label_names if label_names else [])\n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    print(\" Demo falló:\", e)\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROCESO COMPLETADO\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Modelo: {MODEL_TYPE.upper()}\")\n",
        "print(f\"Modelos guardados en: {MODELS_DIR}\")\n",
        "print(f\"Reportes guardados en: {REPORTS_DIR}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMuRgMsXhMEI"
      },
      "outputs": [],
      "source": [
        "# === Sustituye SOLO esta función por la versión de abajo ===\n",
        "import tempfile, shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "def export_best_model_to_onnx(model_type=None, models_dir=None, opset_candidates=(17, 15, 13)):\n",
        "    \"\"\"\n",
        "    Exporta a ONNX de forma robusta:\n",
        "      1) Carga el mejor .keras/.h5 (o lo reconstruye si sólo hay pesos).\n",
        "      2) Intenta convertir con ConcreteFunction (from_function).\n",
        "      3) Si falla, exporta a SavedModel y convierte desde ahí.\n",
        "    \"\"\"\n",
        "    model_type = model_type or MODEL_TYPE\n",
        "    models_dir = models_dir or MODELS_DIR\n",
        "\n",
        "    # 1) Obtener el modelo listo\n",
        "    best_path = _ensure_best_model_file(model_type)  # usa tu helper\n",
        "    model = tf.keras.models.load_model(best_path)\n",
        "\n",
        "    # 2) Deducir la forma de entrada (T,F)\n",
        "    in_shape = model.inputs[0].shape  # (None, T, F)\n",
        "    T = int(in_shape[1]) if in_shape[1] is not None else SEQ_LEN\n",
        "    Fok = int(in_shape[2]) if in_shape[2] is not None else (F or 0)\n",
        "    assert T > 0 and Fok > 0, f\"Input shape inválido: {in_shape}\"\n",
        "\n",
        "    # 3) Firma de entrada explícita\n",
        "    spec = [tf.TensorSpec([None, T, Fok], tf.float32, name=\"input\")]\n",
        "    onnx_out = os.path.join(models_dir, f\"best_{model_type}_model.onnx\")\n",
        "\n",
        "    last_err = None\n",
        "    for ops in opset_candidates:\n",
        "        print(f\"[ONNX] Exportando con opset={ops} ...\")\n",
        "\n",
        "        # ---- A) Intento: ConcreteFunction (compatible Keras 3) ----\n",
        "        try:\n",
        "            @tf.function(input_signature=spec)\n",
        "            def _wrapped(x):\n",
        "                return model(x)\n",
        "            concrete = _wrapped.get_concrete_function()\n",
        "\n",
        "            _onnx_model, _ = tf2onnx.convert.from_function(\n",
        "                concrete_function=concrete,\n",
        "                opset=ops,\n",
        "                input_signature=spec,\n",
        "                output_path=onnx_out\n",
        "            )\n",
        "            print(f\"[OK] Exportado (from_function) → {onnx_out}\")\n",
        "            return onnx_out, (T, Fok)\n",
        "        except Exception as e_func:\n",
        "            last_err = e_func\n",
        "            print(f\"[WARN] Falla from_function (opset={ops}): {e_func}\")\n",
        "\n",
        "        # ---- B) Fallback: SavedModel → ONNX ----\n",
        "        tmpdir = tempfile.mkdtemp(prefix=\"savedmodel_for_onnx_\")\n",
        "        try:\n",
        "            # Keras 3 tiene .export; si no, usamos tf.saved_model.save\n",
        "            try:\n",
        "                model.export(tmpdir)  # Keras 3\n",
        "            except Exception:\n",
        "                tf.saved_model.save(model, tmpdir)  # Keras 2.x compat\n",
        "\n",
        "            _onnx_model, _ = tf2onnx.convert.from_saved_model(\n",
        "                saved_model_dir=tmpdir,\n",
        "                opset=ops,\n",
        "                output_path=onnx_out\n",
        "            )\n",
        "            print(f\"[OK] Exportado (from_saved_model) → {onnx_out}\")\n",
        "            return onnx_out, (T, Fok)\n",
        "        except Exception as e_sm:\n",
        "            last_err = (last_err, e_sm)\n",
        "            print(f\"[WARN] Falla from_saved_model (opset={ops}): {e_sm}\")\n",
        "        finally:\n",
        "            shutil.rmtree(tmpdir, ignore_errors=True)\n",
        "\n",
        "    raise RuntimeError(f\"No se pudo exportar a ONNX. Último error: {last_err}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "TB5TG8-SYVz0",
        "outputId": "134873e4-fb7f-42b7-d996-e75fc05baa1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded basic model artifacts.\n",
            "Using label_names from classes.json (21)\n",
            "Por favor, sube un vídeo custom para iniciar el pipeline.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bba0cce5-8dcf-483f-912b-5883e45489b6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bba0cce5-8dcf-483f-912b-5883e45489b6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving solo.mp4 to solo (3).mp4\n",
            "Vídeo 'solo (3).mp4' subido y guardado temporalmente en: /tmp/tmp3b4qnx1f.mp4\n",
            "\n",
            "Ejecutando pipeline completo para: /tmp/tmp3b4qnx1f.mp4\n",
            "Iniciando pipeline completo para: /tmp/tmp3b4qnx1f.mp4\n",
            "1. Extrayendo keypoints con MediaPipe...\n",
            "Procesados 100 frames...\n",
            "Procesados 200 frames...\n",
            "Procesados 300 frames...\n",
            "Procesados 400 frames...\n",
            "Procesados 500 frames...\n",
            "Procesados 600 frames...\n",
            "Procesados 700 frames...\n",
            "Procesados 800 frames...\n",
            "Procesados 900 frames...\n",
            "Procesados 1000 frames...\n",
            "Procesados 1100 frames...\n",
            "Procesados 1200 frames...\n",
            "Procesados 1300 frames...\n",
            "Procesados 1400 frames...\n",
            "Procesados 1500 frames...\n",
            "Procesados 1600 frames...\n",
            "Procesados 1700 frames...\n",
            "Procesados 1800 frames...\n",
            "Keypoints saved to: /content/drive/MyDrive/asistente_coreografico/data/processed/custom_videos/custom_video_keypoints.npy\n",
            "\n",
            "2. Ejecutando análisis básico...\n",
            "Sugerencias (timeline) guardadas en: /content/drive/MyDrive/asistente_coreografico/data/processed/custom_videos/custom_video_suggestions_timeline.csv\n",
            "\n",
            "3. Cargando modelo ...\n",
            "Using CoreographyLSTM structure for weights: best_lstm_model.h5\n",
            "Could not load advanced model: Layer count mismatch when loading weights from file. Model expected 4 layers, found 5 saved layers.\n",
            "Skipping advanced analysis.\n",
            "\n",
            "7. Generando vídeo anotado...\n",
            "Vídeo anotado generado en: /content/drive/MyDrive/asistente_coreografico/reports/custom_video_full_analysis_annotated.mp4\n",
            "\n",
            "8. Guardando resultados combinados...\n",
            "Resumen del análisis guardado en: /content/drive/MyDrive/asistente_coreografico/reports/full_analysis_summary.json\n",
            "\n",
            "Pipeline completo finalizado.\n",
            "\n",
            "=== Resultados Finales ===\n",
            "Sugerencias Combinadas:\n",
            "1. Sugerencia para ch02 (Confianza: 0.40) [Fuente: basic_model]\n",
            "2. Sugerencia para ch03 (Confianza: 0.40) [Fuente: basic_model]\n",
            "\n",
            "Vídeo anotado disponible en: /content/drive/MyDrive/asistente_coreografico/reports/custom_video_full_analysis_annotated.mp4\n",
            "Archivo temporal de vídeo eliminado: /tmp/tmp3b4qnx1f.mp4\n"
          ]
        }
      ],
      "source": [
        "# INTEGRACION FINAL: PIPELINE COMPLETO para un vídeo subido por el usuario\n",
        "# - Sube vídeo del usuario\n",
        "# - Procesa con MediaPipe para keypoints\n",
        "# - Ejecuta análisis básico (features + sugerencias básicas)\n",
        "# - Carga/usa modelo avanzado (LSTM/Transformer entrenado previamente)\n",
        "# - Ejecuta inferencia avanzada (predicción por secuencias)\n",
        "# - Combina sugerencias (básicas + avanzadas)\n",
        "# - Visualiza resultados (anota vídeo)\n",
        "\n",
        "\n",
        "import os, json, math, io, glob, tempfile\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, precision_recall_curve, roc_curve\n",
        "from joblib import load\n",
        "from google.colab import files\n",
        "\n",
        "# --- Variables base (compatibles con celdas 1–11) ---\n",
        "try:\n",
        "    BASE\n",
        "except NameError:\n",
        "    BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "\n",
        "try:\n",
        "    MODEL_TYPE\n",
        "except NameError:\n",
        "    MODEL_TYPE = \"lstm\"  # 'lstm' | 'bilstm' | 'transformer'\n",
        "\n",
        "MODELS_DIR  = f\"{BASE}/models\"\n",
        "REPORTS_DIR = f\"{BASE}/reports\"\n",
        "ART_DIR     = f\"{BASE}/artifacts\"\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
        "\n",
        "# --- Cargar artefactos del modelo básico (si existen) ---\n",
        "try:\n",
        "    imp          = load(f\"{ART_DIR}/imputer.joblib\")\n",
        "    scaler       = load(f\"{ART_DIR}/scaler.joblib\")\n",
        "    model_basic  = load(f\"{ART_DIR}/model_ovr_logreg.joblib\")\n",
        "    feature_cols = pd.read_csv(f\"{ART_DIR}/feature_cols.csv\", header=None)[0].tolist()\n",
        "    label_names  = pd.read_csv(f\"{ART_DIR}/label_names.csv\", header=None)[0].tolist()\n",
        "    print(\"Loaded basic model artifacts.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load basic model artifacts: {e}\")\n",
        "    imp = None; scaler = None; model_basic = None\n",
        "    feature_cols = []\n",
        "    label_names  = [\"label1\", \"label2\", \"label3\"]  # fallback\n",
        "\n",
        "# --- Unificar label_names con clases del modelo avanzado (si existen) ---\n",
        "classes_json = os.path.join(ART_DIR, \"classes.json\")\n",
        "if os.path.exists(classes_json):\n",
        "    try:\n",
        "        label_names_adv = json.load(open(classes_json))\n",
        "        if isinstance(label_names_adv, list) and len(label_names_adv) > 0:\n",
        "            label_names = label_names_adv\n",
        "            print(f\"Using label_names from classes.json ({len(label_names)})\")\n",
        "    except Exception as e:\n",
        "        print(\"WARN: could not read classes.json:\", e)\n",
        "\n",
        "# --- Mapa de sugerencias ---\n",
        "try:\n",
        "    label_to_text\n",
        "except NameError:\n",
        "    label_to_text = {ln: f\"Sugerencia para {ln}\" for ln in label_names}\n",
        "\n",
        "# --- Utilidades ---\n",
        "def ensure_dir(p): os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def sliding_windows(X, win_len=100, hop=20):\n",
        "    \"\"\"X: (T, F) -> (Nw, win_len, F) + índices [(s,e),...]. Padding si T<win_len.\"\"\"\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    T, F = X.shape\n",
        "    if T <= 0:\n",
        "        return np.zeros((0, win_len, F), dtype=np.float32), []\n",
        "    if T < win_len:\n",
        "        pad = np.zeros((win_len - T, F), dtype=np.float32)\n",
        "        Xp = np.vstack([X, pad])\n",
        "        return np.expand_dims(Xp, 0), [(0, min(T, win_len))]\n",
        "    idx, ws = [], []\n",
        "    for s in range(0, T - win_len + 1, hop):\n",
        "        e = s + win_len\n",
        "        ws.append(X[s:e])\n",
        "        idx.append((s, e))\n",
        "    return np.stack(ws, axis=0), idx\n",
        "\n",
        "def plot_topk_bars(scores_vec, labels, k=10, title=\"Top etiquetas\"):\n",
        "    scores = np.array(scores_vec).astype(float)\n",
        "    if scores.ndim > 1: scores = scores.mean(axis=0)\n",
        "    k = min(k, len(labels))\n",
        "    idx = np.argsort(scores)[::-1][:k]\n",
        "    plt.figure(figsize=(8, max(4, 0.45*k)))\n",
        "    plt.barh(range(k), scores[idx])\n",
        "    plt.yticks(range(k), [labels[i] for i in idx]); plt.gca().invert_yaxis()\n",
        "    plt.xlabel(\"Score\"); plt.title(title); plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_window_heatmap(win_scores, labels, win_idx, title=\"Heatmap temporal (ventanas x etiquetas)\"):\n",
        "    if win_scores is None or len(win_scores)==0:\n",
        "        print(\"No hay puntuaciones por ventana para heatmap.\")\n",
        "        return\n",
        "    H = np.asarray(win_scores)\n",
        "    plt.figure(figsize=(min(14, 2 + 0.6*H.shape[0]), 0.5*H.shape[1] + 2))\n",
        "    plt.imshow(H.T, aspect=\"auto\", interpolation=\"nearest\")\n",
        "    plt.yticks(range(len(labels)), labels)\n",
        "    xt = [f\"{s}-{e}\" for (s,e) in win_idx]\n",
        "    plt.xticks(range(len(win_idx)), xt, rotation=45, ha='right')\n",
        "    plt.xlabel(\"Ventanas (frames)\"); plt.ylabel(\"Etiquetas\")\n",
        "    plt.title(title); plt.tight_layout(); plt.show()\n",
        "\n",
        "def overlay_text_on_video(video_path, timeline, out_path, max_lines=4):\n",
        "    \"\"\"Escribe sugerencias activas en cada frame y exporta MP4 anotado.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"No se pudo abrir el vídeo: {video_path}\")\n",
        "        return None\n",
        "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    FPS = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    vw = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), FPS, (W,H))\n",
        "    t = 0\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        active = []\n",
        "        for seg in timeline:\n",
        "            if int(seg[\"start_f\"]) <= t <= int(seg[\"end_f\"]):\n",
        "                active = seg.get(\"sugerencias\", [])\n",
        "        y0 = 30\n",
        "        for s in active[:max_lines]:\n",
        "            cv2.putText(frame, f\"• {s}\", (20, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 4, cv2.LINE_AA)\n",
        "            cv2.putText(frame, f\"• {s}\", (20, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1, cv2.LINE_AA)\n",
        "            y0 += 28\n",
        "        vw.write(frame); t += 1\n",
        "    cap.release(); vw.release()\n",
        "    return out_path\n",
        "\n",
        "def _normalize_combined_suggestions(sug_list, default_conf=0.4, default_source=\"basic_model\"):\n",
        "    \"\"\"Normaliza lista de sugerencias a [{'suggestion','confidence','source'}, ...].\"\"\"\n",
        "    out = []\n",
        "    for s in (sug_list or []):\n",
        "        if isinstance(s, dict):\n",
        "            out.append({\n",
        "                \"suggestion\": str(s.get(\"suggestion\",\"\")),\n",
        "                \"confidence\": float(s.get(\"confidence\", default_conf)),\n",
        "                \"source\": s.get(\"source\", default_source)\n",
        "            })\n",
        "        else:\n",
        "            out.append({\"suggestion\": str(s), \"confidence\": float(default_conf), \"source\": default_source})\n",
        "    return out\n",
        "\n",
        "# --- Entrada / extracción de vídeo ---\n",
        "def upload_video():\n",
        "    \"\"\"Permite al usuario subir un vídeo desde su dispositivo (interfaz de Colab).\"\"\"\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No se subió ningún archivo.\")\n",
        "        return None\n",
        "    video_name = list(uploaded.keys())[0]\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(video_name)[1]) as temp_video_file:\n",
        "        temp_video_file.write(uploaded[video_name])\n",
        "        video_path = temp_video_file.name\n",
        "    print(f\"Vídeo '{video_name}' subido y guardado temporalmente en: {video_path}\")\n",
        "    return video_path\n",
        "\n",
        "def process_video_with_mediapipe(video_path, output_dir):\n",
        "    \"\"\"Procesa vídeo con MediaPipe Pose y guarda keypoints (33*4 por frame).\"\"\"\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose(static_image_mode=False, model_complexity=2, enable_segmentation=False, min_detection_confidence=0.5)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"No se pudo abrir el vídeo para MediaPipe: {video_path}\")\n",
        "        return None, None\n",
        "    all_keypoints = []; frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose.process(rgb_frame)\n",
        "        if results.pose_landmarks:\n",
        "            frame_keypoints = []\n",
        "            for landmark in results.pose_landmarks.landmark:\n",
        "                frame_keypoints.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "            all_keypoints.append(frame_keypoints)\n",
        "        else:\n",
        "            all_keypoints.append([np.nan] * 33 * 4)\n",
        "        frame_count += 1\n",
        "        if frame_count % 100 == 0:\n",
        "            print(f\"Procesados {frame_count} frames...\")\n",
        "    cap.release()\n",
        "    if not all_keypoints:\n",
        "        print(\"No keypoints extracted from the video.\")\n",
        "        return None, None\n",
        "    keypoints_array = np.array(all_keypoints)\n",
        "    output_path = os.path.join(output_dir, \"custom_video_keypoints.npy\")\n",
        "    np.save(output_path, keypoints_array)\n",
        "    print(f\"Keypoints saved to: {output_path}\")\n",
        "    return keypoints_array, output_path\n",
        "\n",
        "# --- Features coreográficos básicos con MediaPipe ---\n",
        "def interpolate_nan_1d(y):\n",
        "    y = y.astype(np.float32); T = len(y); idx = np.arange(T); mask = np.isfinite(y)\n",
        "    if mask.sum() == 0: return y\n",
        "    last = np.nan\n",
        "    for i in range(T):\n",
        "        if not np.isfinite(y[i]) and np.isfinite(last): y[i] = last\n",
        "        else: last = y[i]\n",
        "    last = np.nan\n",
        "    for i in range(T-1,-1,-1):\n",
        "        if not np.isfinite(y[i]) and np.isfinite(last): y[i] = last\n",
        "        else: last = y[i]\n",
        "    mask = np.isfinite(y)\n",
        "    if 2 <= mask.sum() < T: y[~mask] = np.interp(idx[~mask], idx[mask], y[mask])\n",
        "    return y\n",
        "\n",
        "def clean_nan_interpolate(K, min_valid_ratio=0.10):\n",
        "    Kc = K.copy(); T,J,D = Kc.shape; used = []\n",
        "    for j in range(J):\n",
        "        valid = np.isfinite(Kc[:,j,:]).all(axis=1)\n",
        "        if valid.mean() < min_valid_ratio: Kc[:,j,:] = np.nan; continue\n",
        "        for d in range(D): Kc[:,j,d] = interpolate_nan_1d(Kc[:,j,d])\n",
        "        if np.isfinite(Kc[:,j,:]).any(): used.append(j)\n",
        "    if not used:\n",
        "        print(\"No hay articulaciones válidas tras limpieza e interpolación.\")\n",
        "        return Kc, []\n",
        "    return Kc[:,used,:], used\n",
        "\n",
        "def features_coreograficos_mediapipe(K):\n",
        "    \"\"\"Resumen de features robusto para sugerencias básicas.\"\"\"\n",
        "    T,J,D = K.shape\n",
        "    K_2d = K[:,:,:2]\n",
        "\n",
        "    amp_mean = (np.nanmax(K_2d, axis=0) - np.nanmin(K_2d, axis=0)).mean(axis=0)\n",
        "    amp_x = float(amp_mean[0]); amp_y = float(amp_mean[1])\n",
        "\n",
        "    vel = np.linalg.norm(np.diff(K_2d, axis=0), axis=2)\n",
        "    vel_media = float(np.nanmean(vel)) if vel.size else 0.0\n",
        "\n",
        "    MEDIAPIPE_COCO_PAIRS_SIMPLIFIED = [(11,12), (13,14), (15,16), (23,24), (25,26), (27,28)]\n",
        "    pair_vals=[]\n",
        "    for a,b in MEDIAPIPE_COCO_PAIRS_SIMPLIFIED:\n",
        "        if a < J and b < J:\n",
        "            diff = K_2d[:,a,:] - K_2d[:,b,:]     # [T,2]\n",
        "            d = np.linalg.norm(diff, axis=1)     # [T]\n",
        "            pair_vals.append(np.nanmean(d))\n",
        "    sim_raw = float(np.nanmean(pair_vals)) if pair_vals else np.nan\n",
        "\n",
        "    com = (K[:,23,:2] + K[:,24,:2]) / 2 if J >= 25 else K[:,:,:2].mean(axis=1)\n",
        "    nivel_p10 = float(np.nanpercentile(com[:,1], 10))\n",
        "    nivel_p90 = float(np.nanpercentile(com[:,1], 90))\n",
        "    nivel_rango = float(nivel_p10 - nivel_p90)\n",
        "\n",
        "    disp = np.diff(com,axis=0)\n",
        "    dirs = np.arctan2(disp[:,1], disp[:,0] + 1e-9)\n",
        "    cambios = np.abs(np.diff(dirs))\n",
        "    variedad_dir = float(np.nanmean(cambios)) if len(cambios)>0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"amplitud_x\": amp_x, \"amplitud_y\": amp_y, \"velocidad_media\": vel_media,\n",
        "        \"simetria\": sim_raw, \"nivel_rango\": nivel_rango, \"variedad_direcciones\": variedad_dir,\n",
        "        \"frames\": int(T), \"joints\": int(J), \"dims\": int(D)\n",
        "    }\n",
        "\n",
        "def inferir_desde_features_dict(feats: dict):\n",
        "    \"\"\"Inferencia con pipeline básico (imputer + scaler + OVR-LogReg).\"\"\"\n",
        "    if imp is None or scaler is None or model_basic is None or not feature_cols or not label_names:\n",
        "        print(\"Cannot perform basic inference: missing basic model artifacts or labels/features.\")\n",
        "        return [], []\n",
        "    x = np.array([[feats.get(c, np.nan) for c in feature_cols]], dtype=float)\n",
        "    x_imp = imp.transform(x)\n",
        "    x_scaled = scaler.transform(x_imp)\n",
        "    if hasattr(model_basic, 'predict'):\n",
        "        yhat = model_basic.predict(x_scaled)[0]\n",
        "        predicted_labels = [label_names[i] for i, v in enumerate(yhat) if v==1]\n",
        "        sugerencias = [label_to_text.get(lbl, lbl) for lbl in predicted_labels]\n",
        "        return predicted_labels, sugerencias\n",
        "    else:\n",
        "        print(\"Basic model does not have 'predict'.\")\n",
        "        return [], []\n",
        "\n",
        "def run_basic_analysis(video_path, keypoints_array, output_dir, win_sec=5.0, hop_sec=2.5):\n",
        "    \"\"\"Ejecuta análisis básico por ventanas; devuelve timeline y keypoints procesados.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    FPS = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    cap.release()\n",
        "\n",
        "    T = keypoints_array.shape[0]; J = 33; D = 4\n",
        "    K_raw = keypoints_array.reshape(T, J, D)\n",
        "\n",
        "    Kc, used = clean_nan_interpolate(K_raw, min_valid_ratio=0.10)\n",
        "    if not used:\n",
        "        print(\"No hay articulaciones válidas tras limpieza/interpolación.\")\n",
        "        return {\"csv\": None, \"video\": video_path, \"timeline\": [], \"processed_keypoints\": None, \"suggestions\": []}\n",
        "\n",
        "    win = max(1, int(round(win_sec * FPS)))\n",
        "    hop = max(1, int(round(hop_sec * FPS)))\n",
        "    T_clean = len(Kc)\n",
        "\n",
        "    rows = []; timeline = []; suggestions_list = []\n",
        "    for start in range(0, T_clean - 1, hop):\n",
        "        end = min(T_clean - 1, start + win)\n",
        "        if end - start < max(8, int(0.25 * win)): break\n",
        "        Kw = Kc[start:end]\n",
        "        feats = features_coreograficos_mediapipe(Kw)\n",
        "        labels, suggs = inferir_desde_features_dict(feats)\n",
        "\n",
        "        timeline.append({\"start_f\": int(start), \"end_f\": int(end), \"sugerencias\": suggs})\n",
        "        suggestions_list.extend(suggs)\n",
        "\n",
        "        row = {\"start_f\": int(start), \"end_f\": int(end), \"start_s\": start / FPS, \"end_s\": end / FPS}\n",
        "        row.update(feats)\n",
        "        row[\"labels\"] = \"|\".join(labels)\n",
        "        row[\"sugerencias\"] = \"|\".join(suggs)\n",
        "        rows.append(row)\n",
        "\n",
        "    out_csv = os.path.join(output_dir, \"custom_video_suggestions_timeline.csv\")\n",
        "    pd.DataFrame(rows).to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
        "    print(f\"Sugerencias (timeline) guardadas en: {out_csv}\")\n",
        "\n",
        "    return {\n",
        "        \"csv\": out_csv,\n",
        "        \"video\": video_path,\n",
        "        \"timeline\": timeline,\n",
        "        \"processed_keypoints\": Kc,\n",
        "        \"suggestions\": list(set(suggestions_list))\n",
        "    }\n",
        "\n",
        "# --- Modelo avanzado (estructura LSTM Bi) para carga de pesos ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "class CoreographyLSTM:\n",
        "    def __init__(self, input_shape=(100,51), num_labels=8):\n",
        "        inp = layers.Input(shape=input_shape)\n",
        "        x = layers.Masking(mask_value=0.0)(inp)\n",
        "        x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
        "        x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "        x = layers.Dense(128, activation=\"relu\")(x)\n",
        "        out = layers.Dense(num_labels, activation=\"sigmoid\")(x)\n",
        "        self.model = models.Model(inp, out)\n",
        "        self.model.compile(optimizer=optimizers.Adam(1e-3),\n",
        "                           loss=\"binary_crossentropy\",\n",
        "                           metrics=[tf.keras.metrics.AUC(name=\"auc\"),\n",
        "                                    tf.keras.metrics.Precision(name=\"precision\"),\n",
        "                                    tf.keras.metrics.Recall(name=\"recall\")])\n",
        "    def predict_sequence(self, seq_2d):\n",
        "        # seq_2d: (T,F) -> ventanas (Nw, win, F), promedio de scores\n",
        "        Xw, idx = sliding_windows(seq_2d, win_len=self.model.input_shape[1], hop=20)\n",
        "        if len(Xw)==0: return np.zeros((self.model.output_shape[-1],), dtype=float)\n",
        "        probs = self.model.predict(Xw, verbose=0)  # (Nw, C)\n",
        "        return probs.mean(axis=0), probs, idx\n",
        "\n",
        "# --- Pipeline Integrado ---\n",
        "def full_analysis_pipeline(video_path, win_sec=5.0, hop_sec=2.5, conf_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Ejecuta el pipeline completo para un vídeo custom.\n",
        "    \"\"\"\n",
        "    print(f\"Iniciando pipeline completo para: {video_path}\")\n",
        "    output_dir_process = os.path.join(BASE, \"data/processed/custom_videos\")\n",
        "    ensure_dir(output_dir_process)\n",
        "\n",
        "    # Keypoints con MediaPipe\n",
        "    print(\"1. Extrayendo keypoints con MediaPipe...\")\n",
        "    keypoints_array, keypoints_path = process_video_with_mediapipe(video_path, output_dir_process)\n",
        "    if keypoints_array is None:\n",
        "        print(\"Fallo al extraer keypoints. Abortando pipeline.\")\n",
        "        return None\n",
        "\n",
        "    # Análisis básico\n",
        "    print(\"\\n2. Ejecutando análisis básico...\")\n",
        "    basic_results = run_basic_analysis(video_path, keypoints_array, output_dir_process, win_sec=win_sec, hop_sec=hop_sec)\n",
        "    if basic_results.get(\"processed_keypoints\") is None:\n",
        "        print(\"Análisis básico no produjo keypoints procesados. Abortando pipeline.\")\n",
        "        return basic_results\n",
        "\n",
        "    #  Cargar modelo\n",
        "    print(\"\\n3. Cargando modelo ...\")\n",
        "    # Prioridad: best_advanced_model.h5 → fallback best_{MODEL_TYPE}_model.h5\n",
        "    advanced_model_paths = [\n",
        "        os.path.join(MODELS_DIR, \"best_advanced_model.h5\"),\n",
        "        os.path.join(MODELS_DIR, f\"best_{MODEL_TYPE}_model.h5\")\n",
        "    ]\n",
        "    advanced_model_path = next((p for p in advanced_model_paths if os.path.exists(p)), None)\n",
        "    advanced_model = None\n",
        "\n",
        "    T_processed, J_processed, D_processed = basic_results[\"processed_keypoints\"].shape\n",
        "\n",
        "    # Obtener FPS para fijar tamaño de ventana temporal\n",
        "    cap_tmp = cv2.VideoCapture(video_path)\n",
        "    fps_tmp = cap_tmp.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    cap_tmp.release()\n",
        "    assumed_input_shape = (int(win_sec * fps_tmp), J_processed * D_processed)\n",
        "    assumed_num_labels = len(label_names)\n",
        "\n",
        "    try:\n",
        "        if advanced_model_path is not None:\n",
        "            print(f\"Using CoreographyLSTM structure for weights: {os.path.basename(advanced_model_path)}\")\n",
        "            advanced_model_keras = CoreographyLSTM(input_shape=assumed_input_shape, num_labels=assumed_num_labels).model\n",
        "            advanced_model_keras.load_weights(advanced_model_path)\n",
        "            advanced_model = advanced_model_keras\n",
        "            print(\"Advanced model loaded.\")\n",
        "        else:\n",
        "            print(\"No advanced model weights found. Skipping advanced analysis.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load advanced model: {e}\")\n",
        "        print(\"Skipping advanced analysis.\")\n",
        "        advanced_model = None\n",
        "\n",
        "    #  Preparar estructura de resultados combinados\n",
        "    combined_results = {\n",
        "        \"basic_analysis\": basic_results,\n",
        "        \"advanced_prediction_mean\": None,\n",
        "        \"window_scores\": None,\n",
        "        \"window_index\": None,\n",
        "        \"combined_suggestions\": basic_results.get(\"suggestions\", []),  # strings al principio\n",
        "        \"trained_now\": False\n",
        "    }\n",
        "    combined_results[\"combined_suggestions\"] = _normalize_combined_suggestions(\n",
        "        combined_results.get(\"combined_suggestions\", [])\n",
        "    )\n",
        "\n",
        "    # Inferencia sobre el  modelo\n",
        "    if advanced_model:\n",
        "        print(\"\\n4. Ejecutando inferencia avanzada...\")\n",
        "        Kc = basic_results[\"processed_keypoints\"]\n",
        "        if Kc.ndim == 3:\n",
        "            T_kc, J_kc, D_kc = Kc.shape\n",
        "            X_fit_advanced = Kc.reshape(T_kc, J_kc * D_kc)\n",
        "        else:\n",
        "            X_fit_advanced = Kc\n",
        "\n",
        "        # Alinear dimensión de features\n",
        "        model_input_features = advanced_model.input_shape[-1]\n",
        "        if X_fit_advanced.shape[1] < model_input_features:\n",
        "            pad = np.zeros((X_fit_advanced.shape[0], model_input_features - X_fit_advanced.shape[1]), dtype=np.float32)\n",
        "            X_fit_aligned = np.hstack([X_fit_advanced, pad])\n",
        "        elif X_fit_advanced.shape[1] > model_input_features:\n",
        "            X_fit_aligned = X_fit_advanced[:, :model_input_features]\n",
        "        else:\n",
        "            X_fit_aligned = X_fit_advanced\n",
        "\n",
        "        # Ventanas deslizantes\n",
        "        win_len_model = advanced_model.input_shape[1]\n",
        "        Xw, idx = sliding_windows(X_fit_aligned, win_len=win_len_model, hop=max(5, win_len_model//5))\n",
        "\n",
        "        if len(Xw) > 0:\n",
        "            yw = advanced_model.predict(Xw, verbose=0)  # (Nseq, C)\n",
        "            y_vec = yw.mean(axis=0)                     # (C,)\n",
        "            combined_results[\"advanced_prediction_mean\"] = y_vec.tolist()\n",
        "            combined_results[\"window_scores\"] = yw.tolist()\n",
        "            combined_results[\"window_index\"] = idx\n",
        "\n",
        "            # Fusionar sugerencias\n",
        "            print(\"\\n5. Combinando sugerencias...\")\n",
        "            # Arrancamos con las ya normalizadas (básicas)\n",
        "            sug = list(combined_results[\"combined_suggestions\"])\n",
        "            # Añadir avanzadas por umbral global\n",
        "            for lbl_idx, s in enumerate(y_vec):\n",
        "                if s >= conf_threshold and lbl_idx < len(label_names):\n",
        "                    sug_text = label_to_text.get(label_names[lbl_idx], label_names[lbl_idx])\n",
        "                    if not any(d.get(\"suggestion\") == sug_text for d in sug):\n",
        "                        sug.append({\"suggestion\": sug_text, \"confidence\": float(s), \"source\": \"advanced_model\"})\n",
        "            # Orden final\n",
        "            combined_results[\"combined_suggestions\"] = sorted(sug, key=lambda x: x.get(\"confidence\", 0.0), reverse=True)\n",
        "\n",
        "            # Visualizaciones (Top etiquetas + Heatmap)\n",
        "            print(\"\\n6. Visualizaciones...\")\n",
        "            plot_topk_bars(y_vec, label_names, k=min(12, len(label_names)), title=\"Top etiquetas (modelo avanzado)\")\n",
        "            plot_window_heatmap(yw, label_names, idx, title=\"Heatmap temporal (modelo avanzado)\")\n",
        "        else:\n",
        "            print(\"No se pudieron construir ventanas para el modelo avanzado. Saltando inferencia avanzada.\")\n",
        "\n",
        "    #  Generar vídeo anotado\n",
        "    print(\"\\n7. Generando vídeo anotado...\")\n",
        "    timeline_combined = []\n",
        "    basic_timeline = basic_results.get(\"timeline\", [])\n",
        "    if basic_timeline:\n",
        "        # Usa top-K globales por ventana (simple)\n",
        "        top_k_texts = [d[\"suggestion\"] for d in combined_results[\"combined_suggestions\"][:4]]\n",
        "        for basic_win in basic_timeline:\n",
        "            start_f = int(basic_win[\"start_f\"]); end_f = int(basic_win[\"end_f\"])\n",
        "            timeline_combined.append({\"start_f\": start_f, \"end_f\": end_f, \"sugerencias\": list(top_k_texts)})\n",
        "    elif combined_results.get(\"window_index\"):\n",
        "        # Si no hay línea base, usa los índices del modelo avanzado\n",
        "        idx = combined_results[\"window_index\"]\n",
        "        win_scores = combined_results.get(\"window_scores\")\n",
        "        for k, (s, e) in enumerate(idx):\n",
        "            s = int(s); e = int(e)\n",
        "            active_suggs_text = []\n",
        "            if win_scores and len(win_scores) > k:\n",
        "                # Umbral por ventana (simple)\n",
        "                arr = np.array(win_scores[k], dtype=np.float32)  # (C,)\n",
        "                j_idx = np.where(arr >= conf_threshold)[0]\n",
        "                active_suggs_text = [label_to_text.get(label_names[j], label_names[j]) for j in j_idx if j < len(label_names)]\n",
        "            if not active_suggs_text:\n",
        "                active_suggs_text = [d[\"suggestion\"] for d in combined_results[\"combined_suggestions\"][:4]]\n",
        "            timeline_combined.append({\"start_f\": s, \"end_f\": e, \"sugerencias\": active_suggs_text[:4]})\n",
        "    else:\n",
        "        print(\"No hay timeline de básico ni avanzado. Saltando anotación de vídeo.\")\n",
        "        combined_results[\"annotated_video\"] = None\n",
        "        print(\"\\nPipeline completo finalizado (sin vídeo anotado).\")\n",
        "        return combined_results\n",
        "\n",
        "    out_mp4 = os.path.join(REPORTS_DIR, f\"custom_video_full_analysis_annotated.mp4\")\n",
        "    annotated_video_path = overlay_text_on_video(video_path, timeline_combined, out_mp4)\n",
        "    combined_results[\"annotated_video\"] = annotated_video_path\n",
        "    print(f\"Vídeo anotado generado en: {annotated_video_path}\")\n",
        "\n",
        "    #  Guardar resumen\n",
        "    print(\"\\n8. Guardando resultados combinados...\")\n",
        "    summary_path = os.path.join(REPORTS_DIR, \"full_analysis_summary.json\")\n",
        "    summary_data = {\n",
        "        \"video_path\": video_path,\n",
        "        \"basic_analysis_csv\": basic_results.get(\"csv\"),\n",
        "        \"annotated_video\": combined_results.get(\"annotated_video\"),\n",
        "        \"combined_suggestions\": combined_results.get(\"combined_suggestions\"),\n",
        "        \"advanced_prediction_mean\": combined_results.get(\"advanced_prediction_mean\"),\n",
        "        \"trained_now\": combined_results.get(\"trained_now\", False)\n",
        "    }\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"Resumen del análisis guardado en: {summary_path}\")\n",
        "\n",
        "    print(\"\\nPipeline completo finalizado.\")\n",
        "    return combined_results\n",
        "\n",
        "# --- Ejecutar el pipeline completo ---\n",
        "print(\"Por favor, sube un vídeo custom para iniciar el pipeline.\")\n",
        "try:\n",
        "    custom_video_path = upload_video()\n",
        "    if custom_video_path and os.path.exists(custom_video_path):\n",
        "        print(f\"\\nEjecutando pipeline completo para: {custom_video_path}\")\n",
        "        final_results = full_analysis_pipeline(custom_video_path)\n",
        "\n",
        "        print(\"\\n=== Resultados Finales ===\")\n",
        "        if final_results:\n",
        "            print(\"Sugerencias Combinadas:\")\n",
        "            if final_results.get(\"combined_suggestions\"):\n",
        "                for i, sug in enumerate(final_results[\"combined_suggestions\"], 1):\n",
        "                    print(f\"{i}. {sug.get('suggestion', 'N/A')} (Confianza: {sug.get('confidence', np.nan):.2f}) [Fuente: {sug.get('source', 'N/A')}]\")\n",
        "            else:\n",
        "                print(\"No combined suggestions generated.\")\n",
        "\n",
        "            if final_results.get(\"annotated_video\"):\n",
        "                print(\"\\nVídeo anotado disponible en:\", final_results[\"annotated_video\"])\n",
        "            else:\n",
        "                print(\"\\nVídeo anotado no generado.\")\n",
        "\n",
        "            # Limpiar el archivo de vídeo temporal\n",
        "            if custom_video_path.startswith(tempfile.gettempdir()):\n",
        "                os.remove(custom_video_path)\n",
        "                print(f\"Archivo temporal de vídeo eliminado: {custom_video_path}\")\n",
        "        else:\n",
        "            print(\"El pipeline completo no se ejecutó correctamente.\")\n",
        "    else:\n",
        "        print(\"No se subió ningún vídeo o el archivo no se encontró.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrió un error durante la ejecución del pipeline completo: {e}\")\n",
        "    if 'custom_video_path' in locals() and custom_video_path and os.path.exists(custom_video_path) and custom_video_path.startswith(tempfile.gettempdir()):\n",
        "        os.remove(custom_video_path)\n",
        "        print(f\"Archivo temporal de vídeo eliminado por error: {custom_video_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "WhRiYSaYVpDI",
        "outputId": "0dd37b74-866f-4492-f0bc-b9a070467dac"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-392399011.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSkPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# === GUARDAR MODELO EN /models ===\n",
        "\n",
        "import os\n",
        "import json\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.pipeline import Pipeline as SkPipeline\n",
        "\n",
        "# Rutas base\n",
        "BASE        = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "ART         = f\"{BASE}/artifacts\"\n",
        "MODELS_DIR  = f\"{BASE}/models\"\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# Cargar objetos desde memoria o desde /artifacts\n",
        "\n",
        "try:\n",
        "    imp      # SimpleImputer ya en memoria\n",
        "    scaler   # RobustScaler ya en memoria\n",
        "    clf      # OneVsRest(LogReg) ya en memoria\n",
        "except NameError:\n",
        "    # Si no están en el workspace actual, cargamos desde disco\n",
        "    imp_path    = f\"{ART}/imputer.joblib\"\n",
        "    scaler_path = f\"{ART}/scaler.joblib\"\n",
        "    clf_path    = f\"{ART}/model_ovr_logreg.joblib\"\n",
        "\n",
        "    assert os.path.exists(imp_path),    f\"Falta: {imp_path}\"\n",
        "    assert os.path.exists(scaler_path), f\"Falta: {scaler_path}\"\n",
        "    assert os.path.exists(clf_path),    f\"Falta: {clf_path}\"\n",
        "\n",
        "    imp    = joblib.load(imp_path)\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    clf    = joblib.load(clf_path)\n",
        "\n",
        "\n",
        "# Cargar columnas de features y nombres de etiquetas\n",
        "\n",
        "try:\n",
        "    feature_cols\n",
        "except NameError:\n",
        "    feat_path = f\"{ART}/feature_cols.csv\"\n",
        "    assert os.path.exists(feat_path), f\"Falta: {feat_path}\"\n",
        "    feature_cols = pd.read_csv(feat_path, header=None)[0].tolist()\n",
        "\n",
        "try:\n",
        "    label_names\n",
        "except NameError:\n",
        "    labels_path = f\"{ART}/label_names.csv\"\n",
        "    assert os.path.exists(labels_path), f\"Falta: {labels_path}\"\n",
        "    label_names = pd.read_csv(labels_path, header=None)[0].tolist()\n",
        "\n",
        "\n",
        "# Construir Pipeline y guardar\n",
        "\n",
        "pipe = SkPipeline([\n",
        "    (\"imputer\", imp),     # SimpleImputer ya ajustado\n",
        "    (\"scaler\",  scaler),  # RobustScaler ya ajustado\n",
        "    (\"clf\",     clf)      # OneVsRest(LogisticRegression) ya ajustado\n",
        "])\n",
        "\n",
        "pipe_path = f\"{MODELS_DIR}/pipeline_ovr_logreg.joblib\"\n",
        "joblib.dump(pipe, pipe_path)\n",
        "print(f\" Pipeline guardado en: {pipe_path}\")\n",
        "\n",
        "# Metadatos del pipeline\n",
        "meta = {\n",
        "    \"sklearn_version\": sklearn.__version__,\n",
        "    \"model_path\": pipe_path,\n",
        "    \"onnx_path\": None,  # Export no realizado en esta celda\n",
        "    \"feature_cols\": list(feature_cols),\n",
        "    \"label_names\": list(label_names),\n",
        "    \"notes\": \"Pipeline con SimpleImputer + RobustScaler + OneVsRest(LogReg) entrenado.\"\n",
        "}\n",
        "meta_path = f\"{MODELS_DIR}/pipeline_metadata.json\"\n",
        "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
        "print(f\" Metadatos guardados en: {meta_path}\")\n",
        "\n",
        "\n",
        "#  Ejemplo de carga rápida y prueba de inferencia (opcional)\n",
        "\n",
        "# Si X_raw no existe en el workspace, intenta cargar el CSV de features;\n",
        "# si no existe o falta alguna columna, no ejecuta la demo.\n",
        "try:\n",
        "    X_raw\n",
        "except NameError:\n",
        "    X_raw = None\n",
        "\n",
        "CSV = f\"{BASE}/data/processed/aistpp/features_coreograficos.csv\"\n",
        "if X_raw is None and os.path.exists(CSV):\n",
        "    try:\n",
        "        df = pd.read_csv(CSV)\n",
        "        missing = [c for c in feature_cols if c not in df.columns]\n",
        "        if missing:\n",
        "            print(f\"No se puede crear X_raw: faltan {len(missing)} columnas en el CSV (ej: {missing[:5]})\")\n",
        "        else:\n",
        "            X_raw = df[feature_cols].copy()\n",
        "    except Exception as e:\n",
        "        print(f\" No se pudo cargar {CSV}: {e}\")\n",
        "\n",
        "# Carga el pipeline y, si hay X_raw, prueba la inferencia\n",
        "pipe = joblib.load(pipe_path)\n",
        "\n",
        "if X_raw is not None:\n",
        "    try:\n",
        "        y_pred  = pipe.predict(X_raw)        # 0/1 por etiqueta\n",
        "        y_proba = pipe.predict_proba(X_raw)  # (n_samples, n_classes)\n",
        "        print(\"Pred shape:\", y_pred.shape, \"| Proba shape:\", y_proba.shape)\n",
        "    except Exception as e:\n",
        "        print(f\" Error en inferencia de ejemplo: {e}\")\n",
        "else:\n",
        "    print(\" Ejemplo de inferencia omitido (no hay X_raw). \"\n",
        "          \"Carga un DataFrame y selecciona feature_cols para probar.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xvriZVvuMd0"
      },
      "source": [
        "#construccion final del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mutGsVYxuQp8",
        "outputId": "18cc00e7-81f4-4ad1-bae8-f7d6590fb923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE: /content/drive/MyDrive/asistente_coreografico\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#  Configuración básica y rutas\n",
        "\n",
        "import os, glob, json, math, random\n",
        "import numpy as np\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "ART  = f\"{BASE}/artifacts\"\n",
        "MOD  = f\"{BASE}/models\"\n",
        "os.makedirs(ART, exist_ok=True)\n",
        "os.makedirs(MOD, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "print(\"BASE:\", BASE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seTcnfOTuV0_",
        "outputId": "503291c4-ebe9-4db8-d4d7-11b9b7201420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Usando SEQ_LEN=30, HOP=15\n",
            "Cargando datos de AIST++...\n",
            "[INFO] classes.json cargado (21 clases).\n",
            "  - procesados 200/1408\n",
            "  - procesados 400/1408\n",
            "  - procesados 600/1408\n",
            "  - procesados 800/1408\n",
            "  - procesados 1000/1408\n",
            "  - procesados 1200/1408\n",
            "  - procesados 1400/1408\n",
            "[DATA] OK=1408 | Bad=0 | Total=1408 | t=213.8s\n",
            "[DATA] Clips: 1408 | J=17 | D=3\n",
            "[SEQ] X=(72887, 30, 51) y=(72887, 21) (seq_len=30, hop=15)\n",
            "[SEQ] Ventanas: N=72887, T=30, feat=51  (esperado J*D=51)\n",
            "[OK] Guardado: /content/drive/MyDrive/asistente_coreografico/artifacts/kp_windows.npz\n",
            "     kp: (72887, 30, 17, 3) | y: (72887,) | classes: 21 | mask: (72887, 30, 17)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Construir ART/kp_windows.npz desde AIST++\n",
        "# Usa tus funciones: load_aist_data() y prepare_sequences()\n",
        "# Salida: artifacts/kp_windows.npz con:\n",
        "#   - 'kp'      -> [N, T, K, D]  (D=2 ó 3, según dataset)\n",
        "#   - 'y'       -> [N] enteros (clase por ventana)\n",
        "#   - 'classes' -> lista de nombres de clase\n",
        "#   - 'mask'    -> [N, T, K] (opcional, 1 si KP finito en todos los dims)\n",
        "\n",
        "import os, json, numpy as np\n",
        "\n",
        "# --- Rutas ---\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "ART  = f\"{BASE}/artifacts\"\n",
        "os.makedirs(ART, exist_ok=True)\n",
        "\n",
        "# --- Chequeo de dependencias (deben existir en celdas previas) ---\n",
        "for fn in (\"load_aist_data\", \"prepare_sequences\"):\n",
        "    assert fn in globals(), f\"❌ Falta la función '{fn}'. Ejecuta las celdas donde se definió.\"\n",
        "\n",
        "# --- Parámetros (usa los mismos entrenados) ---\n",
        "SEQ_LEN = globals().get(\"SEQ_LEN\", 30)\n",
        "HOP     = globals().get(\"HOP\", 15)\n",
        "\n",
        "print(f\"[INFO] Usando SEQ_LEN={SEQ_LEN}, HOP={HOP}\")\n",
        "\n",
        "# Cargar clips crudos (lista de arrays (T,J,D)) + etiquetas y nombres\n",
        "kp_list, y_list, label_names = load_aist_data()\n",
        "assert len(kp_list) > 0, \"❌ No hay clips en AIST++.\"\n",
        "J, D = kp_list[0].shape[1], kp_list[0].shape[2]\n",
        "print(f\"[DATA] Clips: {len(kp_list)} | J={J} | D={D}\")\n",
        "\n",
        "# Crear ventanas (devuelve X:[N,T,J*D] aplanado y y:[N,C])\n",
        "X_flat, y_oh = prepare_sequences(kp_list, y_list, seq_len=SEQ_LEN, hop=HOP)\n",
        "N, T = X_flat.shape[0], X_flat.shape[1]\n",
        "print(f\"[SEQ] Ventanas: N={N}, T={T}, feat={X_flat.shape[2]}  (esperado J*D={J*D})\")\n",
        "assert X_flat.shape[2] == J * D, \"❌ El número de features no coincide con J*D. Revisa SEQ_LEN/HOP o los datos.\"\n",
        "\n",
        "# Re-dar forma: [N, T, J*D] -> [N, T, J, D]\n",
        "kp_windows = X_flat.reshape(N, T, J, D).astype(np.float32)\n",
        "\n",
        "# Etiquetas enteras por ventana y máscara opcional\n",
        "y_int = np.argmax(y_oh, axis=1).astype(np.int64)  # [N]\n",
        "mask  = np.isfinite(kp_windows).all(axis=-1).astype(np.uint8)  # [N,T,K]\n",
        "\n",
        "# Guardar .npz con el formato esperado por tu loader\n",
        "out_npz = f\"{ART}/kp_windows.npz\"\n",
        "np.savez_compressed(\n",
        "    out_npz,\n",
        "    kp=kp_windows,                  # [N,T,K,D]\n",
        "    y=y_int,                        # [N]\n",
        "    classes=np.array(label_names, dtype=object),\n",
        "    mask=mask                       # [N,T,K] (opcional)\n",
        ")\n",
        "print(f\"[OK] Guardado: {out_npz}\")\n",
        "print(f\"     kp: {kp_windows.shape} | y: {y_int.shape} | classes: {len(label_names)} | mask: {mask.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR9cXV3ouYns",
        "outputId": "d4eee4f9-f5ac-4e55-cf43-db582c9f5bde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Normalización completada\n",
            "kp_norm: (72887, 30, 17, 2) | mask_kp: (72887, 30, 17) | D(norm)= 2\n",
            "[OK] Guardado: /content/drive/MyDrive/asistente_coreografico/artifacts/kp_windows_norm.npz\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Normalización de keypoints por ventana\n",
        "#  - Carga kp si no existe en memoria\n",
        "#  - Devuelve kp_norm [N,T,K,2] y mask_kp [N,T,K]\n",
        "#  - Guarda  ART/kp_windows_norm.npz\n",
        "\n",
        "import os, json\n",
        "import numpy as np\n",
        "\n",
        "# --- Rutas por defecto si no existen en el entorno ---\n",
        "try:\n",
        "    ART\n",
        "except NameError:\n",
        "    BASE = \"/content/drive/MyDrive/asistente_coreografico\"\n",
        "    ART  = f\"{BASE}/artifacts\"\n",
        "os.makedirs(ART, exist_ok=True)\n",
        "\n",
        "# --- Loader mínimo, compatible con tus formatos previos ---\n",
        "def _try_load_keypoints(art_dir):\n",
        "    npz_path = f\"{art_dir}/kp_windows.npz\"\n",
        "    if os.path.exists(npz_path):\n",
        "        data = np.load(npz_path, allow_pickle=True)\n",
        "        kp = data[\"kp\"]                         # [N,T,K,2/3]\n",
        "        y  = data[\"y\"] if \"y\" in data else None\n",
        "        mask = data[\"mask\"] if \"mask\" in data else None\n",
        "        classes = data[\"classes\"].tolist() if \"classes\" in data else None\n",
        "        return kp, y, mask, classes\n",
        "    npy_path = f\"{art_dir}/kp_windows.npy\"\n",
        "    if os.path.exists(npy_path):\n",
        "        kp = np.load(npy_path, allow_pickle=True)\n",
        "        y_path = f\"{art_dir}/y_labels.npy\"\n",
        "        y = np.load(y_path) if os.path.exists(y_path) else None\n",
        "        classes_path = f\"{art_dir}/classes.json\"\n",
        "        classes = json.load(open(classes_path)) if os.path.exists(classes_path) else None\n",
        "        return kp, y, None, classes\n",
        "    raise FileNotFoundError(\"No encuentro ART/kp_windows.(npz|npy). Genera primero las ventanas.\")\n",
        "\n",
        "# --- Si kp no está en memoria, lo cargamos ---\n",
        "try:\n",
        "    kp  # variable ya existente\n",
        "except NameError:\n",
        "    kp, y, mask_file, classes = _try_load_keypoints(ART)\n",
        "else:\n",
        "    # si ya existía, ponemos None para no reusar máscara externa accidentalmente\n",
        "    mask_file = None\n",
        "    classes = None\n",
        "    y = None\n",
        "\n",
        "# --- Normalización: centra y escala robusto por ventana ---\n",
        "# Índices opcionales (si los supieras); aquí usamos centrado/escala robustos\n",
        "IDX_PELVIS = None\n",
        "IDX_L_SHO  = None\n",
        "IDX_L_HIP  = None\n",
        "\n",
        "def compute_mask_and_xy(kp_arr):\n",
        "    # kp_arr: [N,T,K,2] o [N,T,K,3]\n",
        "    if kp_arr.shape[-1] == 3:\n",
        "        xy   = kp_arr[..., :2]\n",
        "        conf = kp_arr[..., 2]\n",
        "        mask = (conf > 0.05).astype(np.float32)\n",
        "    else:\n",
        "        xy   = kp_arr\n",
        "        mask = (~np.isnan(kp_arr[..., 0])).astype(np.float32)\n",
        "    return xy.astype(np.float32), mask\n",
        "\n",
        "def robust_center_scale(xy, mask):\n",
        "    # xy: [N,T,K,2], mask: [N,T,K]\n",
        "    N, T, K, _ = xy.shape\n",
        "    out  = xy.copy()\n",
        "    m_out = mask.copy()\n",
        "    for i in range(N):\n",
        "        for t in range(T):\n",
        "            valid = m_out[i, t] > 0.5\n",
        "            if not np.any(valid):\n",
        "                continue\n",
        "            pts = out[i, t, valid, :]\n",
        "            center = np.median(pts, axis=0, keepdims=True)     # [1,2]\n",
        "            out[i, t, :, :] -= center\n",
        "            d = np.linalg.norm(pts - center, axis=1)\n",
        "            scale = np.percentile(d, 90) if d.size > 0 else 1.0\n",
        "            if scale < 1e-3:\n",
        "                scale = 1.0\n",
        "            out[i, t, :, :] /= scale\n",
        "    return out, m_out\n",
        "\n",
        "#  Extrae xy y máscara inferida\n",
        "xy, mask_kp = compute_mask_and_xy(kp)\n",
        "\n",
        "#  Si el archivo traía 'mask', se fuciona (AND lógico)\n",
        "if 'mask_file' in locals() and mask_file is not None:\n",
        "    mask_kp = (mask_kp * (mask_file > 0.5).astype(np.float32))\n",
        "\n",
        "# Normaliza\n",
        "kp_norm, mask_kp = robust_center_scale(xy, mask_kp)\n",
        "\n",
        "print(\"[OK] Normalización completada\")\n",
        "print(\"kp_norm:\", kp_norm.shape, \"| mask_kp:\", mask_kp.shape, \"| D(norm)=\", kp_norm.shape[-1])\n",
        "\n",
        "# Guardar normalizados para etapas siguientes ---\n",
        "save_norm = True\n",
        "if save_norm:\n",
        "    out = {\"kp\": kp_norm, \"mask\": mask_kp}\n",
        "    if y is not None:\n",
        "        out[\"y\"] = y\n",
        "    if classes is not None:\n",
        "        out[\"classes\"] = np.array(classes, dtype=object)\n",
        "    out_path = f\"{ART}/kp_windows_norm.npz\"\n",
        "    np.savez_compressed(out_path, **out)\n",
        "    print(f\"[OK] Guardado: {out_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVLUCUHIubcg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2372b372-a373-441b-8f4c-d6c6b399e52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Teacher probas generadas desde DF alineado con imputer: (72887, 3)\n",
            "Clases (C): 3\n"
          ]
        }
      ],
      "source": [
        "def _align_features_df_any(df_like, cols):\n",
        "    \"\"\"DataFrame con columnas EXACTAS en 'cols' (orden), faltantes -> NaN.\"\"\"\n",
        "    if isinstance(df_like, pd.DataFrame):\n",
        "        for c in cols:\n",
        "            if c not in df_like.columns:\n",
        "                df_like[c] = np.nan\n",
        "        return df_like[cols]\n",
        "    elif isinstance(df_like, (dict, pd.Series)):\n",
        "        return pd.DataFrame([{c: df_like.get(c, np.nan) for c in cols}])[cols]\n",
        "    elif isinstance(df_like, np.ndarray):\n",
        "        # si viene ndarray con el nº correcto de columnas, lo envolvemos\n",
        "        if df_like.ndim != 2:\n",
        "            raise ValueError(f\"Esperaba 2D array, got shape {df_like.shape}\")\n",
        "        if len(cols) != df_like.shape[1]:\n",
        "            raise ValueError(f\"Las dimensiones no coinciden: X.shape[1]={df_like.shape[1]} vs len(feature_cols)={len(cols)}\")\n",
        "        return pd.DataFrame(df_like, columns=cols)\n",
        "    else:\n",
        "        raise ValueError(\"Formato no soportado para df_like\")\n",
        "\n",
        "def try_load_teacher(ART, y, C, kp_norm=None, feature_cols=None):\n",
        "    \"\"\"\n",
        "    Prioridad:\n",
        "      1) ART/teacher_probas.npy\n",
        "      2) Si hay imp+scaler+clf y features X_*.npy -> predict_proba\n",
        "      3) Si NO hay X_*.npy: construir un DF con columnas=feature_cols y NaN (imputer rellena);\n",
        "         opcionalmente añadir stats simples desde kp_norm si existe.\n",
        "      4) Fallback final: label smoothing desde y (single-label o multi-label).\n",
        "    \"\"\"\n",
        "    if feature_cols is None or len(feature_cols) == 0:\n",
        "        raise RuntimeError(\"feature_cols no está disponible; carga ART/feature_cols.csv antes.\")\n",
        "\n",
        "    p_teacher_path = f\"{ART}/teacher_probas.npy\"\n",
        "    if os.path.exists(p_teacher_path):\n",
        "        P = np.load(p_teacher_path)\n",
        "        print(\"[OK] Cargadas teacher_probas.npy:\", P.shape)\n",
        "        return P\n",
        "\n",
        "    imp_path, scaler_path, clf_path = f\"{ART}/imputer.joblib\", f\"{ART}/scaler.joblib\", f\"{ART}/model_ovr_logreg.joblib\"\n",
        "    has_pipeline = all(os.path.exists(p) for p in [imp_path, scaler_path, clf_path])\n",
        "\n",
        "    if has_pipeline:\n",
        "        imp    = joblib.load(imp_path)\n",
        "        scaler = joblib.load(scaler_path)\n",
        "        clf    = joblib.load(clf_path)\n",
        "\n",
        "        # 2a) ¿Existen features precomputadas del profesor?\n",
        "        for c in (f\"{ART}/X_fusion.npy\", f\"{ART}/X_feats.npy\", f\"{ART}/features.npy\"):\n",
        "            if os.path.exists(c):\n",
        "                X_raw = np.load(c)\n",
        "                if X_raw.shape[1] != len(feature_cols):\n",
        "                    print(f\"[WARN] {os.path.basename(c)} tiene {X_raw.shape[1]} cols y el pipeline espera {len(feature_cols)}. Ignoro y genero alineado.\")\n",
        "                    break\n",
        "                X_df = _align_features_df_any(X_raw, feature_cols)\n",
        "                Ximp = imp.transform(X_df); Xs = scaler.transform(Ximp)\n",
        "                P = clf.predict_proba(Xs).astype(np.float32)\n",
        "                np.save(p_teacher_path, P)\n",
        "                print(\"[OK] Teacher probas generadas desde features guardadas:\", P.shape)\n",
        "                return P\n",
        "        # 2b) No hay X_*.npy válidas: construir DF vacío alineado y (opcional) inyectar stats simples\n",
        "        N_guess = None\n",
        "        feats_dict = {col: np.nan for col in feature_cols}  # plantilla por fila\n",
        "        rows = []\n",
        "\n",
        "        if kp_norm is not None:\n",
        "            # kp_norm: [N,T,K,2] -> N filas\n",
        "            N_guess = kp_norm.shape[0]\n",
        "            # estadísticas simples opcionales (si te suenan nombres en feature_cols, añade aquí los cálculos)\n",
        "            # p.ej., si existe una columna llamada \"mean_x\", \"mean_y\", etc., podrías mapear algo:\n",
        "            # (dejamos NaN por defecto para que el imputer rellene)\n",
        "            for i in range(N_guess):\n",
        "                rows.append(feats_dict.copy())\n",
        "        else:\n",
        "            # si no tenemos kp_norm, al menos intenta deducir N de y\n",
        "            if y is not None:\n",
        "                N_guess = len(y)\n",
        "                rows = [feats_dict.copy() for _ in range(N_guess)]\n",
        "            else:\n",
        "                raise RuntimeError(\"No hay features del profesor ni kp_norm ni y para construir P_teacher.\")\n",
        "\n",
        "        X_df = pd.DataFrame(rows, columns=feature_cols)\n",
        "        Ximp = imp.transform(X_df); Xs = scaler.transform(Ximp)\n",
        "        P = clf.predict_proba(Xs).astype(np.float32)\n",
        "        np.save(p_teacher_path, P)\n",
        "        print(\"[OK] Teacher probas generadas desde DF alineado con imputer:\", P.shape)\n",
        "        return P\n",
        "\n",
        "    # 3) Fallback final: label smoothing a partir de y\n",
        "    if y is None:\n",
        "        raise RuntimeError(\"No hay teacher_probas, ni pipeline clásico, ni y para fallback.\")\n",
        "    y_arr = np.asarray(y)\n",
        "    if y_arr.ndim == 1:  # single-label (clase entera por muestra)\n",
        "        C_fbk = int(C) if C is not None else int(np.max(y_arr) + 1)\n",
        "        eps = 0.1\n",
        "        P = np.full((len(y_arr), C_fbk), eps/(C_fbk-1), dtype=np.float32)\n",
        "        P[np.arange(len(y_arr)), y_arr.astype(int)] = 1.0 - eps\n",
        "    else:  # multilabel (N,C)\n",
        "        eps = 0.1\n",
        "        P = (1.0 - eps) * y_arr.astype(np.float32) + eps * (1.0 - y_arr.astype(np.float32)) / max(1, y_arr.shape[1]-1)\n",
        "    print(\"[OK] Teacher probas fallback (label smoothing):\", P.shape)\n",
        "    return P\n",
        "\n",
        "# ==== USO ====\n",
        "# Requiere: ART, feature_cols, y (opcional), C (opcional), kp_norm (opcional)\n",
        "P_teacher = try_load_teacher(ART, y, C, kp_norm=kp_norm, feature_cols=feature_cols)\n",
        "\n",
        "# Alinear N con kp_norm si difiere\n",
        "N_pt, N_kp = P_teacher.shape[0], kp_norm.shape[0]\n",
        "if N_pt != N_kp:\n",
        "    N_min = min(N_pt, N_kp)\n",
        "    print(f\"[WARN] N mismatch entre teacher ({N_pt}) y kp_norm ({N_kp}); recorto a {N_min}.\")\n",
        "    P_teacher = P_teacher[:N_min]\n",
        "    kp_norm   = kp_norm[:N_min]\n",
        "    if isinstance(y, np.ndarray):\n",
        "        y = y[:N_min]\n",
        "\n",
        "C = P_teacher.shape[1]\n",
        "print(\"Clases (C):\", C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMcztuRcueBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae840c53-f27c-4e2d-aca8-52a32387aae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_tr: (58309, 30, 34) | X_va: (14578, 30, 34) | P_tr: (58309, 3)\n"
          ]
        }
      ],
      "source": [
        "# Dataset y splits\n",
        "# Entrada estudiante: secuencia [T, K*2] (aplanamos K,2)\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "\n",
        "#  Inferir N, T, K a partir de kp_norm\n",
        "assert 'kp_norm' in globals(), \"kp_norm no está en memoria.\"\n",
        "assert kp_norm.ndim == 4, f\"kp_norm debe ser [N,T,K,D], recibido {kp_norm.shape}\"\n",
        "\n",
        "N, T, K, D = kp_norm.shape\n",
        "if D >= 2:\n",
        "    # nos quedamos con (x,y) si viniera [x,y,(z/conf)]\n",
        "    kp_xy = kp_norm[..., :2]\n",
        "else:\n",
        "    raise ValueError(f\"Última dimensión de kp_norm debe ser ≥2 (x,y). Recibido D={D}\")\n",
        "\n",
        "# Construir X_seq = [N, T, K*2]\n",
        "X_seq = kp_xy.reshape(N, T, K * 2).astype(np.float32)\n",
        "\n",
        "# Alinear longitudes con P_teacher si hiciera falta\n",
        "assert 'P_teacher' in globals(), \"P_teacher no está en memoria; genera/ carga las probabilidades del profesor.\"\n",
        "if len(P_teacher) != N:\n",
        "    n_min = min(N, len(P_teacher))\n",
        "    print(f\"[WARN] N mismatch (kp_norm={N}, P_teacher={len(P_teacher)}). Recorto a {n_min}.\")\n",
        "    X_seq = X_seq[:n_min]\n",
        "    kp_xy = kp_xy[:n_min]\n",
        "    P_teacher = P_teacher[:n_min]\n",
        "    if 'y' in globals() and y is not None:\n",
        "        y = y[:n_min]\n",
        "    N = n_min  # actualizar\n",
        "\n",
        "# Etiqueta dura para estratificación\n",
        "if 'y' in globals() and y is not None:\n",
        "    y_arr = np.asarray(y)\n",
        "    if y_arr.ndim == 2:        # multilabel -> tomamos argmax como proxy\n",
        "        y_hard = np.argmax(y_arr, axis=1).astype(int)\n",
        "    else:                      # single label\n",
        "        y_hard = y_arr.astype(int)\n",
        "else:\n",
        "    y_hard = np.argmax(P_teacher, axis=1).astype(int)\n",
        "\n",
        "# Elegir n_splits seguro (cada clase debe tener ≥ n_splits muestras)\n",
        "SEED = globals().get('SEED', 42)\n",
        "counts = np.bincount(y_hard)\n",
        "min_per_class = counts[counts > 0].min() if np.any(counts > 0) else 0\n",
        "n_splits = min(5, int(min_per_class)) if min_per_class >= 2 else 2\n",
        "\n",
        "# Si solo hay una clase o no hay suficientes muestras por clase, usar ShuffleSplit estratificado\n",
        "if len(np.unique(y_hard)) < 2 or min_per_class < 2:\n",
        "    print(\"[INFO] Estratificación no viable (clases insuficientes). Uso StratifiedShuffleSplit con test_size=0.2.\")\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
        "    tr_idx, va_idx = next(sss.split(X_seq, y_hard))\n",
        "else:\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "    tr_idx, va_idx = next(skf.split(X_seq, y_hard))\n",
        "\n",
        "# Construir splits\n",
        "X_tr, X_va = X_seq[tr_idx], X_seq[va_idx]\n",
        "y_tr, y_va = y_hard[tr_idx], y_hard[va_idx]\n",
        "P_tr, P_va = P_teacher[tr_idx], P_teacher[va_idx]\n",
        "\n",
        "print(\"X_tr:\", X_tr.shape, \"| X_va:\", X_va.shape, \"| P_tr:\", P_tr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY9Fa4duugNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f4b7d92-4c88-4dfa-ce19-7d2a6ec616cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Epoch 01 | TR 3.3075 (CE 8.3177 | KD 1.1603) | VA 3.1424 (CE 8.2985 | KD 0.9326) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 02 | TR 3.1477 (CE 8.3110 | KD 0.9348) | VA 3.1305 (CE 8.5339 | KD 0.8147) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 03 | TR 3.1244 (CE 8.2425 | KD 0.9309) | VA 3.0960 (CE 8.2285 | KD 0.8963) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 04 | TR 3.0987 (CE 8.1591 | KD 0.9299) | VA 3.0795 (CE 8.2452 | KD 0.8656) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 05 | TR 3.0727 (CE 8.0712 | KD 0.9305) | VA 3.0532 (CE 8.1245 | KD 0.8798) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 06 | TR 3.0455 (CE 7.9815 | KD 0.9301) | VA 3.0258 (CE 7.8950 | KD 0.9390) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 07 | TR 3.0164 (CE 7.8831 | KD 0.9307) | VA 2.9932 (CE 7.6898 | KD 0.9804) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 08 | TR 2.9840 (CE 7.7724 | KD 0.9318) | VA 2.9649 (CE 7.7140 | KD 0.9296) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 09 | TR 2.9477 (CE 7.6548 | KD 0.9304) | VA 2.9368 (CE 7.6613 | KD 0.9120) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 10 | TR 2.9203 (CE 7.5616 | KD 0.9312) | VA 2.9115 (CE 7.7092 | KD 0.8554) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 11 | TR 2.8863 (CE 7.4509 | KD 0.9300) | VA 2.8866 (CE 7.4908 | KD 0.9133) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 12 | TR 2.8564 (CE 7.3520 | KD 0.9297) | VA 2.8731 (CE 7.5963 | KD 0.8489) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 13 | TR 2.8300 (CE 7.2659 | KD 0.9288) | VA 2.8521 (CE 7.5293 | KD 0.8476) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 14 | TR 2.7999 (CE 7.1623 | KD 0.9303) | VA 2.8493 (CE 7.7640 | KD 0.7431) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 15 | TR 2.7766 (CE 7.0898 | KD 0.9281) | VA 2.8247 (CE 7.2594 | KD 0.9241) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 16 | TR 2.7467 (CE 6.9899 | KD 0.9282) | VA 2.7999 (CE 7.2625 | KD 0.8873) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 17 | TR 2.7228 (CE 6.9113 | KD 0.9277) | VA 2.7876 (CE 7.2710 | KD 0.8662) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 18 | TR 2.6988 (CE 6.8331 | KD 0.9270) | VA 2.7814 (CE 7.2191 | KD 0.8795) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 19 | TR 2.6741 (CE 6.7515 | KD 0.9267) | VA 2.7783 (CE 7.2226 | KD 0.8736) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 20 | TR 2.6518 (CE 6.6764 | KD 0.9269) | VA 2.7692 (CE 7.1700 | KD 0.8831) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 21 | TR 2.6316 (CE 6.6134 | KD 0.9251) | VA 2.7703 (CE 7.0268 | KD 0.9461) | ACC 0.083 | LR 1.00e-03\n",
            "Epoch 22 | TR 2.6109 (CE 6.5456 | KD 0.9247) | VA 2.7608 (CE 7.1823 | KD 0.8659) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 23 | TR 2.5909 (CE 6.4787 | KD 0.9248) | VA 2.7639 (CE 7.4247 | KD 0.7663) | ACC 0.083 | LR 1.00e-03\n",
            "Epoch 24 | TR 2.5678 (CE 6.4036 | KD 0.9240) | VA 2.7457 (CE 7.0552 | KD 0.8988) | ACC 0.083 | LR 1.00e-03\n",
            "  ↳ guardado best checkpoint: /content/drive/MyDrive/asistente_coreografico/models/student_bigru_best.pt\n",
            "Epoch 25 | TR 2.5520 (CE 6.3537 | KD 0.9228) | VA 2.7523 (CE 7.2587 | KD 0.8210) | ACC 0.083 | LR 1.00e-03\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# Modelo estudiante: BiGRU + MLP  (KD = alpha*KL + (1-alpha)*CE)\n",
        "# - Entrada: [B, T, D], D = K*2\n",
        "# - Salida : [B, C]\n",
        "# - Alinea clases: etiquetas y teacher_probs coherentes\n",
        "# =========================================================\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from inspect import signature\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ---------- Comprobaciones previas ----------\n",
        "need = ['X_tr','X_va','y_tr','y_va','P_tr','P_va']\n",
        "missing = [n for n in need if n not in globals()]\n",
        "if missing:\n",
        "    raise NameError(f\"Faltan variables previas {missing}. Ejecuta antes el paso de split/estratificación.\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------- Alineación de clases (evita 'Target out of bounds') ----------\n",
        "def normalize_labels(y):\n",
        "    y = np.asarray(y)\n",
        "    if y.ndim != 1:\n",
        "        # si llega one-hot / multilabel, tomar argmax como etiqueta dura\n",
        "        y = np.argmax(y, axis=1)\n",
        "    y = y.astype(np.int64)\n",
        "    uniq = np.unique(y)\n",
        "    # si las clases empiezan en 1 (o >0), desplaza a 0..C-1\n",
        "    if 0 not in uniq:\n",
        "        y = y - uniq.min()\n",
        "    return y\n",
        "\n",
        "def pad_probs(P, C_eff):\n",
        "    P = np.asarray(P, dtype=np.float32)\n",
        "    if P.shape[1] == C_eff:\n",
        "        return P\n",
        "    P_pad = np.zeros((P.shape[0], C_eff), dtype=np.float32)\n",
        "    P_pad[:, :P.shape[1]] = P\n",
        "    return P_pad\n",
        "\n",
        "y_tr = normalize_labels(y_tr)\n",
        "y_va = normalize_labels(y_va)\n",
        "\n",
        "# Número de clases efectivo: que quepan todas las etiquetas y las columnas de P\n",
        "C_eff = int(max(P_tr.shape[1], P_va.shape[1], y_tr.max()+1, y_va.max()+1))\n",
        "P_tr = pad_probs(P_tr, C_eff)\n",
        "P_va = pad_probs(P_va, C_eff)\n",
        "\n",
        "# D_in y C desde los splits (evita depender de T/K globales)\n",
        "D_in = int(np.prod(X_tr.shape[2:]))  # asume X_tr shape [N, T, D]\n",
        "C    = C_eff\n",
        "\n",
        "# MODELS_DIR por si no existe en el entorno\n",
        "MODELS_DIR = globals().get(\"MODELS_DIR\", \"./models\")\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- Modelo ----------\n",
        "class StudentBiGRU(nn.Module):\n",
        "    def __init__(self, d_in, hidden=192, layers=1, num_classes=8, dropout=0.15):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=d_in, hidden_size=hidden, num_layers=layers,\n",
        "            batch_first=True, bidirectional=True,\n",
        "            dropout=0.0 if layers == 1 else dropout\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden*2, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, num_classes)\n",
        "        )\n",
        "    def forward(self, x):          # x: [B,T,D]\n",
        "        out, _ = self.gru(x)       # [B,T,2H]\n",
        "        h = out.mean(dim=1)        # average pooling temporal\n",
        "        return self.head(h)        # [B,C]\n",
        "\n",
        "model = StudentBiGRU(d_in=D_in, hidden=192, layers=1, num_classes=C, dropout=0.15).to(device)\n",
        "\n",
        "# ---------- DataLoaders ----------\n",
        "BATCH = 128\n",
        "\n",
        "def make_loader(X, y, P, train=True):\n",
        "    X_t = torch.tensor(X, dtype=torch.float32)\n",
        "    y_t = torch.tensor(y, dtype=torch.long)\n",
        "    P_t = torch.tensor(P, dtype=torch.float32)\n",
        "    ds  = TensorDataset(X_t, y_t, P_t)\n",
        "    return DataLoader(ds, batch_size=BATCH, shuffle=train, drop_last=False)\n",
        "\n",
        "dl_tr = make_loader(X_tr, y_tr, P_tr, train=True)\n",
        "dl_va = make_loader(X_va, y_va, P_va, train=False)\n",
        "\n",
        "# ---------- Pérdidas y Optimizador ----------\n",
        "ce     = nn.CrossEntropyLoss()\n",
        "kl     = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "T_temp = 2.5\n",
        "alpha  = 0.7\n",
        "\n",
        "opt   = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# Scheduler ReduceLROnPlateau compatible con versiones sin 'verbose'\n",
        "def make_plateau_scheduler(optimizer, mode=\"min\", factor=0.5, patience=3, min_lr=0.0, threshold=1e-4, cooldown=0):\n",
        "    params = signature(optim.lr_scheduler.ReduceLROnPlateau.__init__).parameters\n",
        "    if 'verbose' in params:\n",
        "        return optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode=mode, factor=factor, patience=patience,\n",
        "            threshold=threshold, cooldown=cooldown, min_lr=min_lr, verbose=True\n",
        "        )\n",
        "    else:\n",
        "        return optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode=mode, factor=factor, patience=patience,\n",
        "            threshold=threshold, cooldown=cooldown, min_lr=min_lr\n",
        "        )\n",
        "\n",
        "sched = make_plateau_scheduler(opt, mode=\"min\", factor=0.5, patience=3)\n",
        "\n",
        "# ---------- Utilidades KD ----------\n",
        "def _row_normalize(p, eps=1e-8):\n",
        "    \"\"\"Normaliza por fila para que la KL tenga un target válido (suma=1).\"\"\"\n",
        "    p = p.clone()\n",
        "    s = p.sum(dim=1, keepdim=True)\n",
        "    p = p / (s + eps)\n",
        "    mask_zero = (s.squeeze(1) < eps)\n",
        "    if mask_zero.any():\n",
        "        p[mask_zero] = 1.0 / p.shape[1]\n",
        "    return p\n",
        "\n",
        "# ---------- Train / Eval ----------\n",
        "def train_one_epoch(model, dl, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = total_ce = total_kd = total_n = 0.0\n",
        "    for Xb, yb, Pb in dl:\n",
        "        Xb, yb, Pb = Xb.to(device), yb.to(device), Pb.to(device)\n",
        "        Pb_soft = _row_normalize(Pb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(Xb)                              # [B,C]\n",
        "        loss_ce = ce(logits, yb)\n",
        "        log_qT  = F.log_softmax(logits / T_temp, dim=1)\n",
        "        loss_kd = kl(log_qT, Pb_soft) * (T_temp ** 2)\n",
        "\n",
        "        loss = alpha * loss_kd + (1.0 - alpha) * loss_ce\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bsz = Xb.size(0)\n",
        "        total_loss += loss.item()    * bsz\n",
        "        total_ce   += loss_ce.item() * bsz\n",
        "        total_kd   += loss_kd.item() * bsz\n",
        "        total_n    += bsz\n",
        "\n",
        "    return total_loss/max(1,total_n), total_ce/max(1,total_n), total_kd/max(1,total_n)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dl, device):\n",
        "    model.eval()\n",
        "    total_loss = total_ce = total_kd = total_n = 0.0\n",
        "    correct = 0\n",
        "    for Xb, yb, Pb in dl:\n",
        "        Xb, yb, Pb = Xb.to(device), yb.to(device), Pb.to(device)\n",
        "        Pb_soft = _row_normalize(Pb)\n",
        "\n",
        "        logits = model(Xb)\n",
        "        loss_ce = ce(logits, yb)\n",
        "        log_qT  = F.log_softmax(logits / T_temp, dim=1)\n",
        "        loss_kd = kl(log_qT, Pb_soft) * (T_temp ** 2)\n",
        "        loss = alpha * loss_kd + (1.0 - alpha) * loss_ce\n",
        "\n",
        "        bsz = Xb.size(0)\n",
        "        total_loss += loss.item()    * bsz\n",
        "        total_ce   += loss_ce.item() * bsz\n",
        "        total_kd   += loss_kd.item() * bsz\n",
        "        total_n    += bsz\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "\n",
        "    acc = correct / max(1,total_n)\n",
        "    return total_loss/max(1,total_n), total_ce/max(1,total_n), total_kd/max(1,total_n), acc\n",
        "\n",
        "# ---------- Loop principal ----------\n",
        "EPOCHS = 25\n",
        "best_va = float('inf')\n",
        "best_path = os.path.join(MODELS_DIR, \"student_bigru_best.pt\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_ce, tr_kd = train_one_epoch(model, dl_tr, opt, device)\n",
        "    va_loss, va_ce, va_kd, va_acc = evaluate(model, dl_va, device)\n",
        "\n",
        "    # Reducir LR según validación\n",
        "    sched.step(va_loss)\n",
        "    current_lr = opt.param_groups[0]['lr']\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"TR {tr_loss:.4f} (CE {tr_ce:.4f} | KD {tr_kd:.4f}) | \"\n",
        "          f\"VA {va_loss:.4f} (CE {va_ce:.4f} | KD {va_kd:.4f}) | \"\n",
        "          f\"ACC {va_acc:.3f} | LR {current_lr:.2e}\")\n",
        "\n",
        "    # Guardar mejor checkpoint por VA loss\n",
        "    if va_loss < best_va:\n",
        "        best_va = va_loss\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(\"  ↳ guardado best checkpoint:\", best_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEjW-fxLui-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc71713-b6f0-4645-c4d0-cc29d074005c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train=0.8013 | val=0.7292 | acc=0.354\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 02 | train=0.6649 | val=0.7088 | acc=0.374\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 03 | train=0.6450 | val=0.6994 | acc=0.385\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 04 | train=0.6313 | val=0.6926 | acc=0.386\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 05 | train=0.6232 | val=0.6888 | acc=0.389\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 06 | train=0.6159 | val=0.6840 | acc=0.394\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 07 | train=0.6108 | val=0.6850 | acc=0.396\n",
            "Epoch 08 | train=0.6037 | val=0.6810 | acc=0.396\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 09 | train=0.5964 | val=0.6737 | acc=0.409\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 10 | train=0.5902 | val=0.6714 | acc=0.416\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 11 | train=0.5848 | val=0.6733 | acc=0.408\n",
            "Epoch 12 | train=0.5788 | val=0.6708 | acc=0.411\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 13 | train=0.5727 | val=0.6680 | acc=0.416\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 14 | train=0.5683 | val=0.6674 | acc=0.415\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 15 | train=0.5636 | val=0.6697 | acc=0.421\n",
            "Epoch 16 | train=0.5579 | val=0.6667 | acc=0.423\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 17 | train=0.5541 | val=0.6624 | acc=0.426\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 18 | train=0.5474 | val=0.6611 | acc=0.426\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 19 | train=0.5422 | val=0.6576 | acc=0.433\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 20 | train=0.5391 | val=0.6622 | acc=0.428\n",
            "Epoch 21 | train=0.5356 | val=0.6579 | acc=0.434\n",
            "Epoch 22 | train=0.5317 | val=0.6575 | acc=0.437\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 23 | train=0.5265 | val=0.6581 | acc=0.438\n",
            "Epoch 24 | train=0.5237 | val=0.6587 | acc=0.434\n",
            "Epoch 25 | train=0.5208 | val=0.6571 | acc=0.441\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 26 | train=0.5179 | val=0.6569 | acc=0.432\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 27 | train=0.5134 | val=0.6518 | acc=0.445\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 28 | train=0.5104 | val=0.6539 | acc=0.443\n",
            "Epoch 29 | train=0.5070 | val=0.6524 | acc=0.443\n",
            "Epoch 30 | train=0.5053 | val=0.6518 | acc=0.444\n",
            "Epoch 31 | train=0.5017 | val=0.6552 | acc=0.441\n",
            "Epoch 32 | train=0.4790 | val=0.6412 | acc=0.456\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 33 | train=0.4708 | val=0.6405 | acc=0.459\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 34 | train=0.4670 | val=0.6407 | acc=0.461\n",
            "Epoch 35 | train=0.4646 | val=0.6404 | acc=0.461\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Epoch 36 | train=0.4618 | val=0.6411 | acc=0.458\n",
            "Epoch 37 | train=0.4601 | val=0.6419 | acc=0.460\n",
            "Epoch 38 | train=0.4583 | val=0.6405 | acc=0.461\n",
            "Epoch 39 | train=0.4550 | val=0.6428 | acc=0.461\n",
            "Epoch 40 | train=0.4451 | val=0.6350 | acc=0.470\n",
            "  ↳ [SAVE] Mejor modelo en /content/drive/MyDrive/asistente_coreografico/models/student_kp.pt\n",
            "Mejor val_loss: 0.6349816401483329\n"
          ]
        }
      ],
      "source": [
        "# Entrenamiento con Early Stopping\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "best_val = 1e9\n",
        "patience = 8\n",
        "bad = 0\n",
        "EPOCHS = 40\n",
        "\n",
        "def kd_loss(logits, P_teacher, T=2.5, alpha=0.7, y_hard=None):\n",
        "    # logits: [B,C]\n",
        "    log_p = nn.functional.log_softmax(logits/T, dim=1)\n",
        "    pt    = nn.functional.softmax(P_teacher, dim=1)  # asegurar distribución\n",
        "    loss_kd = kl(log_p, pt)\n",
        "    if y_hard is None:\n",
        "        return loss_kd\n",
        "    loss_ce = ce(logits, y_hard)\n",
        "    return alpha*loss_kd + (1-alpha)*loss_ce\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    tr_loss = 0.0\n",
        "    for xb, yb, pb in dl_tr:\n",
        "        xb, yb, pb = xb.to(device), yb.to(device), pb.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = kd_loss(logits, pb, T=T_temp, alpha=alpha, y_hard=yb)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "        tr_loss += loss.item() * xb.size(0)\n",
        "    tr_loss /= len(dl_tr.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    va_loss = 0.0\n",
        "    va_acc  = 0.0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb, pb in dl_va:\n",
        "            xb, yb, pb = xb.to(device), yb.to(device), pb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = kd_loss(logits, pb, T=T_temp, alpha=alpha, y_hard=yb)\n",
        "            va_loss += loss.item() * xb.size(0)\n",
        "            pred = logits.argmax(1)\n",
        "            va_acc += (pred == yb).float().sum().item()\n",
        "    va_loss /= len(dl_va.dataset)\n",
        "    va_acc  /= len(dl_va.dataset)\n",
        "\n",
        "    sched.step(va_loss)\n",
        "    print(f\"Epoch {epoch:02d} | train={tr_loss:.4f} | val={va_loss:.4f} | acc={va_acc:.3f}\")\n",
        "\n",
        "    if va_loss < best_val - 1e-4:\n",
        "        best_val = va_loss\n",
        "        bad = 0\n",
        "        torch.save(model.state_dict(), f\"{MOD}/student_kp.pt\")\n",
        "        print(\"  ↳ [SAVE] Mejor modelo en\", f\"{MOD}/student_kp.pt\")\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "print(\"Mejor val_loss:\", best_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81mrfNgoumD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9425024d-eb9c-485c-fe10-df6bcc872b84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Exportado ONNX: /content/drive/MyDrive/asistente_coreografico/models/student_kp.onnx\n"
          ]
        }
      ],
      "source": [
        "#  Exportar a ONNX (para Streamlit u otros backends)\n",
        "#    Input: [1, T, K*2]  -> Output: [1, C]\n",
        "\n",
        "import torch\n",
        "\n",
        "best_path = f\"{MOD}/student_kp.pt\"\n",
        "model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "dummy = torch.randn(1, T, K*2, device=device)\n",
        "onnx_path = f\"{MOD}/student_kp.onnx\"\n",
        "torch.onnx.export(\n",
        "    model, dummy, onnx_path,\n",
        "    input_names=[\"seq\"], output_names=[\"logits\"],\n",
        "    dynamic_axes={\"seq\": {0: \"batch\", 1: \"time\"}, \"logits\": {0: \"batch\"}},\n",
        "    opset_version=17\n",
        ")\n",
        "print(\"[OK] Exportado ONNX:\", onnx_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Inferencia 1 ventana con StudentBiGRU (robusto)\n",
        "# - Autodetecta rutas, clases y dimensiones\n",
        "# - Sin dependencias de ART, C, kp globales\n",
        "# ============================================\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ---------- Rutas del proyecto ----------\n",
        "BASE = globals().get(\"BASE\", \"/content/drive/MyDrive/asistente_coreografico\")\n",
        "ART  = globals().get(\"ART\",  f\"{BASE}/artifacts\")\n",
        "MOD  = globals().get(\"MOD\",  f\"{BASE}/models\")\n",
        "os.makedirs(ART, exist_ok=True)\n",
        "os.makedirs(MOD, exist_ok=True)\n",
        "\n",
        "# ---------- Modelo ----------\n",
        "class StudentBiGRU(nn.Module):\n",
        "    def __init__(self, d_in, num_classes, hidden=192, layers=1, dropout=0.15):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=d_in, hidden_size=hidden, num_layers=layers,\n",
        "            batch_first=True, bidirectional=True,\n",
        "            dropout=0.0 if layers == 1 else dropout\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden*2, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, num_classes)\n",
        "        )\n",
        "    def forward(self, x):                 # x: [B,T,D]\n",
        "        y, _ = self.gru(x)                # [B,T,2H]\n",
        "        h = y.mean(dim=1)                 # media temporal\n",
        "        return self.head(h)               # [B,C]\n",
        "\n",
        "# ---------- Normalización de una ventana ----------\n",
        "def normalize_one_window(kp_window: np.ndarray) -> np.ndarray:\n",
        "    xy = kp_window[..., :2].astype(np.float32)\n",
        "    if kp_window.shape[-1] == 3:\n",
        "        conf = kp_window[..., 2]\n",
        "        mask = (conf > 0.05).astype(np.float32)\n",
        "    else:\n",
        "        mask = (~np.isnan(xy[..., 0])).astype(np.float32)\n",
        "    pts = xy[mask > 0.5].reshape(-1, 2)\n",
        "    if len(pts) == 0:\n",
        "        return np.zeros_like(xy, dtype=np.float32)\n",
        "    center = np.median(pts, axis=0, keepdims=True)\n",
        "    d = np.linalg.norm(pts - center, axis=1)\n",
        "    scale = np.percentile(d, 90) if len(d) else 1.0\n",
        "    if scale < 1e-3: scale = 1.0\n",
        "    return (xy - center) / scale\n",
        "\n",
        "# ---------- Localizar checkpoint del estudiante ----------\n",
        "def find_student_checkpoint() -> str:\n",
        "    # 1) variable global\n",
        "    path = globals().get(\"STUDENT_PT\", f\"{MOD}/student_kp.pt\")\n",
        "    if os.path.exists(path):\n",
        "        return path\n",
        "    # 2) fallback típico\n",
        "    alt = f\"{MOD}/student_bigru_best.pt\"\n",
        "    if os.path.exists(alt):\n",
        "        print(f\"[WARN] No existe {path}. Uso alternativo: {alt}\")\n",
        "        return alt\n",
        "    # 3) el más reciente con patrón razonable\n",
        "    cands = sorted(Path(MOD).glob(\"*.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "    for p in cands:\n",
        "        if \"student\" in p.name or \"bigru\" in p.name:\n",
        "            print(f\"[WARN] Uso checkpoint más reciente: {p}\")\n"
      ],
      "metadata": {
        "id": "OUoe3a-4BI9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# - Validación + Export\n",
        "# - Alinea versiones sklearn\n",
        "# - Carga dataset/artefactos del proyecto\n",
        "# - Corre inferencia y genera sugerencias\n",
        "# - Reexporta artefactos consistentes para la app (/artifacts)\n",
        "# - Emite timeline.csv y suggestions.csv\n",
        "# ============================================\n",
        "\n",
        "# ---------- 0) Dependencias y versiones ----------\n",
        "import sys, os, json, time, glob, math, warnings, textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "# Fija versiones coherentes con tus artefactos entrenados\n",
        "# (ajusta si fuera necesario en tu entorno de Colab)\n",
        "!pip -q install \"scikit-learn==1.7.1\" \"joblib==1.4.2\" \"numpy==1.26.4\" \"pandas==2.2.2\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.pipeline import Pipeline as SkPipeline\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------- 1) Rutas del proyecto ----------\n",
        "BASE = \"/content/drive/MyDrive/asistente_coreografico\"  # <- AJUSTA si usas otra carpeta\n",
        "ART  = f\"{BASE}/artifacts\"\n",
        "MOD  = f\"{BASE}/models\"\n",
        "DATA = f\"{BASE}/data\"\n",
        "\n",
        "os.makedirs(ART, exist_ok=True)\n",
        "os.makedirs(MOD, exist_ok=True)\n",
        "os.makedirs(f\"{BASE}/reports\", exist_ok=True)\n",
        "\n",
        "print(f\"[RUTAS]\\nBASE={BASE}\\nART={ART}\\nMOD={MOD}\\nDATA={DATA}\")\n",
        "\n",
        "# ---------- 2) Localización de dataset / features ----------\n",
        "# Intentamos detectar características usadas en el entrenamiento anterior.\n",
        "# Soporta múltiples formatos (npz, parquet, csv) para ser robusto.\n",
        "def find_first(*patterns):\n",
        "    for pat in patterns:\n",
        "        matches = sorted(glob.glob(pat))\n",
        "        if matches:\n",
        "            return matches[0]\n",
        "    return None\n",
        "\n",
        "FEATS_NPZ   = find_first(f\"{DATA}/processed/*feats*.npz\", f\"{DATA}/*feats*.npz\")\n",
        "FEATS_NPY   = find_first(f\"{DATA}/processed/*feats*.npy\", f\"{DATA}/*feats*.npy\")\n",
        "FEATS_PARQ  = find_first(f\"{DATA}/processed/*feats*.parquet\")\n",
        "FEATS_CSV   = find_first(f\"{DATA}/processed/*feats*.csv\")\n",
        "WINS_JSON   = find_first(f\"{DATA}/processed/*windows*.json\", f\"{DATA}/*windows*.json\")\n",
        "WINS_CSV    = find_first(f\"{DATA}/processed/*windows*.csv\", f\"{DATA}/*windows*.csv\")\n",
        "\n",
        "print(\"[DATASET DETECTADO]\")\n",
        "print(\"  FEATS_NPZ :\", FEATS_NPZ)\n",
        "print(\"  FEATS_NPY :\", FEATS_NPY)\n",
        "print(\"  FEATS_PARQ:\", FEATS_PARQ)\n",
        "print(\"  FEATS_CSV :\", FEATS_CSV)\n",
        "print(\"  WINS_JSON :\", WINS_JSON)\n",
        "print(\"  WINS_CSV  :\", WINS_CSV)\n",
        "\n",
        "def load_features_and_windows():\n",
        "    # --- Carga features ---\n",
        "    X = None\n",
        "    if FEATS_NPZ:\n",
        "        arr = np.load(FEATS_NPZ, allow_pickle=True)\n",
        "        X = arr[\"X\"] if \"X\" in arr else list(arr.values())[0]\n",
        "    elif FEATS_NPY:\n",
        "        X = np.load(FEATS_NPY, allow_pickle=True)\n",
        "    elif FEATS_PARQ:\n",
        "        df = pd.read_parquet(FEATS_PARQ)\n",
        "        X = df.values\n",
        "    elif FEATS_CSV:\n",
        "        df = pd.read_csv(FEATS_CSV)\n",
        "        X = df.values\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No se encontraron features (npz/npy/parquet/csv).\")\n",
        "\n",
        "    # --- Carga ventanas ---\n",
        "    if WINS_JSON:\n",
        "        with open(WINS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "            wins = json.load(f)\n",
        "    elif WINS_CSV:\n",
        "        wdf = pd.read_csv(WINS_CSV)\n",
        "        # Normalización esperada: columnas t0, t1\n",
        "        wins = wdf.to_dict(orient=\"records\")\n",
        "    else:\n",
        "        # Si no hay \"windows\" guardadas, creamos ventanas sintéticas 1s seguidas.\n",
        "        wins = [{\"t0\": float(i), \"t1\": float(i+1)} for i in range(len(X))]\n",
        "\n",
        "    # Aseguramos listas/np arrays\n",
        "    X = np.array(X)\n",
        "    if isinstance(wins, dict):\n",
        "        wins = [wins]\n",
        "    return X, wins\n",
        "\n",
        "X, windows = load_features_and_windows()\n",
        "print(f\"[OK] X shape = {X.shape} | windows = {len(windows)}\")\n",
        "\n",
        "# ---------- 3) Carga de artefactos de entrenamiento ----------\n",
        "# Esperamos estos ficheros (ajústalos si tus nombres cambian):\n",
        "IMP_PATH   = f\"{ART}/imputer.joblib\"\n",
        "SCL_PATH   = f\"{ART}/scaler.joblib\"\n",
        "CLF_PATH   = f\"{ART}/model_ovr_logreg.joblib\"  # o el que tengas\n",
        "LBL_PATH   = f\"{ART}/labels.json\"              # lista de etiquetas\n",
        "THR_PATH   = f\"{ART}/thresholds.json\"          # map etiqueta->umbral (float)\n",
        "\n",
        "assert os.path.exists(IMP_PATH), f\"Falta: {IMP_PATH}\"\n",
        "assert os.path.exists(SCL_PATH), f\"Falta: {SCL_PATH}\"\n",
        "assert os.path.exists(CLF_PATH), f\"Falta: {CLF_PATH}\"\n",
        "\n",
        "imp    = joblib.load(IMP_PATH)\n",
        "scaler = joblib.load(SCL_PATH)\n",
        "clf    = joblib.load(CLF_PATH)\n",
        "\n",
        "labels_list = []\n",
        "if os.path.exists(LBL_PATH):\n",
        "    with open(LBL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        labels_list = json.load(f)\n",
        "\n",
        "thr_map = {}\n",
        "if os.path.exists(THR_PATH):\n",
        "    with open(THR_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        thr_map = json.load(f)\n",
        "\n",
        "print(f\"[ARTEFACTOS] labels={len(labels_list)} | thresholds={len(thr_map)}\")\n",
        "\n",
        "# ---------- 4) Clasificación y umbrales ----------\n",
        "# pred_prob -> aplica umbrales por etiqueta si existen; si no, usa 'thr_default'\n",
        "def predict_labels_batch(X_batch, thr_default=0.5):\n",
        "    # Pipeline simple y robusto con imputer + scaler + clf\n",
        "    X_imp = imp.transform(X_batch)\n",
        "    X_std = scaler.transform(X_imp)\n",
        "\n",
        "    # Probabilidades por clase (OneVsRestClassifier / LogisticRegressionCV)\n",
        "    if hasattr(clf, \"predict_proba\"):\n",
        "        P = clf.predict_proba(X_std)\n",
        "    else:\n",
        "        # Fallback a decision_function si no hay predict_proba\n",
        "        if hasattr(clf, \"decision_function\"):\n",
        "            dec = clf.decision_function(X_std)\n",
        "            # Sigmoide para pseudo-probas\n",
        "            P = 1.0 / (1.0 + np.exp(-dec))\n",
        "        else:\n",
        "            raise RuntimeError(\"El clasificador no soporta predict_proba ni decision_function.\")\n",
        "\n",
        "    # P shape: (N, C)\n",
        "    N, C = P.shape\n",
        "    out_labels = []\n",
        "    out_scores = []\n",
        "    for i in range(N):\n",
        "        labels_i, scores_i = [], []\n",
        "        for c in range(C):\n",
        "            label_name = labels_list[c] if c < len(labels_list) else f\"class_{c}\"\n",
        "            thr = float(thr_map.get(label_name, thr_default)) if thr_map else thr_default\n",
        "            if P[i, c] >= thr:\n",
        "                labels_i.append(label_name)\n",
        "                scores_i.append(float(P[i, c]))\n",
        "        out_labels.append(labels_i)\n",
        "        out_scores.append(scores_i)\n",
        "    return out_labels, out_scores\n",
        "\n",
        "# ---------- 5) Motor de sugerencias ----------\n",
        "# Define aquí el mismo mapping que usas en el Colab/app\n",
        "SUGGESTIONS_MAP = {\n",
        "    # \"etiqueta\": \"texto de sugerencia\"\n",
        "    \"alineacion_brazos\": \"Ajusta la alineación de codos y muñecas en ángulos complementarios al torso para ganar limpieza.\",\n",
        "    \"centro_gravedad\": \"Rebaja el centro de gravedad al inicio del giro y estabiliza el core antes del impulso.\",\n",
        "    \"transferencia_peso\": \"Marca un micro-plié al transferir peso para amortiguar y mantener el eje.\",\n",
        "    \"mirada_foco\": \"Define un foco visual claro antes del desplazamiento para orientar direccionalidad.\",\n",
        "    \"sincronizacion_grupo\": \"Escucha el phrasing común y ancla entradas en los tiempos fuertes para cohesión.\",\n",
        "    \"extensiones_linea\": \"Extiende dedos y empeines para completar la línea hasta el extremo distal.\",\n",
        "    \"dinamica_contrastes\": \"Introduce contraste de dinámicas (sostenido vs. acento) para clarificar intención.\",\n",
        "    \"respiracion_fraseo\": \"Apoya la musicalidad con respiración en anacrusas y cierres para articular el fraseo.\",\n",
        "    \"espacial_path\": \"Optimiza el path espacial evitando zigzags no intencionales; usa diagonales limpias.\",\n",
        "    \"aterrizaje_silencioso\": \"Aterriza con metatarso-suelo progresivo para minimizar ruido y proteger articulaciones.\",\n",
        "}\n",
        "\n",
        "def suggestions_from_labels(labels):\n",
        "    sugs = []\n",
        "    for lb in labels:\n",
        "        if lb in SUGGESTIONS_MAP:\n",
        "            sugs.append(SUGGESTIONS_MAP[lb])\n",
        "    # Si no hubo match exacto, ofrece un comodín si te interesa:\n",
        "    if not sugs and labels:\n",
        "        sugs.append(\"Refina el eje postural y verifica la alineación cabeza-hombros-cadera en la transición.\")\n",
        "    return sugs\n",
        "\n",
        "# ---------- 6) Inferencia sobre el dataset y export ----------\n",
        "def run_full_inference_and_export(X, windows, thr_default=0.5, out_dir=BASE):\n",
        "    t0 = time.time()\n",
        "    out_rows = []\n",
        "    Y_labels, Y_scores = predict_labels_batch(X, thr_default=thr_default)\n",
        "\n",
        "    for i, (lbls, scs) in enumerate(zip(Y_labels, Y_scores)):\n",
        "        w = windows[i] if i < len(windows) else {}\n",
        "        t_start = float(w.get(\"t0\", i*1.0))\n",
        "        t_end   = float(w.get(\"t1\", (i+1)*1.0))\n",
        "        sug = suggestions_from_labels(lbls)\n",
        "\n",
        "        out_rows.append({\n",
        "            \"win_id\": i,\n",
        "            \"t_start\": t_start,\n",
        "            \"t_end\": t_end,\n",
        "            \"labels\": \",\".join(lbls),\n",
        "            \"scores\": \",\".join([f\"{s:.4f}\" for s in scs]),\n",
        "            \"suggestions\": \" | \".join(sug)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(out_rows)\n",
        "    timeline_csv   = f\"{out_dir}/reports/timeline.csv\"\n",
        "    suggs_csv      = f\"{out_dir}/reports/suggestions.csv\"\n",
        "    df.to_csv(timeline_csv, index=False, encoding=\"utf-8\")\n",
        "    df[[\"win_id\",\"t_start\",\"t_end\",\"suggestions\"]].to_csv(suggs_csv, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    # Exporta un \"bundle\" de control de versiones para la app:\n",
        "    versions = {\n",
        "        \"scikit_learn\": \"1.7.1\",\n",
        "        \"joblib\": \"1.4.2\",\n",
        "        \"numpy\": \"1.26.4\",\n",
        "        \"pandas\": \"2.2.2\",\n",
        "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    }\n",
        "    with open(f\"{ART}/versions.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(versions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Garantiza labels.json y thresholds.json para la app\n",
        "    if labels_list:\n",
        "        with open(f\"{ART}/labels.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(labels_list, f, ensure_ascii=False, indent=2)\n",
        "    if thr_map:\n",
        "        with open(f\"{ART}/thresholds.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(thr_map, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    print(f\"[OK] Exportados:\\n  {timeline_csv}\\n  {suggs_csv}\\nTiempo: {dt:.2f}s | filas={len(df)}\")\n",
        "    return df\n",
        "\n",
        "df = run_full_inference_and_export(X, windows, thr_default=0.5, out_dir=BASE)\n",
        "display(df.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "1pRLESmSLPf2",
        "outputId": "235dd81a-73b5-4ee2-eb99-ac5f8f7f5660"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3553895185.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSkPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}